
mysql基本架构：

    客户端
        ↓
    /**** Server层 ↓ ****/
    连接器                                   
        ↓                                       ↘
    分析器：词法分析，语法分析   →    查询缓存：命中则直接返回结果(连接器与分析器都会查询缓存)
        ↓
    优化器：执行计划生成，索引选择
        ↓
    执行器：操作引擎，返回结果
    /**** Server层 ↑ ****/
        ↓
    存储引擎：存放数据，提供读写接口，支持InnoDB、MyISAM、Memory等



连接器：
    任何数据库操作的第一步，都会先连接到数据库上，这个时候接待的就是连接器。连接器负责跟客户端建立连接、获取权限
、维持和管理连接。

    一般的连接命令如下：mysql -h$ip -P$port -u$user -P，然后输入密码即可。如果用户名或密码不对，就会收到一个
Access denied for user错误，然后客户端程序结束。如果认证通过，连接器会到权限表里查出该用户拥有的权限，之后这个连
接里面的权限判断逻辑都将依赖于此时读到的权限。这意味着一个用户成功建立连接后，即使修改了权限，也不会影响已存在
连接的权限，只有新的连接才会使用新的权限设置

    连接完成后，如果没有后续的动作，这个连接就处于空闲状态，可以在show processlist命令中看到它，其中command列
为的值Sleep就代表空闲连接。如果客户端太长时间没动静，连接器就会自动将它断开，这个时间由wait_timeout控制，默认
8小时。这时候如果连接再发送请求就会收到错误提示：Lost connection to MySQL server during query。此时就需要重新
建立连接再执行请求了

    数据库里面，长连接是指建立成功后，如果客户端有持续请求，则一直使用同一个连接。短连接是指每次执行完很少几次查
询就断开连接，下次查询再重新建立一个。由于连接的建立过程比较复杂，因此建议尽量减少建立连接的动作，也就是尽量使
用长连接
    但是全部使用长连接后，可能有时候MySQL占用内存涨得特别快，这是因为MySQL在执行过程中临时使用的内存是管理在
连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接积累下来，可能会因为内存占用太大而被系统强行杀
掉(OOM)，这种现象看起来就像是MySQL异常重启了
    要解决这个问题可以考虑两种方案：
        1、定期断开长连接，或者每执行一个占用内存的大查询后，就断开连接，需要时再重连
        2、MySQL 5.7以上版本，可以在每次执行一个比较大的操作后，执行mysql_reset_connection来重新初始化连接资源。
    这个过程不需要重连和重新认证，但是会将连接恢复到刚创建完时的状态


查询缓存：
    MySQL拿到一个查询请求后，会先到查询缓存看看是不是执行过这条语句。之前执行过的查询与结果可能会以key-value
的形式被缓存在内存中。key是查询语句，value是查询结果。如果key能匹配就会直接返回value
    如果语句不再查询缓存中，就会执行后面的阶段。执行完成后再将结果放入查询缓存中。所以如果命中缓存，直接返回结果
效率就会很高。

    但是大多数情况下都不建议用缓存，因为缓存的失效非常频繁，只要有对一个表的更新，这个表的所有查询都会被清空。对
于更新压力大的数据库来说，缓存的命中率会非常低。除非业务是一张静态表才适合使用查询缓存
    MySQL也提供了按需使用的方式。将query_cache_type设置成DEMAND就是默认不使用查询缓存。如果使用查询缓存就可
以在SELECT后面增加SQL_CACHE来显式指定

    需要注意的是MySQL 8.0之后查询缓存功能已经删掉了

分析器：
    如果没有命中查询缓存，就要真正开始执行语句了，因此需要对SQL语句做解析。
    分析器会先做词法分析。输入的是一串SQL语句，MySQL需要识别出里面的字符串分别是什么，代表什么。例如SELECT * 
FROM T WHERE id=1，从SELECT关键字里可以识别出来这是一个查询语句，将T识别为表名，将id识别为列id。
    识别完词法后就要做语法分析，语法分析器会根据语法规则，判断输入的这个SQL语句是否满足MySQL的语法。如果语句不
对，就会收到You have an error in your SQL syntax的错误提醒

优化器：
    经过了分析器，MySQL就知道要做什么了。在开始执行之前，还要经过优化器的处理。
    优化器是在表里有多个索引的时候，决定使用哪个索引；或者在一个语句有多张表的时候，据欸的那个各个表的连接顺序。

执行器：
    MySQL通过分析器知道了要做什么，通过优化器知道了该怎么做，于是就进入了执行器阶段，开始执行语句。
    开始执行的时候，要先判断一下你对这个表T有没有执行查询的权限，如果没有，就会返回没有权限的错误。如果有权限，
执行器就会根据表的引擎定义，去使用这个引擎提供的接口。然后调用InnoDB引擎接口去扫表或者铜鼓索引去获取表中对应
的行，满足条件则加入结果集，不是则跳过，然后取下一行重复相同的判断，直到最后一行，然后执行器将所有满足条件的行
组成的记录集作为结果集返回给客户端。
    数据库的慢查询日志中可以看到一个rows_examined的字段，表示这个语句执行过程中扫描了多少行，这个行就是在执行器
每次调用引擎获取数据行的时候累加的。有时候执行器调用一次，在引擎内部扫描了多行，因此引擎扫描行数跟rows_examined
并不是完全相同的


MySQL的记录更新：
    MySQL如果每次更新操作都需要写进磁盘，然后磁盘也要找到对应的记录，然后再更新，整个过程IO、查找成本都很高。
    为了解决这个问题，MySQL的设计者就用了WAL技术来提升更新效率，WAL全程是Write Ahead Logging，它的关键点
就是先写日志，再写磁盘。当有一条记录需要更新的时候，InnoDB引擎就会先把记录写到redo log里，并更新内存。同时，
InnoDB引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做
    如果redo log不多，MySQL可以等空闲时再将其更新到磁盘，如果redo log特别多，并且写满了，那MySQL只能先将一部
分redo log更新到磁盘中，然后将这些记录从日志中清掉，为后面的新日志腾出空间

    InnoDB引擎层的redo log(重做日志)是一组文件，比如是4个文件，每个文件1GB，那么redo log一共可以记录4GB的操作，
从头开始写，写到末尾就回到开头循环写。InnoDB记录了两个位置，其中write pos是当前记录的位置，一边写一边后移，写
到最后一个文件就回到第一个文件开头；checkpoint是当前要擦除的位置，也是往后推移并且循环的，在擦除前要把对应的记
录更新到表文件，更新后才能将checkpoint后移
    而write pos和checkpoint之间就是redo log还空着的部分，可以用来记录新的操作。如果write pos追上checkpoint就表示
redo log满了，这时候不能再执行新的更新操作，得停下来将checkpoint推进一下
    有了redo log后，InnoDB就可以保证即使数据库发生重启，之前提交的记录都不会丢失，这个能力称为crash-safe
    redo log记录的是这个页做了什么，比如id=2的行c字段+2

redo log的落盘：
    事务提交会将日志写入log buffer中，然后的落盘机制通过innodb_flush_log_at_trx_commit参数来控制，有三个值：
        0：每秒写入一次os buffer，并调用fsync()刷盘
        1：每次都写入os buffer，并调用fsync()刷盘
        2：每次都写入os buffer，每秒调用一次fsync()刷盘

    server层的binlog(归档日志)，上面讲的redo log是InnoDB引擎特有的，而binlog是MySQL的server层实现，所有引擎都可
以使用。binlog是追加写入的，binlog文件写到一定大小后会切换到下一个，并不会像redo log那样覆盖以前的日志
    binlog的记录有两种模式，原始sql与row格式，row格式会记录更新前与更新后的值

    binlog的落盘：
        sync_binlog的值表示每写缓存多少次就同步到磁盘，1表示每次都写入

    了解了以上的概念之后，然后就是update语句执行的内部流程：
        1、执行器先通过引擎找到id=2这行。由于id是主键，引擎直接用树搜索找到这一行。如果这一行所在的数据页本来就在
    内存中，就直接返回给执行器；否则就先从磁盘读入内存，然后再返回
        2、执行器拿到引擎给的行数据，把这个值加上1，得到新的行数据，再调用引擎接口写入这行新数据
        3、引擎将这行新数据更新到内存中，同时将这个更新操作记录到redo log中，此时redo log处于prepare状态。然后告知
    执行器执行完成了，随时可以提交事务
        4、执行器生成这个操作的binlog，并将binlog刷盘
        5、执行器调用引擎的提交事务接口，将刚刚写入的redo log改成commit状态，update语句结束
        补充：上面这个流程如果在binlog刷盘后出现断电，数据就会不一致了，因此MySQL重启后恢复会检查redo log是否处于
    prepare阶段，如果是的话就检查binlog是否完整，通过commit的时候写入到redo log中的XID来到binlog中进行匹配，如
    果能匹配则自动进行commit，否则就(撤销redo log的记录还是将数据补充到binlog?)
        这就是二阶段提交，先prepare再commit，二阶段提交是跨系统维持数据逻辑一致性常用的一个方案


事务：
    串行化是指对于同一行记录，写会加写锁，读会加读锁，当读写锁出现冲突时，后访问的事务，必须等前一个事务执行完成才能
继续执行

    MySQL中，每条记录更新的时候都会记录同一条回滚操作。不同时刻启动的事务会有不同的read-view，同一条记录在系统中可以
存在多个版本，这就是数据库的多版本并发控制(MVCC)。如果要得到某个旧的版本的值，就要通过当前值执行回滚操作，一直往前
获取旧的值。因此即使另外一个事务同时正在执行，这个事务也跟当前事务不会冲突，因为他们处于不同的read-view中
    这些数据的不同版本就记录在undo log中，新的版本会引用旧的版本，每个版本都有自己的事务id。当没有事务需要再用到这些回
滚日志时，回滚日志就会被删除。不需要就是指系统里正在执行的事务没有比这个回滚日志更早的read-view的时候
    长事务意味着系统里会存在很老的事务视图。由于这些事务随时可能访问数据库里的任何数据，所以这个事务提交前，数据库里它
可能用到的回滚日志都必须保留，这就会导致大量占用存储空间，除此之外长事务还会占用锁资源，因此尽量不要使用长事务

    事务的自动提交需要set autocommit=1，当开启自动提交后，如果没有显式地进行begin或start transaction，那么每执行一条
SELECT或UPDATE都会自动启动与提交事务，如果显式地启动事务，那么就也需要显式地commit或rollback。开启自动提交的好处
就避免了单独执行一个SELECT或UPDATE时事务自动启动，但是不会自动提交
    但是这会带来多一次交互的问题，因为自动提交开启后，如果要手动提交就需要主动执行一次begin。这种情况可以通过使用
commit work and chain语法语法来解决，这个语法是提交事务并自动启动下一个事务

    可以通过在information_schema库的innodb_trx表中查询长事务，比如以下语句可以用来查询持续时间超过60S的事务：
select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))>60
    如果要避免长事务，可以通过max_execution_time参数指定事务最长执行时间，也可以通过前面讲的表去进行监控
    可以将innodb_undo_tablespaces设置成2，既使用独立的空间去存储undo log，如果为0则代表使用系统表空间：ibdata1

    事务的隔离：
        MySQL中有两个视图：一个是view，一个虚拟表，用法跟普通表差不多。另一个是InnoDB在实现MVCC时用到的一致性读视图
    ，即consistent read view，用于支持RC和RR隔离级别的实现
        MySQL中每行数据都有多个版本，每次事务更新时，都会生成一个新的数据版本，并把transaction id赋值给这个数据版本，记
    为row trx_id，同时保留旧的数据版本。也就是说，数据表中的一行记录，可能有多个版本，每个版本都有自己的row trx_id
        语句更新会生成undo log，undo log的内容类似于set k=k+1，所以数据版本并不是真实存在的，而是要通过undo log根据最
    新版本一次次倒推算出来的：首先拿到当前最新数据行与它对应的上一个undo log位置，然后去那个undo log位置找到上个版本的
    日志与上上次undo log的位置，一直重复这个过程，直到找到需要的数据版本

        每个事务都有一个数组，用来保持保存这个事务启动瞬间，当前正在活跃的所有事务ID，活跃是指启动但没提交。数组里最小事务
    id记为低水位，最大事务id+1记为高水位，小于低水位的事务一定都提交了，大于高水位的事务一定都未提交，在高水位与低水位中
    间的事务则要判断是否在事务id数组里，在里面代表未提交，不在里面代表已提交。已提交的事务可见，未提交则不可见。MySQL在
    查询的时候拿行版本的事务id去与事务的id数组做对比来决定是否可见，不可见则一直往前获取版本。这样下来无所事务在什么时候
    查询，看到的行数据结果都是一致的，所以我们称之为一致性读
        但是多个事务同时更新同一行的时候，后面的事务就会被阻塞，而且不能用历史版本，否则如果进行k=k+1这样的操作会导致更新
    丢失，因此更新操作总是读当前的值后再写入的，这称为当前读，可以理解为临时创建一个新的视图去执行这条更新。更新成功之后视
    图里最新版本的事务id就是当前事务，因此当前事务能读到更新后最新的值，如果未更新到内容，最新版本的事务id还是其它事务，就
    无法看到最新的值。只要行被加锁，执行的就是当前读，例如UPDATE，或者SELECT * FROM t WHERE id=1，后面加上
    lock in share mode或for update都能读到最新值
        RR和RC实现的区别是：RR在事务开始的时候创建read view；而RC在每个语句执行前都会创建一个read view(因此每条语句都能知
    道最新有哪些事务是已提交的)

索引：
    索引的出现就是为了提高数据查询的效率，就像书的目录一样。

    索引有三种比较常见的数据结构：
        哈希表：
            哈希表是一种以键值对存储数据的结构，只要输入key就能找到value，当出现相同key的情况，就会拉出一个链表。哈希表
        这种结构只适用于等值查询，不适合区间查询

        有序数组：
            有序数组在等值查询和范围查询中的性能都非常优秀，如果要范围查询，就先用二分法找到起始的位置，然后往后遍历就行了
        。但是有序数组在更新的时候，如果往中间插一条记录就必须得挪动后面的所有记录，成本太高，所以有序数组只适用于存储不
        会再修改的数据
            这个时间复杂度是O(log(N))

        二叉搜索树：
            二叉搜索树的特点是：父节点左子树所有节点的值都小于父节点的值，右子树所有节点的值都大于父节点的值。这个时间复杂
        度是O(log(N))。为了维持这个查询的复杂度，就需要保持这颗树是平衡二叉树
            树可以有二叉，也可以有多叉(N叉)，多叉树就是每个节点都有多个儿子，儿子之间的大小保证从左到右递增。为了让一个查询
        尽量少地读盘，就必须让查询尽量少访问数据块。那么就不应该用二叉树，而是用多叉树，因为二叉树会导致树高很高
            以InnoDB的一个整数字段为例，N叉这个N差不多是1200，如果这颗树高是4，1200^3就可以存17亿了。考虑到树根的数据块
        总是放在内存中的，因此在一个10亿级别的表上查询一个整形字段的索引，最多只需要访问3次磁盘。由于第二层也有很大概率在
        内存中因此访问磁盘的平均次数可能会更少。因为MySQL的存储单位是页，而一个页的默认大小是16K，一个整形(bigint)的长度
        是8B，每个索引还跟着6B子树的指针，所以一个节点大概能放下16K/14B≈1200个索引

    InnoDB的索引模型：
        在InnoDB中，表都是根据主键顺序以索引的形式存放的，由于InnoDB使用了B+树索引模型，所以数据都是存储在B+树中的。每
    个索引在InnoDB里面都对应一颗B+树
        每张表的主键索引也是一颗B+树，索引是ID，叶子节点是整行数据，在InnoDB里，主键索引也被称为聚簇索引(clustered index)
    ；非主键索引的叶子节点内容是主键的值，在InnoDB中，非主键索引也被称为二级索引(secondary index)
        如果搜索的条件是ID的话，既主键查询的方式，则只需要搜索ID这颗B+树；如果搜索的是条件是普通索引的话，就需要通过搜索
    普通索引树，得到对应的ID，再到ID索引搜索一次，这个再去ID索引搜索的过程称为回表。也就是说基于非主键索引的查询需要多
    扫描一颗索引树，因此查询过程中应该尽量用主键查询

    索引维护：
        B+树为了维护索引有序性，在插入新值的时候需要做必要的维护。如果新插入的记录刚好在一个数据页的中间，且这个数据页也
    满了，这时候就需要申请一个新的数据页，然后挪动部分数据过去，这个过程称为页分裂，这种情况下，性能自然会受到影响。除了
    性能外，页分裂还影响了数据页的利用率，原本一个页的数据分到两个页中，整体空间利用率降低了大约50%。
        当然有分裂就有合并，当相邻的两个页删除了数据，利用率很低之后就会将数据页做合并。合并的过程可以认为是页分裂的逆过程
        考虑到自增主键的ID是递增的，因此每次插入数据都是追加操作，不涉及挪动其它记录，也不会触发页分裂，因此自增主键的性能
    会比较高，而又业务逻辑的字段做主键，则往往不容易保证有序插入，这样写入的成本相对较高
        除了性能外，每个非主键索引的叶子节点上都是主键的值，因此主键长度越小，每个普通索引的叶子节点就越小，占用空间也越小
        因此从性能与存储空间方面考量，自增主键往往是更合理的选择。除非表只有一个索引，且该索引是唯一索引，否则就不该用业务
    字段做索引

    覆盖索引：
        如果需要查询的值已经在使用的索引树上了，就可以直接提供查询结果，不需要回表。也就是说在查询中，索引已经覆盖了查询需求
    ，这种情况称为覆盖索引

    最左前缀：
        最左前缀可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符。建立索引的时候，如果能通过调整顺序，可以少
    维护索引，那么这个顺序往往就是需要优先采用的
        如果既有联合索引，又有单个索引。例如a,b，此时如果将索引设计为a,b，那么对b的查询就无法使用到索引了，因此还要额外维护一
    个b索引。这种情况就要考虑空间了，比如b是比a占用空间更大的，这时候就应该创建一个b,a联合索引与一个a单字段索引，这样a索引
    维护两位的代价比b索引维护两次的所占用的空间更小

    索引下推：
        如果一个查询有多个条件，但是使用到的索引只能支持到部分条件，正常情况下是要将索引无法匹配的字段去主键索引里回表查询出
    来进行判断的，但是索引下推可以将联合索引里剩余的字段用来进行判断，而无需回表。
        例如：WHERE name LIKE 'abc%' AND age = 1 AND state = 1，此时如果索引是name,age，那么在abc%用到索引后，还会通过
    age索引来进行等=1的判断，这步在索引遍历过程中就做了，最后拿到过滤后的数据去回表查找state=1的数据，这样一来可以减少回表
    的次数

    如果索引字段的值相同，那么它们会以主键索引进行排序，也就是说，索引a表面上只有a字段，但是它的叶子节点里的id是进行了排序的
，因此a索引实际上是a,id的效果，所以如果创建一个a,id索引的效果跟a索引是完全一致的，跟在后面的id完全可以去掉，联合主键也一样


锁：
    全局锁：
        全局锁能让整个库处于制度状态，命令是：Flush tables with read lock(FTWRL)，它的典型使用场景是做全库逻辑备份。如果执行全
    局锁命令的客户端发生异常断开，MySQL就会自动放开全局锁
        如果想进行全局备份，可以用mysqldump的-single-transactin参数来进行，它利用了事务来保证读一致性，因此不会锁库也不会出现
    数据错误

    表级锁：
        MySQL里表级锁有两种，分别是：

        表锁：
            表锁的语法是：lock tables t1 read, t2 write，代表分别为t1加读锁，为t2加写锁。释放锁的语法是：unlock t1,t2，客户端断开的时
        候也会自动释放锁。表锁在加锁后，就只能操作被锁定的表，以前面的例子为例，就只能读t1、读写t2，连写t1都不允许，其他表也不允
        许访问。
            由于表锁粒度太大，通常不会使用lock tables命令来控制并发，毕竟锁住整个表的影响还是太大了

        元数据锁(meta data lock，MDL)：
            MDL不需要显式使用，在访问一个表的时候会被自动加上。当对一个表做增删改查的时候，会自动加上MDL读锁；当变更表结构的时
        候，会自动加上MDL写锁。哪怕前面有读锁导致写锁获取阻塞，写锁后面的所有读锁获取也都会被阻塞，类似于一个队列，前面的获取
        了后面的才能获取。元数据锁会在事务提交时自动释放

        行锁：
            行锁就是针对数据表中行记录的锁，行锁会在需要的时候才加上，事务提交才释放。这个就是二阶段锁协议
            因此如果事务中需要锁多个行，就要把最可能造成锁冲突的锁放后放，这可以减少事务持锁的时间，可以提交并发度

    死锁和死锁检测：
        当事务a更新t1表，事务b更新t2表后，然后它们再去更新对方的表，此时就会出现死锁。当出现死锁后有两种策略：
            1、等待锁超时，这个超时时间可以通过innodb_lock_wait_timeout设置，默认是50s
            2、进行死锁检测，发现死锁后，回滚其中的一个事务，让其余事务继续执行，这个开关通过innodb_deadlock_detect=on来开启

        正常情况下都是使用死锁检测的方案，因为锁超时太久会导致业务无法接受，太快又会出现很多误伤。但是死锁检测也是有额外负担的
    ，每个事务被锁住的时候，都要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了死锁。
        每个新来的线程都要判断会不会由于自己的加入导致死锁，如果有1000个并发线程同时更新同一行，死锁的检测操作就是100W量级，
    虽然最终检测的结果是没有死锁，但是这期间也会消耗大量CPU资源，因此会看到CPU利用率很高，每秒却执行不了几个事务
        有两种思路解决这个问题：1、如果能保证业务不会出现死锁，就把死锁检测关掉。2、在MySQL之前做控制，对相同行的更新做排队。
    还有一种方式是，将一行数据改为多行，例如要记录用户总额，可以分为10条记录来记，更新时随机取一个更新，获取总额时求和即可，
    只是减少时碰到0的情况还要检测其它记录的余额，甚至多个行一起减，比较麻烦，这样也能减少死锁检测的消耗































