大概目标：请求处理(已完成)、消息生产(已完成)、消息消费、事务协调器、组协调器、幂等(已完成)
、副本同步细节(ISR、LW、LEO)、日志处理细节(追加部分已完成)、延迟消息的时间轮应用(已完成)
、组协调器与poll的心跳


kafka broker代码里配置所在：KafkaConfig，配置默认值所在：Defaults

1、initZkClient()，初始化zookeeper客户端，并尝试创建kafka的基础信息节点：
    /consumers
    /brokers/ids
    /brokers/topics
    /config/changes
    /admin/delete_topics
    /brokers/seqid
    /isr_change_notification
    /latest_producer_id_block
    /log_dir_event_notification
    /config/topics
    /config/clients
    /config/users
    /config/brokers

    2、getOrGenerateClusterId()，获取或生成集群id，首先在zk的/cluster/id节点里找，如果找不到就生成一个uuid并放进去
然后返回

    3、getBrokerMetadataAndOfflineDirs()，从log.dirs目录中加载元数据，由于用户可以设置多个日志目录，还会校验这些
目录中meta.properties里的brokerId与clusterId是否全部一致，如果出现不一致则抛出InconsistentBrokerMetadataException

    4、校验meta.properties里的clusterId是否与zk里/cluster/id的一致，如果不一致抛出InconsistentClusterIdException

    5、getOrGenerateBrokerId()，会校验配置里的brokerId与meta.properties里的brokerId是否一致，不一致则抛出
InconsistentBrokerIdException，然后如果两个配置的brokerId都没配置(小于0)，则通过zk的/brokers/seqid节点生成一个
brokerId，brokerId = /brokers/sqeid的版本号 + reserved.broker.max.id

    6、从zk中更新配置，会读两个配置，分别是/config/broker/<default>与/config/broker/0，对应着默认配置与当前
brokerId的配置，zk中的配置会有更高的优先级，并且用户可以通过kafka-configs.sh脚本向zk的/config/changes节点中写入
数据，然后触发配置变更，以此实现动态配置

    7、创建KafkaScheduler，内部就是一个java的任务调度器，线程数是background.threads，默认为10

    8、创建quotaManagers，配额管理器

    9、创建logManager，日志管理器，这个日志是kafka存消息的日志

    10、创建metadataCache，它有着每个分区的元数据缓存，例如它的leader信息

    11、创建tokenCache与credentialProvider，它们提供鉴权相关的功能

    12、创建socketServer，这个组件是kafka用来创建服务器与处理请求的。其内部包含了Acceptor与processor，其中
Acceptor只有一个，而processor取决于num.network.threads。主线程会等待Acceptor启动完成后才会继续，这个行为通过
一个在Acceptor内部的CountDownLatch对象来进行：主线程拿锁等待，Acceptor线程释放锁。这一步暂时还不会启动
Processor，因为要等服务器完全初始化后才能启动处理器
    kafka的请求处理器分为两种：controlPlane和dataPlane，分别是控制面板与数据面板，其中控制面板controlPlane是管理
集群请求的，需要配置control.plane.listener.name属性才会初始化；数据面板dataPlane是管理生产与消费者请求的，这个一
定会启用
    控制面板的服务器配置放在listeners配置中，多个配置多逗号隔开，例如端口，好像也可以写一个url
        
    13、创建replicaManager，副本管理器，然后调用它的startup方法，里面会启动几个调度任务：isr-expiration、
isr-change-propagationshutdown-idle-replica-alter-log-dirs-thread。还会启动一个LogDirFailureHandler线程，这个是
处理失败副本的处理器

    14、创建/brokers/ids/{brokerId}的zk临时节点，并设置节点的data，data里包含当前broker的一些信息，例如端口、
主机名、当前broker的节点路径、jmxPort等

    15、将brokerId、clusterId、version等信息写入meta.properties文件

    16、启动tokenManage，token管理器，token管理器是否会真正干活取决于delegation.token.master.key是否有配置，
它对应着zk里的/delegation_token节点

    17、启动kafkaController集群控制器，然后调用它的startup方法，这个方法会在zk客户端里注册一个状态变更处理器，
用于监听broker与zk之间的会话是否过期，然后往eventManager事件管理器里添加一个StartUp启动事件，然后启动事件
管理器，事件管理器里对StartUp事件的处理是，往zk注册一个节点变更监听器，监听节点为：/controller，当监听到该节点
发生不同的变更时，就往事件管理器里添加不同的事件，例如：Reelect(重选事件)、ControllerChange(控制器变更事件)
    然后就会尝试去获取/controller节点的数据，加入发现/controller节点的数据是空的，就会尝试自己去注册，也就是尝试创
建/controller临时节点，节点数据里brokerId是当前节点的brokerId，然后设置/controller_epoch的数据，epoch的设置会
附带版本号，相当于有个乐观锁的操作，当临时节点与epoch都设置成功，则返回当前设置成功的epoch与节点version
    如果/controller节点创建失败或者/controller_epoch数据设置失败，则抛出ControllerMovedException异常，代表其它
broker成为了控制器，然后捕获ControllerMovedException异常后就会为/controller注册监听器，并从zk拿到controllerId
放到kafkaController.activeControllerId中。选举成功则直接拿当前的brokerId放到kafkaController.activeControllerId，此
外还会设置controllerContext.epoch、controllerContext.epochZkVersion
    为什么有了/controller节点还有设置/controller_epoch，因为/controller节点变更的时候可能有些监听器因为网络原因没
有触发到，导致不同的broker记录的控制器是不同的，因此加了一个/controller_epoch，每次其余broker与控制器交互都会
带上这个epoch，如果它们的epoch比控制器的小，则代表他们正在向一个旧的控制器发送请求，该请求就会被认为是无效请
求，它们就需要更新控制器信息
    /controller_epoch的作用：
    例如broker_A是控制器，但是它挂了，然后zk的监听器只触发了broker_B，没触发到broker_C，然后broker_B成为了新的
控制器，但是此时broker_C依然会向broker_A发送控制器请求，此时就可以利空/controller_epoch来校验，由于broker_A重
启后会获取最新的控制器信息，因此他会发现broker_C给他发送的controller_epoch是旧的，他就会告知broker_C去更新控制
器信息

    18、创建adminManager，看起来像是一个内部使用的对一些操作的封装，例如创建主题、创建分区

    19、创建groupCoordinator，组协调器，并调用它的startup方法，这个startup方法会启动一个调度任务：
delete-expired-group-metadata，用来清除过期的消费组

    20、创建transactionCoordinator，事务协调器，并调用它的startup方法，这个startup方法会启动一个调度任务：
transaction-abort，用来中断超时的事务

    21、如果配置指定了授权信息，则对授权进行初始化

    22、创建fetchManager，拉取管理器

    23、创建KafkaApis放在dataPlaneRequestProcessor变量里，这个类用来处理不同的api，里面有个handle方法，用了
switch跳到不同的方法处理不同的API请求

    24、创建KafkaRequestHandlerPool放在dataPlaneRequestHandlerPool变量里，这个对象在内部会启用多个线程，这些
线程会从RequestChannel里的请求队列中拿出请求，然后用KafkaApis处理请求，线程数量取决于num.io.threads参数，默认
为8。这个线程池是processor线程将连接处理成Request对象后，放进一个requestQueue队列中，由这个线程将Request拿出来
进行后续处理

    25、增加一系列的动态配置监听器与动态配置处理器，然后启动一个dynamicConfigManager动态配置管理器，并调用它
的startup方法，这个方法就是在zk的/config/changes节点注册一个监听器，然后启用一个线程，一直从配置变更队列里获取
事件并触发各种监听器

    26、启动控制面与数据面的processor线程，到了这里已经开始正式处理请求了

    27、然后设置kafka的各种状态，到这里基本已经启动完成了，最后注册mbean，输出个started日志，至此启动完毕



客户端生产者的消息生产流程：
    1、生产者调send方法进行消息发送：KafkaProducer.send
    2、一进来直接走生产者拦截器的onSend方法
     3、拉取集群的信息，也就是构建一个Cluster对象，这个对象包含了生产者感兴趣的topic对应的分区与节点信息，包括集
群控制器是哪个节点、当前集群里有哪些节点、每个节点里有哪些分区、每个topic有哪些分区、每个分区的leader是哪个节
点等等，都保存在一大堆Map里面，主要有两种对象：Node：节点信息，PartitionInfo：分区信息，每个对象代表一个分
区，包含了leader节点是哪个、副本在哪些节点、同步节点是哪些、非同步节点是哪些等信息
    4、通过key与value序列化器对key与value进行序列化，将它们转成byte数组
    5、如果消息未指定分区，则用分区器进行分区，默认是DefaultPartitioner分区器。它通过key进行分区。如果key为null
则通过一个topic独有的计数器进行自增与可用分区进行取模来分区。如果key不为null，则将key计算哈希，并与总分区数
进行取模来分区
    6、将record.headers()设置为只读，这意味着消息头的内容后续无法被修改
    7、估算出这条消息的大小，然后判断是否超出单条消息的最大限制或者生产者缓冲区大小
    8、将消息加入RecordAccumulator累加器
        8.1：为指定topic分区获取一个ProducerBatch泛型的队列，如果已有则拿旧的，否则创建新的
            8.2：尝试将消息加入队列的最后一个批次里面，如果它还有空间，也就是原大小(可能是压缩过的)+当前消息的大小小
    于batch.size。或者批次里连一个消息都没有，那么就不管大小，至少让批次放一个消息
        8.3：如果该队列里没有批次，则从BufferPool类型的free里分配一段ByteBuffer缓存出来，分配的内存在batch.size与
    当前消息大小之间取大的值。缓存池里的缓存是可以被复用的，它将在批次发送之后回收(待确定)
        8.4：创建一个新的ProducerBatch批次，并未它分配缓存内存，然后加入分区队列里，并尝试将消息加入这个批次，
     由于这是一个全新批次，因此这次总会成功。
        8.5：加入成功后会将future与用户的callback绑定在Thunk对象并保存在batch的thunks里，这个似乎是为了触发
    callback时保证消息回调的顺序与生产的顺序相同
        8.6：所有ProducerBatch的append都是同步代码块下进行，因此不会因为并发导致批次大小溢出
    9、sender线程，sender线程是在KafkaProducer的构造器里启动的
    10、sender线程的内容大概为：
        1、如果处于running状态下，则一直尝试消费RecordAccumulator累加器并发送消息
        2、如果不处于running状态，且不是强制关闭，且累加器里还有消息，则将累加器里剩余的消息发送完成
        3、如果是强制关闭，则中断累加器里所有的消息，就是批次不允许再追加、删除队列里所有批次、完成批次的future
    、回收分配的内存
        4、关闭客户端
    11、sendProducerData方法从累加器里获取批次并发送
        11.1：拿到累加器里所有分区的leader节点，如果没有leader且批次队列里有数据，则将topic加入
    unknownLeaderTopics，也就是无leader的topic集合
        11.2：判断队列里的第一个分区是否就绪，例如是否达到批次等待最大时间、批次是否已满、缓存池是否有线程在等待
    分配、累加器是否已被关闭等待，如果满足这些条件任意一个，且不是重试批次且小于重试间隔，就会将leader加入就绪
    节点中返回去，此外还有一个下次重试时间，它会取所有批次重试间隔里最短的返回去，用来跟单次请求间隔取最小值来
    进行nio的select(timeout)操作，而它默认是Integer.max，因此可发送的情况下用的是请求超时的参数
        11.3：因此上面两步操作返回的值包括：无leader的topic列表、批次可以发送的leader列表、下次select时间
        11.4：如果未知leader的topic列表不为空，则将对应的topic加入metadata中，并尝试去更新集群的元数据
        11.5：如果ready的leader节点列表不为空，则判断每个节点是否就绪，例如客户端中节点同时发送的最大请求是否达
    到上限、客户端的各种组件状态是否正常，例如selector与channel等等，如果客户端判断该节点未就绪则将该节点从就
    绪集合中移除，并根据节点的重连接时间配置来决定下次select的时间，依然是跟上面的下次select时间取最小值
        11.6：调用accumulator.drain方法，去拿到可用leader里所有可发送的批次
        11.7：isMuted方法是用来判断该分区是否有正在发送的消息，如果有说明配置要求保证消息顺序，因此只能等上次消
    息发送完成后才能发送，而这次跳过这个分区
        11.8：如果不是重试且未满足重试间隔、且未到达最大请求字节数，就从队列里poll出第一个批次，将这个批次关闭，
    然后加入批次就绪集合中返回。每次sender线程轮询都只会拿每个批次队列里的第一个批次，哪怕后面也有就绪的也要等
    下次
        11.9：如果要保证消息的顺序性，也就是guaranteeMessageOrder为true，也就是
    max.in.flight.requests.per.connection等于1，就会将分区加入累加器的muted集合中，在muted集合中已有的分区就不
    会发送消息，只能等上次发送完后从muted中清除对应的分区才能发下次
        11.10：查找累加器中的过期批次，比如批次创建时间大于请求最大等待时间，就会将批次删除，传入超时异常回调批
    次里的future、回收缓存、记录指标等等，通过这种方式进行的超时是不会重试的，因为它直接进行回调了
        11.11：更新生产者请求的各项指标
        11.12：如果有就绪的leader节点，就将下次select的时间设置为0
        11.13：调用sendProduceRequests正式发送批次消息
        11.14：遍历batches发送消息，这是个map，key为nodeId，value为批次队列，因此每次只给一个发送节点
        11.15：包装成一个RequestCompletionHandler callback对象，这个对象里有一个外部构建的map，key是TopicPartition
    ，value是此次该分区发送的批次，每次每个分区只会发送一个批次，回调里也包含了TopicPartition，因此可以根据响应
    从map里拿到批次，然后进行回调
        11.16：创建一个clientRequest对象，这个client是跟节点绑定的业务request，然后KafkaClient.send，这个client是
    共用的client
        11.17：将请求构建成一个InFlightRequest inFlightRequest对象，然后保存到inFlightRequests中，这里记录的是当
    前所有正在发送的请求
        11.18：以上只是将request注册到selectKey里，它们通过一个KafkaChannel对象来关联
        11.19：client.poll，进行select
        11.20：client.poll后，里面有个metadataUpdater.mybeUpdate，这是发送更新元数据请求的，还有个selector.poll
    ，这是处理就绪的SelectionKey
        11.20：select到key之后，就会调用pollSelectionKeys方法处理key，这个方法里面有个attemptRead(key)操作，这是
    用来将读到的数据转成NetworkReceive的，NetworkReceive就是响应数据的包装，然后保存到stagedReceives里
        11.21：selector.poll的最下一行会调用addToCompletedReceives，将所有receive都加入completedReceives里
        11.22：然后再client.poll里，调用handleCompletedReceives方法，将receive用AbstractResponse.parseResponse
    转成AbstractResponse对象，这个对象会根据api的key转成不同的响应类型，如果这些响应是元数据或者api版本响应，
    则去更新元数据与api版本的信息，否则就都转成ClientResponse类型加入List<ClientResponse> responses对象中
        11.23：在client.poll里，调用completeResponses方法去处理所有的responses，也就是调用它们的callback
        11.24：在handleCompletedReceives中，response和callback的关联保存在inFlightRequests集合里，这个集合保存
    了所有正在请求的对象，每个响应 都从对应的节点请求集合里拿一个最早发起的请求对象，那么怎么保证最早发起的请求
    一定是最早响应的。因为对于相同的node，每个KafkaProducer只会创建一个连接，因此响应都是顺序返回的。

    kafka缓存池的消息存储：
        每个批次都会分配一段内存，批次的发送结束后回收内存，内存指的是一个ByteByffer对象，消息写入批次时，会按照
    key、value、headers的顺序写入，先写入key的长度，再写key的内容，value也一样，headers则先写入header的数量，
    然后再写每个header的长度与内容

    metadata的更新：
        在client.poll里，有个metadataUpdater.maybeUpdate(now)方法，里面会调用maybeUpdate，里面会构建一个
    metadataRequest请求，然后注册到select上，这里会选择一个调用最少的节点进行元数据拉取

    ApiVersions：
        这里的Api指的是操作的类型，比如生产消息、心跳、元数据拉取等待，都在ApiKeys里，由于Kafka有多个不同的版本，
    因此通过从节点拉取api版本列表的确定broker支持哪些api的哪个版本

    错误重试：
        重试是在处理响应的callback时处理的，会将批次进行重新发送，直接加入累加器的队列中，且会加在头部。可以在
    sender的sendProduceRequest方法中看到，里面会创建一个RequestCompletionHandler callback用来构建客户端，这里
    面就包含了重试逻辑，比如重试次数判断，是否是重试异常等等

    好像过期的批次，进行回调后依然会进行发送？

        kafka发送消息时，NetworkClient的InFlightRequests正在发送队列满了咋办，此时会判断是否内部消息，比如元数据或API版
    本请求，如果是的话就硬发，如果不是，比如消息生产请求，就会抛出IllegalStateException异常
        由于sender线程在从RecordAccumulator里拿批次前就会判断该节点请求是否可用，如果可用才会拿，而sender是单线程的，能
    保证不会超，所以正常情况下是不会出现poll出了批次，请求又满了的情况，如果真的出现了，这次发送的所有批次应该都会因为
    报错导致丢失


服务端请求基础流程：
    1、先是Acceptor线程，先进行select，最多等待500ms，然后遍历select到的key，确认key可握手后，先进行握手
    2、有个connectionQuotas连接配额用来进行连接数限制，还可以配置每个监听器的最大连接数，不过默认是int.max，还
能通过max.connections参数来限制整个broker的总连接数，默认也是int.max，假设这两个连接数超过了就会一直阻塞，直到
有旧的请求结束对其进行唤醒。
    还有地址级别的请求限制，如果有通过max.connections.per.ip.overrides配置每个地址的最大连接数，则取地址配置的连接
数进行限制，如果该地址没有特意去配置，最大连接数就取max.connections.per.ip，当这个地址的连接数超过最大连接数时
，就会抛出TooManyConnectionsException异常，上层捕获到这个异常后会关闭scoket
    3、connectionQuotas的连接数限制通过后，就会返回selectionKey里的socketChannel，并设置它的一些属性
    4、将processor的数量放入retriesLeft变量，用来当做重试次数；然后用currentProcessorIndex对processor的数量进行
取模，这个变量每次都会+1，用来轮询所有的processor，然后就在超过重试次数之前一直轮processor，去调assignNewConnection
方法，如果返回false就代表请求处理完成，如果返回true就换下一个processor
    5、拿processor去执行assignNewConnection方法，这个方法里面就是将socketChannel放入processor的newConnections
，如果放成功就返回true，否则就看是不是轮询到最后一个processor了，如果不是最后一个直接返回false，如果是最后一个
则改用put方法，一直等到newConnections有空位。其实总的来说就是尽量放进一个有空间的processor，如果实在没有就在
最后一个processor硬等

    6、然后到Processor线程，首先执行selector.register，这个selector并不是nio的，而是kafka的。它的注册先校验这个连接
id是否已经存在它的连接池中了，如果是则抛出异常，id一般是不会重复的，因为每个processor都有一个index，每次新连接
都会+1，并且最大值是int.max，达到最大值会重置到0。第二部是将socketChannel注册到nioSelector中，并监听READ事件
，然后将连接id绑定到selectionKey中的attach，在将该连接放入连接池与空闲连接管理器中

    7.0、这里漏了个processNewResponses()方法，它会从responseQueue队列里拿响应出来发送响应数据，懒得改数字了，干脆
写个7.0

    7、开始poll方法，这个poll也是进行selectedKey，不同的是这里的Selector里面只注册了读事件，前面那个Selector里只
注册了握手事件
    8、poll方法里，如果连接已经可读，就会用nioSelector.selectdKeys方法select到一批可读的key放到readyKeys变量中，然后
用pollSelectionKeys()方法处理这批readyKeys可读key
    9、pollSelectionKeys()方法会遍历所有的key，首先在空闲管理器里更新这个连接id的活跃时间，然后期间一堆额外工作，正常
来讲会进入attemptRead方法中，就是处理可读的连接，这里面会执行一个channel.read()方法，就是将数据从流里读出来，然后一直
往里执行会走到一个receive.readFrom()方法，这个方法里会先读4b的数据，然后将这4b当做请求内容的大小去重新分配缓冲区，如果
分配成功就全读出来，如果分配失败就会将缓冲区置为Null，并且记录需要分配的内存，然后在上层的kafkaChannel.read中就会执行
mute()方法，这个方法会将kafkaChannel的状态改为MUTED，这意味着内存池没有足够的内存用来处理该请求，也就是将请求静默，然后
删除它的READ事件，然后将processor里的selector.outOfMemory设置为true。如果内存足够，则将kafkaChannel放入完成接收集合：completedReceives。这里的疑惑是kafka前面4b究竟是不是整个请求的长度？书上不是这么讲的

    10、然后是processCompletedReceives()方法，处理已经完全读完流的连接，这里只说正常情况下的流程，非正常的流程很简单，
就是抛异常，这里直接略过。
    10.1、执行RequestHeader.parse解析请求头，这里的解析过程与书上讲的协议格式一样，16位的apiKey、16为的apiVersion、然后
32位请求id，再读16位的cliendId长度，根据长度读数据。然后读取一个变长int的字段数量，再根据字段数量读取每个字段，放到集合
里，起名叫未知标记字段_unknownTaggedFields，不知道这是什么东西，这个有什么用？找不到注释网上也查不到作用
    10.2、然后用header与连接的信息创建一个RequestContext对象，再用RequestContext对象创建一个RequestChannel.Request对象
。再调用requestChannel.sendRequest()方法将req对象放入requestQueue队列中；再将这个连接静默，且将其加入
explicitlyMutedChannels集合中；最后执行一个handleChannelMuteEvent()方法将这个kafkaChannel的muteState静默状态改为
ChannelMuteState.MUTED_AND_RESPONSE_PENDING既静默且等待响应
    10.3、上面的requestQueue会被KafkaRequestHandler线程，这个线程就是每次拿一个req出来处理
    10.4、req分两种：ShutdownRequest与Request，一般而言都是Request，Request会直接跳转到KafkaApis.handle()方法

    11、再然后是processCompletedSends()方法，处理响应发送完成的一些操作，例如从正在响应列表中删除该链接、调用请求完成
回调、将连接改为非静默

    12、再然后调用processDisconnected()方法，将已关闭的连接的连接配额进行回收、

    13、最后是调用closeExcessConnections()方法，关闭多余的连接，这里的多余连接是因为最大连接数是支持动态配置的，因此
如果有大量请求的情况下突然改了配置，就会导致连接过多。当出现这种情况则会挑选一个连接，挑选逻辑是：先看正在关闭连接里
有没有连接，如果有就拿第一个；如果没有就看过期管理器里有没有连接，如果有就拿最快过期的；如果还没有就从连接集合里拿第
一个，这往往是最早的请求。拿到的连接如果不为空就对这个连接进行close
    为什么过期管理器IdleExpiryManager里第一个总是最快过期的，因为它里面是个LinkedHashMap，而后面put的数据每次都是刷新
过期时间的，因所以前面的就是最久没刷新过的，因此前面的总是最旧的


服务端的消息生产请求处理流程(ProduceRequest)：

    1、从前面的【服务端请求基础流程】的10.4开始，进入KafkaApis.handle()方法，里面会匹配到ApiKeys.PRODUCE，随
后进入handleProduceRequest()方法
    2、handleProduceRequest()方法会有一个让人难以理解的操作：request.body[ProduceRequest]，这个body点进去，其实返回的
是bodyAndSize对象的request属性，bodyAndSize对象其实早在Request对象构建时就解析好了，在构造器里干的活，往里面跟会发现
有一句AbstractRequest.parseRequest(apiKey, apiVersion, struct)，这里其实就是根据apiKey将初始的请求转成各种详细的类型，
然后到了request.body[ProduceRequest]只是做了个类型转换而已
    3、然后看这批消息里是否存在事务，如果存在则将事务id取出，并进行校验，如果校验不通过则抛出异常，异常已经在Errors类
定义好了，直接拿就好；否则就看是否幂等消息，如果是也进行校验，校验不通过也抛出一个Errors类的异常
    4、然后构建几个集合：unauthorizedTopicResponses(未授权的topic)、nonExistingTopicResponses(不存在的topic)、
invalidRequestResponses(消息无效，例如版本不匹配)、authorizedRequestInfo(授权通过)，这些集合都是map，用TopicPartition
当key
    5、然后就是构建响应回调方法sendResponseCallback()与统计回调方法processingStatsCallback()，这是两个闭包方法，后面
才会调，因此这里暂且略过
    6、如果授权通过集合authorizedRequestInfo是空的，则调用sendResponseCallback响应一个空Map，否则就调用副本管理器
replicaManager.appendRecords()方法添加消息
    7、最后将请求里的消息清空，以便让GC回收，因为执行完replicaManager.appendRecords()方法后，要么消息已经加入了日志文
件，要么消息已经放入了炼狱缓冲区，因此继续持有这个消息引用反而可能导致消息处理完后不能被GC回收
    8、消息响应流程，待看

副本管理器的消息追加：
    1、执行replicaManager.appendRecords()方法将进行消息追加，接消息生产请求的第6点，因为副本管理器算是一个较为独立的
模块，因此分开记录
    2、先看acks是否有效，既等于-1、1、0中任意一个值，如果有效就继续，否则响应InvalidRequiredAcksException异常
    3、执行appendToLocalLog()方法，这个方有4个参数：
        internalTopicsAllowed：代表是否允许操作内部主题，例如偏移量与事务状态，这个参数只有AdminClient才会为true
        origin：操作来源，类型为AppendOrigin，有三个子类，分别代表副本、协调器、客户端，生产者请求的话就是Client客户端
        entriesPerPartition：此次生产的消息
        requireAcks：acks，0、1、-1
    4、如果acks有效就调用appendToLocalLog()方法，然后遍历每个分区的消息，首先判断是否内部主题，如果是内部主题且
internalTopicsAllowed为false，则抛出InvalidTopicException异常，如果不是则继续执行日志追加
    5、通过getPartitionOrException()方法拿到当前分区的leader的分区对象，类型为Partition，副本管理器里的allPartitions
属性持有这个对象，如果allPartitions中没有这个分区，但是metadataCache又有，且要求返回leader的partition时既
expectLeader为true，就会返回NOT_LEADER_FOR_PARTITION分区没有leader错误
    6、调用partition.appendRecordsToLeader将消息追加到leader，这里面会根据Partition.eaderReplicaIdOpt是否等于当
前brokerId来判断当前broker是否该分区的leader，如果不是则抛出NotLeaderForPartitionException异常，表示当前broker
不是该分区的leader，如果当前broker是leader，则继续执行，这块逻辑调用之前会获取leaderIsrUpdateLock读写锁的读锁
    7、先获取配置的最小同步副本数量min.insync.replicas，默认为1，然后获取当前的同步副本数量，在
partition.inSyncReplicaIds属性里，这里保存了同步副本的brokerId集合，如果同步副本的数量少于配置要求的最小同步副本
，则抛出NotEnoughReplicasException异常，提示当前的同步副本无法满足最少同步副本的要求
    8、到了这一步就已经确定当前broker就是这个分区的leader了，然后调用partition.log对象的appendAsLeader方法，
这个方法里包了一层maybeHandleIOException()方法，当出现IO异常的时候就会将日志目录加入离线日志目录offlineLogDirs
与offlineLogDirQueue中
    9、继续往里面会调用analyzeAndValidateRecords()方法，这个方法会对消息进行验证。例如验证批次的基准偏移量baseOffset
是否从0开始、消息的偏移量是否单调递增的、单个批次的大小是否大于max.message.bytes、客户端的CRC32校验是否和服务
端一致、消息压缩算法是否支持，以上这些校验只是例举了部分，然后将以上信息封装成一个LogAppendInfo对象返回
    10、然后调用trimInvalidBytes()方法对消息进行修剪，什么时候会出现无效字节需要修剪
    11、然后已经确认所有消息都是有效的，加锁，准备追加到日志里面，然后通过isMemoryMappedBufferClosed判断内存
映射缓冲区是否已关闭
    12、然后通过Log.nextOffsetMetadata为LogAppendInfo对象分配下一个偏移量，分配后通过LogValidator.validateMessagesAndAssignOffsets
对这个偏移量与时间戳校验，例如为批次设置最大偏移量、最大时间戳等等
    13、然后根据LogValidator.validateMessagesAndAssignOffsets方法返回的对象的messageSizeMaybeChanged判断消
息大小是否有变更，如果为true则重新校验消息大小
    14、看看是否需要更新leaderEpoch，当请求的leaderEpoch与broker现有的leaderEpoch不一致时，就会将broker里
leaderEpoch大于请求的leaderEpoch且startOffset大于请求的startOffset的epoch实体删掉，当出现删除epoch实体时，就
说明出现了消息乱序，当epochs出现变更时就会将epochs集合的数据写入该分区的leader-epoch-checkpoint检查点文件中
    但是为什么LE会以客户端传过来的为准？
    15、校验单个消息的大小是否超过segment.bytes，既日志片段大小，然后判断当前这个活动片段的大小跟有效期是否已经
超出配置，如果超出就返回一个新的分段，否则返回activeSegment
    16、用批次的偏移量、分段的开始偏移量、分段下次写入的相对位置来构建一个LogOffsetMetadata对象
    17、然后通过analyzeAndValidateProducerState方法验证幂等与事务的状态与收集元数据，如果存在重复批次，也就是这
个批次中消息的起始序列号与结束序列号与batchMetadata中任意一个缓存的批次序号一致，并且这是客户端发起的生产请求
，则代表客户端在重试的过程中出现了消息重复，那么就会将缓存的那个批次的信息放进appendInfo中，然后直接返回
appendInfo，下面就不执行了。
    这里需要了解的是，scals里的foreach是直接返回当前方法的，而不是退出froeach
    producerStateManager.ProducerStateEntry.batchMetadata的大小限制了是5，如果超过5个就会将最旧的元素取出，
这也解释了生产者客户端max.in.flight.requests.per.connection参数为什么不能大于5。因为如果超出5个请求同时发送，就会
造成客户端发了一个请求失败，然后进行重试，第二次发过来时服务器已经把这个批次请求的缓存给清了，找不到相同序列号
的缓存就会认为这个请求是一个新的请求，造成幂等环境下也会出现重复生产
    上面这些逻辑是在有productId时才会进行，因为没有productId代表没开启幂等或事务，无需保证消息的不重复
    那为什么不用一个自增的序号来保证？如果同时发送3个请求，第一个请求因为数据错误被拒绝了，那后面两个请求如果要
严格按照序号进行校验，那肯定是失败的，因为前面中断了，前面失败的请求客户端可能会重试，如果影响到后面的请求是不
合理的，并且就算根据错误类型来判断客户端是否会重试来进行拦截，客户端也可能因为重试次数的原因对可重试异常也不进
行重试。因此维护一个短期内的序号列表更加合理，前面的请求失败也不影响后面的，而且能保证消息不重复生产
    那么如果第一个请求失败了，但是重试间隔又设置了很长，然后后面的请求一直进来，一直请求成功岂不是会将第一次请求
的缓存给顶出来，导致第一个请求再次进来找不到缓存而造成重复生产？因此想要一定保证幂等，就将最大请求数量设置为1
    18、调用segment.append()将日志加入片段文件中，这里的加入只是将消息写入FileChannel中，FileChannel在调用force
方法时会进行刷盘，则LogManage在startup()方法中会启动一系列的定时任务，其中一个任务名叫kafka-log-flusher，这个
任务每隔30S会对所有日志进行一次刷盘。然后根据index.interval.bytes属性判断是否需要更新offsetIndex和timeIndex两个
索引文件的索引信息
    19、更新LEO和HW，这两个点只是在内存中更新，会有定时任务将其保存到磁盘中
    20、更新生产者状态，也就是将此次消息的信息加入producerStateManager里produceId对应的队列中
    21、如果此次生产的消息批次是控制批次，且是中断消息，则将控制消息插入日志索引文件中
    22、更新producerStateManager里的最后一个偏移量为当前批次的最大偏移量+1
    23、调用maybeIncrementFirstUnstableOffset()方法记录LSO，既firstStableOffset，它代表未完成事务第一条消息的偏移
量，如果开启消息的话LAG就等于HW - LSO，否则LAG就等于HW - 分区消费组的偏移量
    24、然后判断缓冲区里消息的数量是否超出flush.messages的配置，如果超出则调用flush()方法进行刷盘
    25、至此消息已添加完毕，回到第4点，既执行appendToLocalLog()之后，将每个分区返回的信息转为PartitionResponse
对象，然后调用recordConversionStatsCallback()方法对响应信息进行统计
    26、然后看是否需要延迟进行请求响应，因为可能需要等同步到其它副本，判断条件是：acks是否为-1、添加的消息不为空
、添加结果的异常数量小于添加的数量(因为如果不小于就说明所有消息全是异常，没必要延迟)
    27、如果无需延迟，则直接调用responseCallback，如果需要延迟，则创建一个DelayedProduce对象，然后调用炼狱：
delayedProducePurgatory.tryCompleteElseWatch()，这个方法会看看是否能不用延迟等待而立即完成这个请求，如果不能
则将请求放入炼狱缓冲区
    是否可以立即完成则是判断当前的isr是否能满足配置的min.insync.replicas，并且HW达到了这个消息的偏移量，如果能满
足则认为这个延迟操作可以完成，否则就加入缓冲区延迟等待。延迟等待有个watchKeys，这个watchKeys就是此次生产消息
的所有topic与分区
    每个副本管理器replicaManager都有4个炼狱对象，分别是delayedProducePurgatory(延迟生产)、delayedFetchPurgatory
(延迟拉取)、delayedDeleteRecordsPurgatory(延迟删除记录)、delayedElectLeaderPurgatory(延迟选举leader)，他们在不同
的变量中，且DelayedOperationPurgatory对象的泛型也不相同。这里是生产消息的逻辑，因此延迟消息的类型就是
DelayedProduce，用的也是delayedProducePurgatory对象
    28、遍历watchKeys，将每个key传入watchForOperation()方法，这里其实就是会拿到key对应的监听器watcher，然后加
入监听集合operations，然后再次尝试完成，如果还完成不了则加入timeoutTimer，这个对象是SystemTimer类型，它里面
有个TimingWheel对象，也就是时间轮，接下来就不管了，等待时间轮到时间超时或者操作被触发然后进行回调
    29、炼狱监听器与时间轮另外讲，这里先讲responseCallback的逻辑，DelayedProduce操作在完成后会触发onComplete
()方法，这个方法会调用responseCallback
    30、然后各种类型转换，最终转成一个RequestChannel.Response对象，并加入processor的responseQueue队列中
    31、processor的run()方法里会执行processNewResponses()，这里会把的responseQueue队列里的元素拿出来，然后给
KafkaChannel监听写事件SelectionKey.OP_WRITE，并给正在响应的集合加一个元素inflightResponses，key就是请求id
    32、继续processor的run()方法，跟可读事件一样，在poll()里面的selector.pollSelectionKeys()方法中会执行attemptWrite
方法，如果该key可写就开始进行响应。执行write方法，然后调用channel.write()进行写，再把channel关联的send加入
completedSends集合中
    33、还是processor的run()方法中，里面会执行processCompletedSends()，这个方法会遍历completedSends，把所有
发送完成的响应从inflightResponses里删掉。后面就是一些后续处理了，例如响应完成回调(默认没有)，更新统计，取消请求
静音等等
    34、kafka发送消息与响应结束后，不会关闭连接，因为kafka的连接是复用的，因此收到请求后进行响应，响应结束就等待
该连接的下一次可读事件

炼狱缓冲区(DelayedOperationPurgatory)：
    炼狱缓冲区的类名叫：DelayedOperationPurgatory，它的作用是处理延迟消息，例如延迟生产、延迟拉取、延迟选举、
延迟删除，不同的延迟操作表现在它的泛型中，延迟操作必须实现DelayedOperation抽象类，例如延迟生产消息就是
DelayedProduce
    而延迟消息有两种方式完成，一个是事件触发，例如延迟生产消息一般是在等待isr集合有足够的副本与HW达到要求，当有
新的副本加入isr集合或者分区HW增加时，就会触发时间让DelayedProduce去尝试完成，如果能完成则调用对应的callback方
法；另一种完成方式是超时，每个DelayedOperationPurgatory炼狱都有一个SystemTimer类型的timeoutTimer属性，这个
SystemTimer类里有一个timingWheel属性，这就是时间轮了，每次调用tryCompleteElseWatch方法时，都会先尝试延迟操
作能否立即完成，如果不能则将延迟操作加入timingWheel中

炼狱运行流程：
    1、调用operation.tryComplete()去尝试将延迟操作完成，如果能完成则直接返回，如果是生产延迟操作则是判断isr能否满
足参数配置所需的最少isr，能满足就返回true，宣告完成成功，后续逻辑无需继续
    2、如果无法完成，这遍历所有需要监听的key，将每个分区加入watcherLists里对应key的watcher中进行监听
    3、加入监听器后再次进行尝试，这次调用的是operation.maybeTryComplete()方法，这个方法与tryComplete()的区别是
，maybeTryComplete()会先拿到延迟操作的锁再调用tryComplete()，这个方法如果拿到锁就去尝试完成，拿不到锁就将重试
设置成true让之前持有锁的线程再次进行尝试完成
    4、如果到这里还无法完成，则判断timerEnabled是否为true，这个值默认为true。然后将延迟操作加入timeoutTimer中
    5、至此延迟操作已经加入操作监听器与超时定时器中，两种方式任一种触发都可以让它尝试完成

操作监听器(Watchers)：
    1、操作监听器是DelayedOperationPurgatory里的一个watcherLists属性，这是一个集合，保存了所有分区的监听器，里
面具体的监听器是Watchers对象
    2、所有会使HW增加的操作都会触发监听器，并尝试完成延迟操作，完成的方法在Partition.tryCompleteDelayedRequests
()方法中，这个方法所有调用的地方都有一个leaderLWIncremented条件，既leader的HW增加，炼狱在Partition创建的时候
就从传进来的replicaManager对象里面被取出来构建成了delayedOperations对象，因此直接调用delayedOperations.
checkAndCompleteAll()方法就会触发生产延迟、消费延迟、删除延迟等炼狱的checkAndComplete方法
    3、这个方法会拿到对应分区对应的watcher，并调用它的tryCompleteWatched()方法
    4、warchers.tryCompleteWatched()会遍历它里面operations对象的所有元素，并执行它们的maybeTryComplete()方法
如果执行成功则删除该延迟操作，这个方法在上面已经说过了
    5、HW增加的地方有：成为leader、成为追随者、isr减少、消息增加等

定时器(SystemTimer)：
    属性：
        tickMs：当前轮每个刻度的时间跨度
        wheelSize：当前轮的总刻度
        interval：当前轮总时间跨度，tickMs * wheelSize
        buckets：任务列表集合
        currentTime：当前时间，这里的时间用的是TimeUnit.NANOSECONDS.toMillis(System.nanoTime())，也就是系统时
    间到现在为止运行的时间，这个值等于startMs - (startMs % tickMs)，这是为了把刻度后多余的值消掉，例如刻度是200，
    startMs是3215，那currentTime就会变成3200
        overflowWheel：父时间轮，如果有时间超过当前所有的刻度，则创建一个父时间轮并将该任务加入父时间轮
        queue：优先级队列，一个SystemTimer里所有的时间轮都公用一个队列

    1、调用add()方法加入一个TimerTask类型的任务，其中DelayedProduce继承了DelayedOperation，而DelayedOperation
又继承了TimerTask，因此直接把延迟操作传进去就行了
    2、将延迟操作与它的超时时间当构造器参数创建一个定时器任务实体TimerTaskEntry对象，超时时间应该是绝对时间
    3、将定时器任务实体对象加入时间轮timingWheel，如果加入失败则说明该任务已经过期或取消，如果取消失败则将其加
入一个taskExecutor线程池中，这个线程池只有一个线程，且是无界队列，因为加入这个线程池的任务说明该任务已经没必要
再等了，DelayedOperation.run()方法会调用forceComplete()进行强行完成，既取消该定时任务与调用callback
    4、然后看看timingWheel.add()，返回值是boolean，意思是添加进时间轮是否成功。每个任务都有一个过期时间，这个
方法会判断任务对象是否已取消，如果已取消则返回false；然后判断是否已过期，如果过期时间小于当前时间+tickMs，则
说明已过期；然后判断过期时间是否小于当前时间+当前轮的总间隔，如果小于则加入当前时间轮；最后就是过期时间超过
当前轮的大小，如果没有父轮就创建父轮，然后加入父轮
    这里很容易疑惑的地方就是加入当前轮的逻辑，它会先用过期时间/tickMs，然后拿结果去对wheelSIze进行取模拿到bucket
，最后将任务加入该bucket，然后设置bucket的过期时间为过期时间/tickMs*tickMs，如果时间有变更则将bucket加入queue
中。一个时间轮不是有很多过期时间吗，不同的过期时间进行取模总会拿到相同的bucket，然后设置过期时间不是会一直变更
，然后重复将bucket加入queue吗？
    因为每次过期时间都等于expiration / tickMs * tickMs，因此如果是在同一个范围内的tickMs，最终过期时间是一样的，不
会发生变更。那么什么情况下会拿到相同bucket里不同的过期时间呢？
    如果想要bucket的过期时间有变更，expiration就必须增加超过tickMs，否则就会被先除后乘给过滤掉后面多余的值。然后
bucket的获取方式是buckets[expiration / tickMs % wheelSIze]，在expiration增加超过tickMs的情况下，如果还想要获取
相同的bucket，那么expiration起码要增加wheelSIze(20)个tickMs才能达到取模的下一个相同值。当expiration已经增加
wheelSIze(20)个tickMs时，那么这个时间轮肯定已经被执行过完整的一轮了，此时重新设置它的expiration并加入queue是没
有任何问题的，因为这个bucket肯定已经被queue延迟队列给清理出来了，所以不存在有重复bucket再同一个queue里的情况
    5、每个炼狱内部都有一个ExpiredOperationReaper(过期操作收割机)，这个线程在炼狱实例化时就会启动，它会调用炼狱
的advanceClock方法，然后进入timeoutTimer.advanceClock，最终会调用delayQueue的poll，poll等待时间是固定的200ms
，如果能poll一个bucket出来，则调用timingWheel.advanceClock(bucket.getExpiration())，再bucket.flush(reinsert)。
    6、其中timeoutTimer.advanceClock()方法，如果过期时间大于当前时间的第一个刻度，就会将当前时间置为：
timeMs - (timeMs % tickMs)，也就是过期时间减去tickMs除不尽的部分，然后一直往父轮更新，这样能保证下一次有延迟
操作进来，前面的轮依然能生效
    7、而bucket.flush(reinsert)方法，会将bucket里的任务清掉，并尝试重进加入时间轮，这个任务往往都是过期的，因此时
间轮里不会加进去，最后还是会加入taskExecutor线程池中，执行取消任务，然后将bucket的过期时间置为-1，这样下来这个
currentTime更新了，bucket里的任务与过期时间也重置了，下次还有任务进来这个bucket就能够被复用了
    8、哪怕currentTime与expiration会随着时间一直增加，进来这个时间轮的expiration一定会保持在20个tickMs内，否则就
去上个时间轮了，只要保证在20个tickMs范围内，那么expiration / tickMs操作就只会出现20种值，这正好对应20个bucket，
只要currentTime不变这20个tickMs的范围就不会有变化，那么每次加入的过期时间只要进了这个时间轮，在expiration / tickMs * tickMs
的算法下都能保证bucket的expiration不会重复，假设currentTiem发生了变化，那么一定会伴随着bucket的重置，这种情况
下expiration是否会重复根本无所谓


客户端消费者的消息拉取流程：
    1、消费者拉取消息要不断调用kafkaConsumer.poll方法
    2、kafkaConusmer是不支持并发的，它在很多操作的入口都会调用acquireAndEnsureOpen()方法，这个方法其实就是
将线程id放到kafkaConsuemr.currentThread属性里面，，如果currentThread已有值并且与当前线程id与不匹配则抛出
ConcurrentModificationException异常，在方法执行结束时调用release()方法释放currentThread属性
    3、校验拉取超时时间是否为负数、是否没有订阅任何主题分区，有则报错
    4、判断是否要退出consumer，主要是根据ConsumerNetworkClient.wakeup属性来判断是否要出异常，用户可以通过
kafkaConsumer.wakeup()方法来将它设置为true，等到下一次poll()就会引发WakeupException异常从而达到优雅退出
    5、根据includeMetadataInTimeout是否为true来用不同的方式去调用updateAssignmentMetadataIfNeeded()方法，
如果为true，那么调用这个方法的过程中也会算在超时时间内，否则就不算在内。includeMetadataInTimeout()方法的作用
有有以下几点：1、加入组协调器，2、分区重分配，3、初始化消费者的偏移量，4、异步提交偏移量。这几点会按需进行，
如果includeMetadataInTimeout为true，那么在超时时间内无法完成这几点，该方法会返回false，然后consumer.poll()方法
就会返回一个空消息集合，如果为false，则一直重试直到返回true
    6、然后就到了pollForFetches()方法，这个方法会开始拉取消息
    7、在pollForFetches()方法中，会先调用fetcher.fetchedRecords()尝试从已有的响应里拉消息，所有拉取响应都放在
completedFetches属性中，消费者会先看completedFetches里是否已有响应可用，如果有，则从已有的数据里拿够
max.poll.records配置的数量，如果数据量超出max.poll.records，则只返回max.poll.records要的部分，否则有多少返回多少
    为什么要分成completedFetches与nextInLineRecords两个属性去存放消息呢？因为考虑到某个拉取完成响应里上次只被取
走一半，那么这个迭代器应该保留下来，下次再拉取就从上次的位置拉取
    8、如果上一步的fetcher.fetchedRecords()里拿不到任何消息，就执行fetcher.sendFetches()，这个方法会尝试发送拉取
请求，里面会使用prepareFetchRequests()方法来构建请求，这个方法里，如果该分区已经在客户端有未被消费的数据，则不
会构建该分区的请求；或者有正在发送的请求也不会再次构建该分区的请求，构建完后就使用client.send发送请求
    在consumerNetwordClient中，有一个unsent，代表未发送的请求，这个未发送请求在fetcher.fetchedRecords()会进行
插入，在真正调用kafkaClient.send时进行删除
    9、中间丢失的序号请忽略，懒得改了
    11、执行client.poll()
    11.1、poll()里第一步是执行firePendingCompletedRequests()，这个方法里会遍历pendingCompletion里所有的元素
，并调用它的fireCompletion()方法，这个方法会将响应插入completedFetches中，而pendingCompletion的数据是
kafkaClient.poll()方法中，调用completeResponses时放进去的。因此第一步就是将响应的数据从pendingCompletion中转移到
completeResponses
    为什么一进来就要调这个方法呢，poll结束时不是调过吗，因为client.poll可能会在其它地方被单独调用，这种情况下
pendingCompletion中的数据就未转移到completeResponses中，因此先进来调一次。并且这个方法还会唤醒selector，不调用
这个方法可能会导致client.poll拿不到数据
    11.2、poll()里的第二步是执行handlePendingDisconnects()，这个方法会将pendingDisconnects集合中所有的节点相关的
请求全都触发失败回调，包括已发送与未发送的，然后设置该节点的相关状态，并关闭连接
    11.3、然后再调用trySend()，trySend()里，会去遍历unsent里的所有请求，并使用KafkaClient进行发送，在里面就是将
channel绑定send，然后注册写事件，等待poll到写事件等逻辑
    11.4、根据pendingCompletion与completedFetches里是否有数据可以获取来判断是否要进行有超时时间的poll，假设这两个
集合中都是空的，则会发生阻塞，阻塞时间是在最大拉取时间、服务端的连接限制时间、组协调器的下一次心跳时间、
retry.backoff.ms(连接重试间隔)时间之间取最小值。如果两者任一个不为空，则调用一次非阻塞poll
    11.5、然后调用checkDisconnects()方法，遍历unsent中所有的节点，判断是否存在连接断开的节点，如果存在则取出这个
节点的请求，并触发它的回调
    11.6、判断wakeup的状态，如果为true，且wakeupDisabled为false(默认是false)，则抛出WakeupException异常，就是判断
是否已被客户端退出的
    11.7、调用maybeThrowInterruptException()，判断线程是否被中断，如果被中断则抛出InterruptException
    11.8、再次调用trySend()，因为此时可能缓冲区空间已被清除或者轮询中的连接已完成
    11.9、调用failExpiredRequests()，遍历unsent里所有的请求，将已超时的请求剔除，并触发一个附带超时异常的fail回调
    11.10、清除unsent集合，以防止这个集合无限增长
    11.11、再调用一次firePendingCompletedRequests()方法，此方法在11.1中介绍过
    11.12、最后调用metadata.maybeThrowAnyException()，用来处理元数据更新期间遇到的不可重试异常，如果有这种情况，
则抛出
    12、看组协调器是否需要挂起或者重新加入，如果需要则返回一个空集合
    13、再次调用fetcher.fetchedRecords()尝试从已完成的集合里拿到消息返回，因为经过poll之后，消息可能已经有了
    14、续第6点，消息拉取到之后，如果消息不为空，就调用fetcher.sendFetches()将依然可拉取的分区再进行一次拉取
，如果再次拉取后存在发送中的请求，则调用一次非阻塞的poll，也就是client.pollNoWakeup()
    15、然后调用interceptors.onConsume，也就是调用消费拦截器，且将拦截器返回的结果返回给消费者
    16、如果消息拉取不到，且已用时间未达到超时时间，就继续上面的循环，直到拉取到消息或者到达超时时间，超时
后依然拉不到消息则返回空集合
    17、最后调用release释放线程锁


服务端的消息拉取请求处理流程(FetcherRequest)：

    1、从前面的【服务端请求基础流程】的10.4开始，进入KafkaApis.handle()方法，里面会匹配到ApiKeys.PRODUCE，随
后进入handleFetchRequest()方法
    2、进来会基于请求里的数据创建fetchRequest、fetchContext、clientMetadata等信息
    3、然后准备两个集合：erroneous与interesting，分别代表错误的分区集合与感兴趣的分区集合，接下来处理方式分为副
本的拉取请求与消费者的拉取请求
        3.1：其中如果fetch请求是foollower发起的，则校验该broker是否有权限访问。如果有权限，则遍历每个请求的分区是
    否在metadataCache中来决定是放在错误还是感兴趣的集合，这里错误就是UnknownTopicOrPartitionException异常；
    如果没权限，则将请求的所有分区都放入错误集合中，这里的错误就是TOPIC_AUTHORIZATION_FAILED异常
        3.2：如果fetch请求是消费者发起的，则验证消费者是否有权限访问请求的每一个分区，然后将验证不通过的分区放入
    错误集合，且异常是TOPIC_AUTHORIZATION_FAILED，验证通过后再看每一个分区是否存在于metadataCache中，如果
    不存在则继续放入错误集合中，且异常是UnknownTopicOrPartitionException，剩余的所有分区都放入感兴趣集合
        3.3：消费者与副本拉取请求在这里的区别就是，副本的权限是针对所有分区的，而消费者的权限可以精确到每一个分区
    4、然后是配额管理器QuotaManagers quotas，如果是副本的拉取请求，就是无限配置，如果是消费者的拉取请求，就是
根据quotas.fetch.getMaxValueInQuotaWindow获取该消费者的最大配额信息
    5、然后在fetchRequest里的maxBytes与服务端的fetch.max.bytes配置之间取一个最小值来当fetchMaxBytes，然后再从
fetchRequest.minBytes与fetchMaxBytes中取最小值来当fetchMinBytes
    6、如果感兴趣的集合interesting是空的，就直接进行callback，否则就调用replicaManager.fetchMessages
    7、























