
客户端：


生产者：

    生产流程：
        1、生产者调send方法进行消息发送：KafkaProducer.send
        2、一进来直接走生产者拦截器的onSend方法
        3、拉取集群的信息，也就是构建一个Cluster对象，这个对象包含了生产者感兴趣的topic对应的分区与节点信息，包括集
    群控制器是哪个节点、当前集群里有哪些节点、每个节点里有哪些分区、每个topic有哪些分区、每个分区的leader是哪个节
    点等等，都保存在一大堆Map里面，主要有两种对象：Node：节点信息，PartitionInfo：分区信息，每个对象代表一个分
    区，包含了leader节点是哪个、副本在哪些节点、同步节点是哪些、非同步节点是哪些等信息
        4、通过key与value序列化器对key与value进行序列化，将它们转成byte数组
        5、如果消息未指定分区，则用分区器进行分区，默认是DefaultPartitioner分区器。它通过key进行分区。如果key为null
    ，则通过一个topic独有的计数器进行自增与可用分区进行取模来分区。如果key不为null，则将key计算哈希，并与总分区数
    进行取模来分区
        6、将record.headers()设置为只读，这意味着消息头的内容后续无法被修改
        7、估算出这条消息的大小，然后判断是否超出单条消息的最大限制或者生产者缓冲区大小
        8、将消息加入RecordAccumulator累加器
            8.1：为指定topic分区获取一个ProducerBatch泛型的队列，如果已有则拿旧的，否则创建新的
            8.2：尝试将消息加入队列的最后一个批次里面，如果它还有空间，也就是原大小(可能是压缩过的)+当前消息的大小小
        于batch.size。或者批次里连一个消息都没有，那么就不管大小，至少让批次放一个消息
            8.3：如果该队列里没有批次，则从BufferPool类型的free里分配一段ByteBuffer缓存出来，分配的内存在batch.size与
        当前消息大小之间取大的值。缓存池里的缓存是可以被复用的，它将在批次发送之后回收(待确定)
            8.4：创建一个新的ProducerBatch批次，并未它分配缓存内存，然后加入分区队列里，并尝试将消息加入这个批次，
        由于这是一个全新批次，因此这次总会成功。
            8.5：加入成功后会将future与用户的callback绑定在Thunk对象并保存在batch的thunks里，这个似乎是为了触发
        callback时保证消息回调的顺序与生产的顺序相同
            8.6：所有ProducerBatch的append都是同步代码块下进行，因此不会因为并发导致批次大小溢出
        9、sender线程，sender线程是在KafkaProducer的构造器里启动的
        10、sender线程的内容大概为：
            1、如果处于running状态下，则一直尝试消费RecordAccumulator累加器并发送消息
            2、如果不处于running状态，且不是强制关闭，且累加器里还有消息，则将累加器里剩余的消息发送完成
            3、如果是强制关闭，则中断累加器里所有的消息，就是批次不允许再追加、删除队列里所有批次、完成批次的future
        、回收分配的内存
            4、关闭客户端
        11、sendProducerData方法从累加器里获取批次并发送
            11.1：拿到累加器里所有分区的leader节点，如果没有leader且批次队列里有数据，则将topic加入
        unknownLeaderTopics，也就是无leader的topic集合
            11.2：判断队列里的第一个分区是否就绪，例如是否达到批次等待最大时间、批次是否已满、缓存池是否有线程在等待
        分配、累加器是否已被关闭等待，如果满足这些条件任意一个，且不是重试批次且小于重试间隔，就会将leader加入就绪
        节点中返回去，此外还有一个下次重试时间，它会取所有批次重试间隔里最短的返回去，用来跟单次请求间隔取最小值来
        进行nio的select(timeout)操作，而它默认是Integer.max，因此可发送的情况下用的是请求超时的参数
            11.3：因此上面两步操作返回的值包括：无leader的topic列表、批次可以发送的leader列表、下次select时间
            11.4：如果未知leader的topic列表不为空，则将对应的topic加入metadata中，并尝试去更新集群的元数据
            11.5：如果ready的leader节点列表不为空，则判断每个节点是否就绪，例如客户端中节点同时发送的最大请求是否达
        到上限、客户端的各种组件状态是否正常，例如selector与channel等等，如果客户端判断该节点未就绪则将该节点从就
        绪集合中移除，并根据节点的重连接时间配置来决定下次select的时间，依然是跟上面的下次select时间取最小值
            11.6：调用accumulator.drain方法，去拿到可用leader里所有可发送的批次
            11.7：isMuted方法是用来判断该分区是否有正在发送的消息，如果有说明配置要求保证消息顺序，因此只能等上次消
        息发送完成后才能发送，而这次跳过这个分区
            11.8：如果不是重试且未满足重试间隔、且未到达最大请求字节数，就从队列里poll出第一个批次，将这个批次关闭，
        然后加入批次就绪集合中返回。每次sender线程轮询都只会拿每个批次队列里的第一个批次，哪怕后面也有就绪的也要等
        下次
            11.9：如果要保证消息的顺序性，也就是guaranteeMessageOrder为true，也就是
        max.in.flight.requests.per.connection等于1，就会将分区加入累加器的muted集合中，在muted集合中已有的分区就不
        会发送消息，只能等上次发送完后从muted中清除对应的分区才能发下次
            11.10：查找累加器中的过期批次，比如批次创建时间大于请求最大等待时间，就会将批次删除，传入超时异常回调批
        次里的future、回收缓存、记录指标等等，通过这种方式进行的超时是不会重试的，因为它直接进行回调了
            11.11：更新生产者请求的各项指标
            11.12：如果有就绪的leader节点，就将下次select的时间设置为0
            11.13：调用sendProduceRequests正式发送批次消息
            11.14：遍历batches发送消息，这是个map，key为nodeId，value为批次队列，因此每次只给一个发送节点
            11.15：包装成一个RequestCompletionHandler callback对象，这个对象里有一个外部构建的map，key是TopicPartition
        ，value是此次该分区发送的批次，每次每个分区只会发送一个批次，回调里也包含了TopicPartition，因此可以根据响应
        从map里拿到批次，然后进行回调
            11.16：创建一个clientRequest对象，这个client是跟节点绑定的业务request，然后KafkaClient.send，这个client是
        共用的client
            11.17：将请求构建成一个InFlightRequest inFlightRequest对象，然后保存到inFlightRequests中，这里记录的是当
        前所有正在发送的请求
            11.18：以上只是将request注册到selectKey里，它们通过一个KafkaChannel对象来关联
            11.19：client.poll，进行select
            11.20：client.poll后，里面有个metadataUpdater.mybeUpdate，这是发送更新元数据请求的，还有个selector.poll
        ，这是处理就绪的SelectionKey
            11.20：select到key之后，就会调用pollSelectionKeys方法处理key，这个方法里面有个attemptRead(key)操作，这是
        用来将读到的数据转成NetworkReceive的，NetworkReceive就是响应数据的包装，然后保存到stagedReceives里
            11.21：selector.poll的最下一行会调用addToCompletedReceives，将所有receive都加入completedReceives里
            11.22：然后再client.poll里，调用handleCompletedReceives方法，将receive用AbstractResponse.parseResponse
        转成AbstractResponse对象，这个对象会根据api的key转成不同的响应类型，如果这些响应是元数据或者api版本响应，
        则去更新元数据与api版本的信息，否则就都转成ClientResponse类型加入List<ClientResponse> responses对象中
            11.23：在client.poll里，调用completeResponses方法去处理所有的responses，也就是调用它们的callback
            11.24：在handleCompletedReceives中，response和callback的关联保存在inFlightRequests集合里，这个集合保存
        了所有正在请求的对象，每个响应 都从对应的节点请求集合里拿一个最早发起的请求对象，那么怎么保证最早发起的请求
        一定是最早响应的。因为对于相同的node，每个KafkaProducer只会创建一个连接，因此响应都是顺序返回的。
            

    kafka缓存池的消息存储：
        每个批次都会分配一段内存，批次的发送结束后回收内存，内存指的是一个ByteByffer对象，消息写入批次时，会按照
    key、value、headers的顺序写入，先写入key的长度，再写key的内容，value也一样，headers则先写入header的数量，
    然后再写每个header的长度与内容

    metadata的更新：
        在client.poll里，有个metadataUpdater.maybeUpdate(now)方法，里面会调用maybeUpdate，里面会构建一个
    metadataRequest请求，然后注册到select上，这里会选择一个调用最少的节点进行元数据拉取

    ApiVersions：
        这里的Api指的是操作的类型，比如生产消息、心跳、元数据拉取等待，都在ApiKeys里，由于Kafka有多个不同的版本，
    因此通过从节点拉取api版本列表的确定broker支持哪些api的哪个版本

    错误重试：
        重试是在处理响应的callback时处理的，会将批次进行重新发送，直接加入累加器的队列中，且会加在头部。可以在
    sender的sendProduceRequest方法中看到，里面会创建一个RequestCompletionHandler callback用来构建客户端，这里
    面就包含了重试逻辑，比如重试次数判断，是否是重试异常等等

    好像过期的批次，进行回调后依然会进行发送？

        kafka发送消息时，NetworkClient的InFlightRequests正在发送队列满了咋办，此时会判断是否内部消息，比如元数据或API版
    本请求，如果是的话就硬发，如果不是，比如消息生产请求，就会抛出IllegalStateException异常
        由于sender线程在从RecordAccumulator里拿批次前就会判断该节点请求是否可用，如果可用才会拿，而sender是单线程的，能
    保证不会超，所以正常情况下是不会出现poll出了批次，请求又满了的情况，如果真的出现了，这次发送的所有批次应该都会因为
    报错导致丢失


消费者：

        假设消费者消费到的偏移量是7，那么此时提交的偏移量就是8，也就是提交的偏移量总是会比消费到的+1，在日志里的
    偏移量也一样

        由于消费者是线程不安全的，因此大部分操作内部都会先调用acquire()，结束时调用release()，这是一个加锁操作，通过
    CAS获取当前线程id进行加锁，如果CAS失败，就代表多个线程在同时操作consumer，就会抛出异常，提示节点未准备好发送请求




AR(Assigned Replicas)：
    分区中所有的副本统称为AR

ISR(In-Sync Replicas)：
    同步副本

OSR(Out Of Sync Replicas)：
    非同步副本

LEO(Log End Offset)：
    标识每个分区中最后一条消息的下一个位置，每个副本都有自己的LEO

HW(Hign Watermark)：
    ISR中最小的LEO即为HW，俗称高水位，消费者只能拉到HW之前的消息

LW(Low Watermark)：
    低水位，代表AR集合中最小的logStartOffset

优先副本：
    指进行leader选举时，该副本会优先被选举为leader

分区重分配：
    如果某个broker挂了，它的分区又有多个副本，此时最多就会将leader转移到其他副本上。但是这个分区的副本已经处于不
可用的状态了，失效的副本不会自动转移到其它可用的节点上，这会影响到系统整体可靠性和可用性。因此就需要进行分区重
分配来将分区副本迁移到可用节点上




日志存储：

    日志文件布局：
        主题 -> 对应多个分区 -> 对应多个副本 -> 对应一个日志目录(一个分区副本) -> 对应多个日志分段(LogSegment) ->↓ 
    对应一个日志文件(.log)、偏移量索引文件(.index)、时间戳索引文件(.timeindex)、以及其它文件

    日志目录所在与格式：
        一般会在kafka安装目录所在盘的/tmp/kafka-logs，命名方式为topic-partition


        kafka向Log中追加消息时是顺序写入的，每个日志目录，也就是分区副本中，只有最后一个LogSegment才能被执行写入操作，
    其余同分区中的所有LogSegment都不能写入数据。最后一个LogSegment称为activeSegment，既活跃的日志分段。随着消息的不断写
    入，当activeSegment满足一定条件时，就需要创建新的activeSegment，之前activeSegment不再是活跃分段，之后追加的消息都将
    写入新的activeSegment

        每个LogSegment中的日志文件都以.log作为文件后缀，每个.log文件都有对应的两个索引文件：.index与.timeindex，分别是
    偏移量索引与时间戳索引文件。每个LogSegment都有一个基准偏移量baseOffset用来表示当前LogSegment中第一条消息的偏移量。
    偏移量是一个64位的整型，日志文件和两个索引文件都是根据baseOffset命名的，名称固定为20位数字，比如每个分区的第一个日
    志文件名为：00000000000000000000.log，这代表该文件的偏移量是从0开始的。这个数字代表的是消息偏移量，而非字节数

        _consumer_offsets偏移量主题初始是不存在的，当第一次有消费者消费消息时会自动创建这个主题。每个kafka-logs根目录都
    会包含4个xxx-checkpoint检查点文件与一个meta.properties元数据文件，这些文件会在kafka服务第一次启动时创建


    日志格式：
        kafka消息从0.8.x之后到现在的2.0.0经历了3个版本：v0、v1、v2，0.8之前的不用再管

        v0版本：
            消息分为两部分：
                LOG_OVERHEAD，即消息头，它包含了：
                    offset(8B)：消息偏移量
                    message size(4B)：消息大小

                RECORD，即消息本身，它包含了：
                    crc32：校验值，校验后面的所有其余信息，4B
                    magic：消息版本号，1B
                    attributes：低三位表示消息压缩类型比如NONE、GZIP、LZ4等，其余位保留，1B
                    key length：key的长度，-1为没有key，4b
                    key：key内容
                    value length：value的长度，-1为没有value，4B
                    value：value内容，也可能会为空，比如墓碑消息
            每个完整的消息都由LOG_OVERHEAD与RECORD两部分组成，由于它们之间总是一对，因此通常也罢它们看作是一条消息的整
        体，每个log文件都由多个这种格式的消息组成
            每个消息最小长度为：4+1+1+4+4=14B，也即v0版本中一条消息长度如果小于14B，就代表它是破损消息而被拒收。kafka保
        存时会在另外加上12B的LOG_OVERHEAD，假设发送一条key为key，value为value的消息，那log文件的大小就为34B=12+4+1+1+4+
        3+4+5，其中3和5分别是key与value的大小

        v1版本：
            v1版本相比v0版本，也就在每条消息的magic后面多了一个8B的时间戳，然后attributes的第4位用来存储时间戳的类型。
        0表示CreateTime，1表示LogAppendTime。这个类型由broker端的log.message.timestamp.type来配置，默认值为CreateTime，
        既生产者创建消息的时间戳，如果用户没有显式指定，那么生产者发送消息时会自动创建这个时间戳
            因此v1版本的消息会比v0版本的大8个字节

        v2版本：
            消息分为两部分：
                Record Batch Header，既批次头，它包含了：
                    first offset：当前消息批次的起始偏移量，8B
                    length：从partition leader epoch字段到整个批次末尾的长度，4B
                    partition leader epoch：分区leader纪元，可以看做分区leader的版本号或更新次数，4B
                    magic：消息格式版本号，1B
                    crc32：校验值，4B
                    attributes：消息属性，低三位代表压缩格式，第四位代表时间戳类型，第五位标识此批次是否处于事务中，第
                六位表示是否控制消息，2B
                    last offset delta：消息批次中最后一个记录的偏移量与first offset的差值，4B
                    first timestamp：消息批次中第一条记录的时间戳，8B
                    max timestamp：消息批次中最大的时间戳，一般情况下是指最后一个消息的时间戳，8B
                    producer id：生产者id，用来支持幂等和事务，8B
                    producer epoch：生产者纪元，用来支持幂等和事务，2B
                    first sequence：用来支持幂等和事务，4B
                    records count：消息批次中的消息个数，4B

                Records：既批次列表，每个批次包含了：
                    length：消息总长度，varint
                    attributes：弃用，但是还存在着，1B
                    timestamp delta：基于批次头时间戳的增量，varlong
                    offset delta：基于批次头偏移量的增量，varint
                    key length：key长度，varint
                    key：key内容
                    value length：value长度，varint
                    value：value内容
                    headers count：header数量，varint
                    headers：header内容
                        header key length：header key的长度
                        header key：header key内容
                        header value length：header value的长度
                        header value：header value的内容
                以上很多地方都采用了增量存储，因为增量配合着变长字段可以进一步节省占用的字节数

    变长字段：
        Varints是kafka从0.11.0版本引入的变长整型，它的作用是：例如value length是-1，原本需要用4个字节来保存，用了
    Varints之后就只需要1个字节来保存，数字越小越节省空间，不过Varints并非会一直节省空间，一个int32一般是4个字节，但是
    用了Varints后最长会占用5个字节，一个int64一般是8个字节，用了Varints后最长会占用10个字节
        不过kafka的使用场景一般length都会比较小，因此总是能节省到空间

    消息压缩：
        常见的压缩算法是数据量越大压缩效果越好，但是一条消息通常不会太大，这就导致了压缩效果不太好。而kafka实现压缩的方
    式是将多条消息一起进行压缩，这样可以保证较好的亚索效果。在一半情况下，生产者发送的压缩数据在broker端的存储时也是保
    持压缩状态的，消费者从服务端获取的也是压缩的消息，消费者处理消息时才会解压消息，这样就保持了生产端到消费端的压缩

        kafka日志使用哪种压缩方式是通过compression.type参数来决定的，默认值为producer，表示保留生产者使用的压缩方式，这
    个参数还可以设置为：GZIP、SNAPPY、LZ4这几种压缩算法，如果配置为uncompressed则表示不压缩

        消息集合被压缩后作为内层消息(inner message)存在，整个内层消息作为外层消息(wrapper message)的value，也就是说压缩
    后，整个压缩结果都作为一个新消息的value，这个消息的key是空的
        生产者在压缩消息时，会为内层每个消息创建一个LOG_OVERHEAD，其中Offset是从0开始的，这代表消息在内层消息里的位置
        而外层消息的偏移量会根据内层消息的数量来决定，例如：上条消息的偏移量为1024，而此次压缩了6条消息，那么外层消息的
    偏移量就是1030，再下条消息就是1031。所以外层消息保存的是内层消息里最后一条消息的绝对偏移量

        压缩消息是指compress message，kafka里还有另外一个compact message，是指日志清理时的压缩，不要搞混

    消息压缩时的timestamp与attributes字段处理：
        外层消息的时间戳处理：timestramp是CreateTime，那么取的是内层消息中最大的时间戳，如果是LogAppendTime，那么取的是
    Kafka服务器的时间戳。
        内层的时间戳处理：外层消息是CreateTime，取得是生产者创建消息时的时间戳；外层是LogAppendTime，内层的时间戳将会忽
    略
        auutibutes的处理：外层消息会根据配置设置，内层消息的时间戳类型位总是CreateTime，因为其它类型不会处理


日志索引：

    每个日志文件都对应两个索引文件，分别是：
        偏移量索引文件：
            用来建立消息偏移量到物理地址之间的映射关系，方便快速定位消息所在的物理文件位置。里面的偏移量保存的都是增量
        数据，基础偏移量用文件名保存，比如，要找偏移量277的消息，那么就先找到小于277的最大日志分段文件，比如250，然后计
        算相对偏移量：277-250=27，之后在250.log日志分段对应的偏移量索引文件中，根据二分查找法来找到偏移量小于27的最大偏
        移量，最后顺着该偏移量对应的物理地址在日志分段中往后找，直到找到对应偏移量的消息
            偏移量索引的存储结构是：前面4B存放偏移量增量，后面4B存放物理地址，由于默认每4K的消息才存放8B的索引，因此1G
        日志只需要2M的索引文件就能完成索引
            kafka强制要求索引文件大小必须是索引项(每一个索引的数据)的整倍数，因此偏移量索引的索引文件大小必须是8B的整倍
        数，如果配置为67B，kafka会将其转为64B，因为64B是小于67B的最大8B的倍数

        时间戳索引文件：
            时间戳索引文件则根据指定的时间戳来查找对应的偏移量信息
            时间戳索引的存储结构是：前面8B存放当前日志分段的最大时间戳，后面4B存储这个时间戳对应的相对偏移量
            由于时间戳索引和偏移量索引是同时插入的，因此时间戳索引会比偏移量索引文件大1/3，时间戳索引的大小也有强制要求
        ，时间戳索引也是通过二分查找法先找到对应的时间戳，然后拿到时间戳对应的相对偏移量，再去偏移量索引里面找物理地址
        ，最后根据物理地址找到消息


    稀疏索引：
        稀疏索引就是，并不会为每条消息构建一个索引，而是每当写入一定的数据量之后，偏移量与时间戳索引文件分别增加一
    个偏移量索引项和时间戳索引项，这取决于log.index.interval.bytes，默认为4096，既4K，增大该值可以减小索引的密度，
    反之亦然
        稀疏索引通过MappedByteBuffer将索引文件映射到内存中，MappedByteBuffer会通过内存映射的方式读写文件内容，
    这种方式直接调用系统底层，没有JVM和系统之间的复制操作，所以效率很高。
        
    kafka中的索引文件以稀疏索引的方式来构造消息的索引，以提高查询速度。偏移量索引文件中的偏移量时单调递增的，当
查询指定偏移量时，使用二分查找法来快速定位偏移量的位置，如果指定的偏移量不在索引文件中，则会返回小于指定偏移量
的最大偏移量；时间戳索引的查询方式与存储规则也跟偏移量基本一致
    稀疏索引是在磁盘空间、内存空间、查找时间多个方面之间的一个折中方案


    零拷贝：
        正常如果要将数据从磁盘读出来到通过网络发送出去，那么要经过4步：
            1、将数据从磁盘读到内核读缓存中
            2、从内核态切换到用户态，并将数据复制到用户模式下
            3、从用户态切换到内核态，并将数据复制到内核模式下(将数据写入socket)
            4、将内核态的数据写复制到网卡设备中传输

        如果使用零拷贝，相当于省去了切换到用户态复制的操作，只剩下：
            1、将数据从磁盘复制到内核读缓存
            2、将读缓存的数据复制到网卡设备中进行传输


Kafka协议：

    协议数据类型：
        boolean：用0和1代表false和true
        int8：有符号整型，占8位
        int16：有符号整型，占16位
        int32：有符号整型，占32位
        int64：有符号整型，占64位
        unit32：无符号整型，占32位
        varint：变长整型，变长类型参考前面的介绍
        varlong：变长长整型
        string：string类型，开头是int16，代表字符串的长度，后面跟着长度个数的UTF-8编码的字符
        nullable_string：可以为空的字符串，与string一样，如果开头的int16是-1，代表此值为null
        bytes：一个字符序列，开头是int32，代表字符序列的长度，后面跟着长度个数的字节
        nullable_bytes：一个可以为空的字符写，为空时int32为-1，其余情况与bytes类型相同
        records：表示kafka中的一个消息序列，可以看做一个nullable_bytes
        array：数组，元素类型可以是任意其它类型，开头是int32，代表元素个数，int32为-1时表示空数组

    kafka请求协议分为两部分：

        RequestHeader(请求头)：
            api_key：API类型表示，比如PRODUCE、FETCH，16位int
            api_version：API版本号，16位int
            correlation_id：由客户端指定的一个数字来标识这次请求的id，服务端响应时会返回相同的id，客户端就能将请求和响
        应对应起来了，32位int
            client_id：客户端id，string

        RequestBody(请求体)：
            请求体的结构由不同的API类型决定


    kafka响应协议分为两部分：

        ResponseHeader(响应头)：
            correlation_id：请求的id，响应会原样返回，32位int

        ResponseBody(响应体)：
            响应体的结构由不同的API类型决定


    消息生产协议(produce)：
      请求：
        ReauestHeader：请求头，这部分格式是统一的，可参考上面描述，不在赘述

        RequestBody：请求体

          transactional_id：事务id，不使用事务时为null，nullable_string类型

          acks：对应生产者的acks参数，int16类型

          timeout：请求超时时间，对应生产者request.timeout.ms，默认为30000，int32类型

          topic_data：生产者发送的消息集合，每个主题都是一个元素，array类型

            topic：主题名称，string类型

              data：主题对应的消息，每个分区都是一个元素，array类型

                partition：分区号，int32类型

                record_set：records类型，分区的消息
                        
      响应：
        ResponseHeader：响应头，这部分格式是统一的，可参考上面描述，不在赘述

        ResponseBody：响应体

          throttle_time_ms：如果超过了配额(quota)限制，则延迟该请求的处理时间，如果没有配置，那么该字段为0，int32类型

          responses：响应的数据集合，跟topic_data一样，以topic划分元素，array类型

            topic：主题名称，string类型

            partition_responses：主题内所有分区的响应集合，array类型

              partition：分区编号，int32类型

              error_code：错误码，用来表示错误类型，int16类型

              base_offset：消息集合的起始偏移量，该参数+发送消息的数量=发送消息最大的偏移量，int64类型

              log_append_time：消息写入broker端的时间，int64类型

              log_start_offset：所在分区的起始偏移量，int64类型


    消息拉取协议(Fetch)：
      请求：
        ReauestHeader：请求头，这部分格式是统一的，可参考上面描述，不在赘述

        RequestBody：请求体

          replica_id：副本的brokerId，用于follower副本向leader发起FetchRequest请求，普通消费者传-1，int32类型

          max_wait_time：最大拉取等待时间，与消费者的fetch.max.wait.ms参数对应，默认为500，int32类型

          min_bytes：最小拉取字节数，与消费者的fetch.min.bytes对应，默认值为1，int32类型

          max_bytes：最大拉取字节数，与消费者的fetch.max.bytes对应，默认值为52428800(50MB)，int32类型

          isolation_level：事务隔离级别，与消费者的isolation_level对应

          session_id：fetch session的id，int32类型

          epoch：fetch session的epoch纪元，int32类型

          topics：所要拉取的主题集合，array类型

            topic：主题名称，string类型

            partitions：分区集合，array类型

              partition：分区编号，int32类型

              fetch_offset：从分区的哪个位置开始读取消息，int64类型

              log_start_offset：专门用于follower副本发起FetchRequest请求，用来指明分区的起始偏移量，普通消费者为-1
            ，int64类型

              max_bytes：该分区最多能拉取的字节数，对应消费者的max.partition.fetch.bytes参数，默认1MB，int32类型

          forgotten_topics_data：从fetch session中指定要去除的拉取信息，array类型

            topic：主题名称，string类型

            patitions：分区编号集合，array类型

      关于fetch session：
        由于fetch请求是非常频繁的，且大多数时候是不会变的，除了起始偏移量，那么如果能将这些信息保存起来，就可以节省这
      部分带宽，且拉取的分区越多节省的就越多
        session_id创建或变更时会发送全量式的FetchRequest，全量式拉取就是请求体中包含所有要拉取的分区信息，当session_id
      稳定时则发送增量式的FetchRequest，里面的topics为空，因此topics里的内容已经被缓存在了客户端与broker，如果需要从当
      前session中取消某些分区的拉取订阅，则使用forgotten_topics_data字段来实现
        epoch应该就是同一个session_id，每次拉取都会+1，用来保证客户端与broker端的一致性
        fetch session机制在大规模分区拉取或同步时很有用，不过这对客户端来说是无感知的，因为一般而言客户端不会订阅太多
      分区

      响应：
        ResponseHeader：响应头，这部分格式是统一的，可参考上面描述，不在赘述

        ResponseBody：响应体

          throttle_time_ms：如果超过了配额(quota)限制，则延迟该请求的处理时间，如果没有配置，那么该字段为0，int32类型

          error_code：错误码，用来表示错误类型，int16类型

          session_id：对应请求的session_id

          responses：响应的数据集合，跟topic_data一样，以topic划分元素，array类型

            topic：topic名称，string类型

            partition_responses：分区响应数组，array类型

              partition：分区号，int32类型

              error_code：错误码，用来表示错误类型，int16类型

              high_watermark：消费者可以读到的最大偏移量(暂且这么理解，书上对Fetch的响应解释很少)，int64类型

              last_stable_offset：该分区相应的最大偏移量，int64类型

              log_start_offset：该分区相应的最小偏移量，int64类型

              aborted_transactions：事务相关信息，array类型

                producer_id：生产者id

                first_offset：生产者偏移量？事务相关的没介绍

              record_set：该分区的消息集合，records类型


时间轮(TimingWheel)：

    时间轮的设计类似于钟表，可以分为多层，例如第一层是每1ms一格，一共20格，第二层是20ms一格，也是20格，第三个
是400ms一格，一共20格，可以看到当前层的每一格都是上一层的总时间。每一层都是一个圈
    然后每个格都有一个自己的任务队列，这个队列里存放着真正的任务对象

    如果想加一个450ms的任务，那么第一层一共才20ms，满足不了；第二层一共400ms，也满足不了，第三层可以满足，所
以放在第三层的下一格，因为当前不一定刚好从第1格开始，可能第三层的第二格都走完了，因此只能往后放。当第一层走完之
后，会从第二层把任务拉下来，然后根据每个任务的具体时间，分配到第一层里面，因为第二层20ms之内的都是同一格，到第
一层还要进一步分格，当第二层走完后，又会从第三层拉任务，并且层级的指针+1
    第三层的任务拉下来时说明已经到了400ms了，但是还有50ms，因此这个任务会被放到第二层的第三格，当走到第二层的第
三格时，说明已经走了40ms了，每格20ms，还剩下10ms，然后又把该任务放在第一层的第10格，待时间推到那里就会执行该
任务
    当然实际情况肯定更加复杂，比如第一层到第10格，第二层到第5格，第三层到第2格，此时插入一个450ms的任务，那么不
能简单的往第三层后面，因为当前这个轮可能已经跑了一半了，后面插入的任务应该把当前这轮已经经过的时间给加上再计算
格子

    时间轮存放任务之后，还需要一个组件去触发轮子的每一个时间格子，kafka中用了DelayQueue来推进时间轮。
    DelayQueue里存放的是TimerTaskList，这个集合是每个时间格子的任务队列集合，DelayQueue只需对任务队列推进即可
，如果加了一个新任务，新任务是从TimingWheel里插入的，时间复杂度为O(1)，通过时间轮与延迟队列的方式，实现了任务
推进与快速添加与删除任务的操作，达到了少量的空间换时间

DelayQueue与TimingWheel的任务队列之间的配合：
    每个格子都有一个任务队列，每次插入新的任务，都会计算该任务的过期时间，如果新任务的过期时间与该队列上次的过期时间不
一样，那么就会用新的过期时间来将任务队列重新加入延迟队列，也就是说如果有两个任务在一个格子中，那么会出现这个集合被加入
延迟队列中两次，并且每次的过期时间都不同，也会被触发两次
    然后阻塞队列里只有最底层的时间轮，如果有上层的时间轮到期了，就会将它们降级，因此哪怕有重复的任务队列插入阻塞队列也
没关系，因为它们都是以刻度作为超时时间的，同一个任务队列里的刻度相同，所以重新set过期时间也没关系，因为不会改动，除非是
初始状态下的任务队列


延时操作：

    在发送消息时，如果acks为-1，那么消息在leader副本的本地日志文件中写入之后，还要等待所有follower副本都收到消息才能告
知客户端发送成功。在一定的时间内，如果follower没有及时拉取消息，在达到超时时间后就会返回超时异常。这个等待消息写入
follower副本的操作就是延时操作，延时操作可以被超时触发或者外部触发

    再例如在拉取消息时，会先读取一次日志文件，如果收集不够足够多的消息，就会创建一个延迟拉取操作(DelayedFetch)来等待消息拉取到一定的数量才返回，或者达到超时时间强制返回

    延时操作创建后会被加入延时操作管理器\炼狱(DelayedOperationPurgatory)，延时操作可能会超时，所以每个延时操作管理器都
会分配一个定时器(SystemTimer)，定时器的底层实现是时间轮(TimingWheel)，时间轮是靠收割机线程(ExpiredOperationReaper)来轮
转的，收割机线程除了推进时间轮外，还会清理监听池中已完成的延时操作


控制器：

    kafka集群中会将其中一个broker选举为控制器，它负责管理整个集群中所有分区和副本的状态。当某个分区的leader副本
出现故障，由控制器负责为该分区选举新的leader副本；当检测到某个分区的ISR集合发生变化时，由控制器负责通知所有
broker更新其元数据信息；当用kafka-topics.sh脚本为某个topic增加分区数量时，同样还是由控制器负责分区的重分配

    kafka的控制器选举依赖于zookeeper，通过在zookeeper中创建/controller临时节点来进行控制器竞选，临时节点内容如
下：
    {"version":1,"brokerid":0,"timestamp":"1529210278988"}
        version：固定为1
        brokerid：成为控制器broker的id编号
        timestamp：竞选成为控制器时的时间戳
    每个broker启动时都会去尝试读取/controller节点的brokerid值，如果读取到brokerid不为-1，代表已经有broker竞选为控
制器，当前broker会放弃竞选，如果/controller节点为空或者数据异常，就会去尝试创建/controller节点

    zookeeper中海油一个与控制器有关的/controller_epoch节点，这个节点是持久节点，内容是一个整形的controller_epoch
值，用来记录控制器发生变更的次数，既当前控制器是第几代控制器，可称为：控制器纪元，这个节点初始值为1，每次选出一
个新的控制器改字段就+1。每个和控制器交互的请求都会携带controller_epoch字段，如果请求的纪元小于内存中的纪元，则
认为这个是向已过期的控制器所发送的请求，这个请求会被认定为是无效请求。如果纪元大于内存中的纪元，那说明已经有新
的控制器当选了
    由此可见kafka通过controller_epoch来保证控制器的唯一性和相关操作的一致性

    控制器的额外职责：
        1、监听分区的变化，也就是在zookeeper的相关节点注册监听器，然后进行对应动作，比如分区重分配、ISR集合变更
        2、监听主题的变化，也就是监听zookeeper中的/brokers/topic和/admin/delete_topics节点，用来处理主题增减和删
    除主题
        3、监听broker变化，也就是监听zookeeper中的brokers/ids节点，用来处理broker增减的变化
        4、从zookeeper中读取所有主题、分区、broker相关的信息，并进行管理，监听所有主题的/brokers/topics/<topic>
    节点，用来处理broker增减的变化
        5、启动并管理分区状态机和副本状态机
        6、更新集群的元数据信息
        7、如果参数auto.leader.rebalance.enable为true，则开启一个名为：auto-leader-rebalance-task的定时任务来维护
    分区的优先副本均衡

    控制器在选举成功后灰度区zookeeper中个个节点的数据来初始化ControllerContext，并且管理这些上下文，比如某个主题
增加了分区，控制器负责创建这些分区并更新上下文信息，然后将这些变更的信息同步到其它broker节点中。
    由于有很多事件都会读取或更新上下文的信息，比如任务触发的事件、监听器触发的事件，如果单纯使用锁来实现，整体性
能会大打折扣，针对这一现象，kafka的控制器使用单线程基于事件队列的模型，将每个事件进行封装，然后按照先后顺序存储
到LinkedBlockQueue中，然后使用一个ControllerEventThread线程按先进先出原则处理事件，也就是将事件串行化

    早期的kafka中，所有broker都会在zookeeper上注册大量监听器，这有造成zookeeper过载的隐患。现在新版的设计中，
只有controller才会在zookeeper上注册相应的监听器，其它broker极少需要再监听zookeeper中的数据变化，这样省了去很多
不必要的麻烦，不过每个broker还是会对/controller节点添加ControllerChangeHnadler监听器来监听控制器的变化
    当/controller节点的数据发生变化时，每个broker都会更新自身内存中的activeControllerId，如果某个broker原来是控制
器，在变更后activeControllerId的值与自身brokerid不一致，那么它就需要退位，关闭对应的资源、比如状态机与监听器
    当/controller节点被删除时，每个broker都会进行选举(争抢/controller临时节点?)，如果broker在节点删除前是控制器，它
还需要进行退位操作，如果有特殊需要，可以手动删除/controller节点来触发新一轮选举，当然手动关闭控制器或者向
/controller节点写入新的节点id对应的内容也可以触发新一轮选举


优雅关闭：
    通过kafka-server-stop.sh来关闭，这种关闭会等待关闭信号，从而让kafka程序有机会去执行关闭钩子



broker.id：

    kafka集群中，每个broker都有唯一的id，broker在启动时会在zookeeper中的/broker/ids路径下创建一个以当前brokerId
为名称的虚节点，broker的健康转改检查就依赖于此虚节点
    当broker下线时，该节点会自动删除，其它broker节点通过判断/broker/ids路径下是否有此broker的brokerId来确认该
broker的健康状态

    broker.id可以在config/server.properties里配置，也可以通过meta.properties文档来配置，kafka的brokerId只有大于等
于0才会正常启动

    meta.properties(broker的元数据文件，在kafka启动时生成)：
        version：固定值，0
        clusterId：集群id
        broker.id：既brokerId，注意事项如下：
            1、如果config/server.properties与meta.properties中的broker.id不一致，则抛出InconsistentBrokerIdException
            2、如果config/server.properties未配置broker.id，就以meta.properties中的broker.id为准
            3、如果config/server.properties与meta.properties中都没有broker.id，则可以通过broker.id.generation.enable和
        reserved.broker.max.id来配合生成brokerId，这个参数默认开启，默认是true和1000，也就是如果自动生成，那么
        brokerId从1001开始，那个max.id的意思是自动生成的id必须大于这个值
            4、这个id自动生成也是通过zookeeper的/brokers/seqid节点来生成的，会通过它的dataVersion字段，每次设置值
        它都会+1，相当于一个发号器
            5、我们大多数情况下broker.id还是习惯在config/server.properties中进行配置


bootstrap.servers：
    代表连接到kafka集群的broker地址列表

metadata.broker.list：
    用来获取元数据的broker列表，元数据就是例如所有broker的topic分区主从信息、控制器信息等待

zookeeper.connect：
    zookeeper的连接地址


消费者协调器和组协调器：

    旧版本的zookeeper通过大量的zookeeperwatcher来监听kafka集群的状态，比如再均衡时就各自进行分区抢占，消费者
之间并不知道彼此操作的结果，这会导致两个问题：
        1、羊群效应，羊群效应是指zookeeper中一个被监听的节点变化，导致大量的Watcher通知要发送到客户端，导致通知
    期间zookeeper其它操作会有延迟，科有可能发生类似死锁的情况
        2、脑裂问题，zookeeper集群之间相互无法通信，可能导致各个消费者获取的状态不一致。但是zookeeper不是不会有
    脑裂问题吗？总是从leader节点获取不就好了
        属性变更只要能保证同步到大多数节点就行了，那如果刚好从少数节点里读取呢，少数节点如果无法连接到leader，是不
    是直接不提供服务了？


    新版本的消费者客户端对此进行了重新设计，将全部消费组分成多个子集，每个消费组的子集都在服务端对应着一个
GroupCoordinator对其进行管理，GroupCoordinator是kafka服务端用户管理消费组的组件。同时消费者客户端也有一个
ConsumerCoordinator组件负责与服务端的GroupCoordinator进行交互，它们之间最重要的就是负责消费者的再均衡操作，
包括分区分配也是在再均衡期间完成的，一般而言触发再均衡的场景有如下几种：
        1、有新的消费者加入消费组
        2、有消费者下线，并不一定是真的下线，例如遇到长时间GC、网络延迟导致长时间未向GroupCoordinator发送心跳，
    服务端就会认为该消费者已下线
        3、有消费者主动退出消费组，例如发送LeaveGroupRequest，或者调用了unsubscrible方法取消订阅某些主题
        4、消费组所对应的GroypCoordinator节点发生变更
        5、消费组订阅的任一主题或主题的分区数量发生变更

    消费者、消费组、组协调器的工作阶段：
        第一阶段(find_coordinator)：
            消费者需要确定它所属的消费组对应的GroupCoordinator所在的broker，并与该broker创建相互通信的网络连接，
        如果消费者已经保存了GroupCoordinator节点的信息，并且连接能正常通信，那么就可以进入第二阶段，否则就需要向
        集群中的负载最小节点发送FindCoordinatorRequest请求来找到对应的GroupCoordinator，这个请求会将groupId、
        coordinator_type、coordinator_key(transactionalId)传过去，如果找到了就返回对应的node_id、host、port等信
        息，coordinator_type是用来标识协调器类型的，0代表GroupCoordinator，1代表TransactionCoordinator。
            关于负责消费者再均衡的broker，是取决于消费者组落在__consumer_offsets主题的哪个分区上，通过取模计算：
        groupId.hashcode % topicPartitionCount，然后取那个分区的leader作为再均衡的broker，这个broker还负责消费者
        位移

        第二阶段(join_group)：
            成功找到消费组对应的GroupCoordinator后就进入加入消费组的阶段，此阶段的消费者会向GroupCoordinator发送
        JoinGroupRequest请求，并处理响应
            JoinGroupRequest结构：
                group_id：消费组id
                session_timeout：对应消费端的session.timeout.ms，如果GroupCoordinator超过指定时间没收到心跳则认为此
              消费组已下线
                rebalance_timeout：对应消费端参数max.poll.interval.ms，表示消费组再均衡时，GroupCoordinator等待消费者
              重新加入的最常等待时间
                member_id：表示GroupCoordinator分配给消费者的id标识，消费者第一次发送JoinGroupRequest时为null
                protocol_type：表示消费组实现的协议，对消费者而言自字段为：consumer
                group_protocols：分区分配策略数组，取决于消费端参数的partition.assignment.strategy，如果配置了多种策略
              ，这个属性就包含多个值，每个元素内容如下：
                  protocol_name：对应PartitionAssignor接口的name()方法，既协议名称
                  protocol_metadata：分为version、topics、user_data三个字段，其中version固定为0，topics对应
                PartitionAssignor接口的subscription()方法返回值类型的Subscription中的topics，user_data对应Subscription
                中的userData，可为空
        JoinGroupResonpse：
            generation_id：标识当前消费组的年代信息

            如果原有的消费者重新加入消费组，那么发送JoinGroupRequest之前还要做一些准备工作：
              1、如果开启自动提交偏移量，那么请求之前要先提交消费位移，这个过程是阻塞执行的
              2、如果消费者有添加再均衡监听器ConsumerRebalanceListener，那么会调用onPartitionsRevoked()方法中的自定
            义逻辑，可以清除一些状态，或者提交消费位移等
              3、由于重新加入消费组，那么之前与GroupCoordinator的心跳也不需要了，因此要禁止心跳检测

            服务端收到JoinGroupRequest后，会生成一个memberId，这个id其实就是client.id + "-" + UUID

            GroupCoordinator还要为消费组选举出一个leader，如果消费者没有leader，那么第一个加入消费组的消费者就是消
        费组的leader，如果leader消费者退出了消费组，就会重新选举一个leader，选举就是将所有消费者的放入Map中，key
        是memberId，value是消费者信息，然后拿map.keys()的第一个元素，也就是拿map里的第一个消费者，很随机也很随意。
        leader的职责是在那到所有消费者的信息后，执行具体的分区分配逻辑

            分配策略的选举，每个消费者都可以设置自己的分区分配策略，每个消费组都要选举出一个令彼此都信服的策略来进行整
        体上的分区分配。这个分区分配的选举并非由leader消费者决定，而是根据各个消费者发送过来的分配策略来决定，具体选举
        过程如下：
                1、收集各个消费者支持的所有分配策略，组成候选集candidates
                2、每个消费者从候选集candidates中找出第一个自身支持的策略，为这个策略投一票
                3、计算候选集中各个策略的选票数，选票最多的策略为当前消费组的分配策略
            如果选出的分配策略消费者不支持，就会抛出IllegalArgumentException：Member does not support protocol，消费者
        支持哪个策略，取决于partition.assignment.strategy参数的配置，在这里配置了就算支持，否则就不支持。分配策略选好
        后，服务端就将JoinGroupResponse响应给各个消费者
            JoinGroupResponse响应给普通消费者的信息，除了基础信息外，就只有leader消费者的member_id、最终选择的策略、
        消费组的年代等信息，只有给leader消费者响应的信息才包含各个消费者的订阅信息，好让leader可以进行分区分配。这意味
        着服务端不参与具体的分区分配，这样即使分区分配策略发生变更，也不需要重启服务端，只要重启客户端即可

        第三阶段(sync_group)：
            leader消费者将第二阶段选举出来的分区分配策略来进行具体的分区分配，然后将分配的结果同步给GroupCoordinator，
        GroupCoordinator再将分配到的分区同步给每个消费者，这个阶段的请求是SyncGroupRequest

            这个阶段的具体过程如下：
                1、消费者发送SyncGroupRequest请求，普通消费者发送的请求里主要属性是member_id和group_id，而leader消费
            者额外还有一个group_assignment数组，里面包含了每个消费者能够消费哪个topic的哪些分区
                2、GroupCoordinator收到leader消费者的分配方案后，会给每个SyncGroupRequest请求发送SyncGroupResponse
            响应，主要是根据group_id与member_id来区分这个请求是哪个消费者，然后从分配方案取出这个消费者对应的分配分区
            并返回
                3、消费者收到自己的分配方案后，会调用PartitionAssignor中的onAssignment()方法，然后调用
            ConsumerRebalanceListener中的onPartitionAssigned()方法，之后开启心跳任务，消费者定期向GroupCoordinator发
            送HeartbeatRequest请求来维持心跳。但是源码里是先开启心跳线程之后才开始Join Group

        第四阶段(heartbeat)：
            进入这个阶段之后，消费组中的所有消费者就会处于正常工作状态，但是在正式消费之前，消费者还需要确定拉取消息的
        起始位置，如果之前已经将最后的消费位移提交到了GroupCoordinator,且GroupCoordinator也将其保存到__consumer_offsets
        主题中，消费者就可以通过OffsetFetchRequest请求获取上次提交的位移并从此位移开始消费
            消费者通过向GroupCoordinator发送心跳来维持它们与消费组的从属关系以及对分区的所有权。心跳线程是一个独立的线
        程，如果消费者停止发送心跳的时间足够长，则整个会话就被判定为过期，GroupCoordinator就会触发一次再均衡。消费者的
        心跳间隔时间由参数heartbeat.interval.ms指定，默认值为3000，这个参数必须必session.timeout.ms小，一般不会超过1/3
            还有一个max.poll.interval.ms参数用来指定消费者poll()方法两次调用之间的最大延迟，如果超过该参数指定的时间消
        费者还没调用poll()，也会触发消费组的再均衡
            除了以上两种维持心跳的被动退出外，还可以使用LeaveGroupRequest请求来主动退出消费组，比如客户端的unsubscrible
        ()方法可以取消对某些主题的订阅


__consumer_offsets：

    一般情况下，当集群第一次有消费者消费消息时，会自动创建__consumer_offsets主题，，它的副本因子受
offsets.topic.replication.factor参数的约束，这个参数默认值为3，而分区数可通过offsets.topic.num.partitions参数设置，默认
为50，客户端提交消费位移使用OffsetCommitRequest请求来实现，该请求的大概内容有：

    消费组id、消费者请求纪元id、消费位移保留时长(retention_time)、消费者id、topic数组、每个topic包含一个分区数组，每个
分区包含具体的分区号、偏移量、元数据

    其中retention_time保持为-1，代表按照broker端的offsets.retention.minutes来决定，这个配置默认为7天，超过这个时间后，
就会通过墓碑消息或者日志压缩策略进行删除。这个时间在2.0.0版本之前为1天，如果用户自己保存了偏移量，过了这个有效期导致分
区被清除，就只能根据auto.offset.reset(最新的或当前日志里最早的)参数来重新决定开始消费的位置
    
    元数据则是在通过Map<TopicPartition, OffsetAndMetadata> offsets提交时，OffsetAndMetadata对象里的metadata属性，如果
不指定则为空字符串，且metadata的大小不能超过broker端的offset.metadata.max.bytes配置，默认为4096

    __consumer_offsets主题的内容如下：
      key：
        version：版本号，当前为1，int16类型
        group：消费组名称，string类型，(为了保持与GroupCoordinator在同一broker，计算消费组分区时仅用group进行计算)
        topic：主题名称，string类型
        partition：分区号，int32类型
      value：
        version：版本号，当前为1，int16类型
        offset：偏移量，int64类型
        metadata：用户提交的元数据，string类型
        commit_timestamp：偏移量提交的时间戳
        expire_timestamp：偏移量过期的时间戳，相当于commit_timestamp + retention_time

    消费位移过期后，会有一个delete-expired-group-metdata的定时任务来负责清理，这个定时任务的执行周期由参数
offsets.retention.check.interval.ms控制，默认为60000，当主题被删除后，对应的消费位移信息也会被一并删除



消息传输保障：

    一般而言消息中间件的传输保障有3个级别：
        1、at most once：最多一次，消息只会发送一次；可能会丢失，但绝不会重复
        2、at least once：最少一次，消息会发送多次；直到发送成功，消息绝不丢失，但可能会重复
        3、exactly once：恰好一次，每条消息肯定会被传输一次，且仅传输一次

    生产者的消息传输保障：
        kafka的消息保障机制依赖于多副本机制，由于kafka无法确定网络故障期间发生了什么，所以生产者通过多次重试来确保消息
    成功写入kafka，这个重试可能会导致消息重复写入，因此这里kafka提供的消息保障为at least once

    消费者的消息传输保障：
        对消费者而言，如果先提交偏移量再处理业务，如果出现业务处理失败的情况则会造成消息丢失,因此这种做法是at most once
    ；如果先处理业务再提交偏移量，如果出现偏移量提交失败，则会造成消息重复消费，因此这种做法是atleast once

    kafka从0.11.0.0版本引入了幂等和事务，以此来实现exactly once semantics(精确一次处理语义)


    幂等：
        所谓的幂等，就是对接口的多次调用所产生的结果和调用一次是一致的。在kafka里，生产者在进行重试的时候可能会造
    成消息重复写入，在使用幂等功能后就可以避免这种情况

        将客户端的enable.idempotence参数设置为true即可开启幂等，幂等默认不开启

        当开启幂等的时候，有几个参数的值必须正确设置才能保证幂等可以实现，否则就会报错：
            retries：重试次数必须大于0，因为如果没有重试那么幂等也没有存在的必要了
            acks：必须为-1，必须要将消息传输给所有同步副本，否则如果刚好leader挂了幂等的序号可能就对不上了
            max.in.flight.requests.per.connection：必须小于等于5，这个参数是客户端对每个braoker同一时间存在的最大请求
        数，我也不知道为什么是5，网上有个说法是：服务端最多只会缓存每个生产者的每个分区的5条消息，当同时发送更多消
        息时，就会出现序号不在缓冲区里，导致不匹配，感觉不一定对，到时候可以在源码看看是不是真的会出现这种情况

        kafka的幂等实现：
            kafka为了实现幂等，引入了producer id(生产者id)和sequence number(序列号)，对应上面的v2版本消息请求里的
        producer id和first seqence

            每个新生产者都会在初始化时分配一个PID，在服务端每个PID与每个分区都有一个对应的序列号，这些序列号从0开始
        单调递增，生产者每发送一个消息都会将它PID与分区号对应的序列号+1
            服务端跟生产者客户端同时维护了这个序列号，每次发消息它们都会同时+1，服务端每收到一条消息，都会判断它的
        序列号值比broker端维护的序列号值大1才会接收，如果消息的序列值比服务端大了不止1，就说明中间有数据尚未被服务
        端写入，broker端就会丢弃该消息，生产者将会抛出OutOfOrderSequenceException异常。如果消息的序列值比服务端
        小，则说明该消息已经被写入过，生产者直接跳过该消息即可

            OutOfOrderSequenceException异常是一个严重的异常，当出现这个异常之后，如果在调用诸如send()、
        begintransaction()、commitTransaction()方法都会抛出IllegalStateException异常

        幂等也有个缺陷，就是只能保证单个生产者单个分区的幂等，不能跨分区运作，而事务可以弥补这个缺陷


    事务：
        事务可以保证对多个分区写的原子性，要么全部成功，要么全部失败

        事务的开启需要客户端提供一个唯一的transactional.id，同时要求必须开启幂等。当然开启事务时会默认开启幂等，如果
    客户端手动关闭幂等又开启事务会导致生产者报ConfigException

        transactionalId与PID一一对应，不同的是transactionalId由用户指定，而PID由kafka内部分配，通过initTransactions()
    方法。为了保证同一个transactionalId只能同时被一个生产者使用，每个生产者通过transactionalId获取PID时，还会获取
    一个单调递增的producer epoch(看上面的v2结构)，如果两个生产者共用一个transactionalId，那么前面开启的生产者会报
    ProducerFencedException

    不过出于以下原因，事务并不能保证所有已提交事务中的所有消息都能被消费到：
        1、采用日志压缩策略的主题，事务中的某些消息可能会被清理
        2、事务中的消息分布在多个日志分段，而老的分段过期被删除
        3、消费者通过seek()方法直接跳过事务消息
        4、消费者没有分配到事务消息的所有分区，从而只能读取到部分事务消息

    KafkaProducer中与事务相关的方法：
        void initTransactions()：
            初始化事务，比如分配PID，执行这个方法的前提是配置了transactionalId，否则就会报错
        void beginTransaction()
            开启事务
        void sendOffsetsToTransaction(Map<TopicPartition, OffsetAndMetadata> offsets, String consumerGroupId)
            为消费者提供在事务内的位移操作，这个方法适合消费-转换-生产模式。它会将偏移量列表发送给指定的消费组协调器
        ，并将这些偏移量视为当前事务的一部分，仅有事务提交成功，该偏移量才会被视为生效，该偏移量应该是最后一个偏移
        量+1。它适用于将消费和生产的消息一起批量处理的场景，也就是必须消费后且生产成功才会提交偏移量的场景
        void commitTransaction()
            提交事务
        void abortTransaction()
            终止事务，可以理解为回滚

        在消费端有一个isolation.level，可以指定事务的隔离级别，它的默认值是read_uncommited，意思是消费者可以消费到
    未提交的事务，当然已提交的事务也是可见的。这个参数还可以设置为read_committed，表示消费端应用不可以看到尚未
    提交的事务。
        如果隔离级别设置为read_committed，那么未提交的消息会在kafka内部缓存，直到生产者执行了commitTransaction()
    方法后才会将这些缓存的消息推送给消费者

    控制消息(ControlBatch)：
        日志文件中除了普通的消息，还有一种专门用来标志一个事务结束的消息，它就是控制消息。
        控制消息有两种类型：COMMIT和ABORT，分别表示事务的提交和中止。Kafka通过这个控制消息来判断对应的事务是
    被提交还是被中止。再结合参数isolation.level配置的隔离级别来决定是否将相应的消息返回给消费者客户端
        需要注意的是，控制消息对消费者客户端是不可见的


    事务协调器(TransactionCoordinator)：
        kafka引入了事务协调器来处理事务，这一点可以类比一下消费组协调器(GroupCoordinator)。每一个生产者都会被指派一个
    特定的TransacationCoordinator，所有的事务逻辑，例如分配PID、事务开始、提交、中止等操作都是由TransactionCoordinator
    来负责的。
        TransactionCoordinator会将事务状态持久化到内部主题__transaction_state中

    事务执行流程：

        1、查找TransactionCoordinator：
            查找对应TransactionCoordinator所在的broker节点。与找GroupCoordinator节点一样，也是通过
        FindCoordinatorRequest请求来进行的，与之不同的是coordinator_type属性为1。
            kafka收到FindCoordinatorRequest请求后,会根据coordinator_key(transactionalId)查找对应的
        TransactionCoordinator节点。如果找到会返回对应的node_id、host、port等信息。查找方式是根据transactionalId的哈希
        值计算主题
            __transaction_state中的分区号，该主题的分区数与__consumer_offsets一样，默认也是50，可通过
        transaction.state.log.num.partitions来配置。找到分区号后，再找到此分区leader所在的broker节点，该broker就是这个
        transactionalId对应的
            这一步骤基本与查找GroupCoordinator的逻辑如出一辙

        2、获取PID
            但凡开启幂等性，都需要TransactionCoordinator分配一个PID。生产者获取PID是通过InitProducerIdRequest请求来实现
        的，这个请求的请求体部分仅有两个参数：transactional_id与transaction_timeout_ms，既事务id与事务超时时间，其中事
        务超时时间由transacation.timeout.ms配置，默认值为60000
            如果生产者未开启事务而只开启幂等性，那么InitProducerRequest请求可以发送给任意的broker。当
        TransactionCoordinator第一次收到包含该transactionalId的InitProducerIdRequest时，它会把transactionalId和对应的
        PID以消息的形式保存到__transaction_state主题中，这类消息也称为事务日志消息。这样可以保证transaction_id与PID的关
        系被持久化，从而保证即使TransactionCoordinator宕机该关联关系也不会丢失.InitProducerRequest还会将producer_epoch
        加1，并对之前生产者未完成的事务进行Commit或Abort
            __transaction_state的消息格式如下：
                key：
                    version：版本号，int16
                    transactional_id：事务id，string
                value：
                    version：版本号，int16
                    producer_id：生产者id，int64
                    producer_epoch：生产者纪元，表示生产者id是第几次被获取，int16
                    transaction_timeout：事务超时时间，int16
                    transaction_status：事务状态，有Empty(0)、Ongoing(1)、PrepareCommit(2)、PrepareAbort(3)、
                CompleteCommit(4)、CompleteAbort(5)、Dead(6)
                    transaction_partitions：事务关联的分区集合，nullable_array
                      topic：主题名称
                      partitions_ids：分区号的数组，array[int32]
                    transaction_entry_timestamp：事务(初始化或者结束，书上没讲，也查不到)的时间戳，int64
                    transaction_start_timestamp：事务开始的时间戳，int64
            InitResponse响应的响应体主要也就是个producer_id与producer_epoch，producer_epoch的作用是，如果传过去的epoch
        小于TransactionCoordinator的epoch，就会被拒绝开启新事务

        3、开启事务
            通过producer的beginTransaction()方法可以开启一个事务，调用该方法后，生产者只会在本地标记已开启一个新的事务
        ，只有发送开启事务后的第一条消息后，TransactionCoordinator才会任务该事务已开启

        4、consumer-transform-produce：消费-转换-生产
            1、发送AddPartitionsToTxnRequest请求，当生产者给一个新的分区发送数据前，它会先向TransactionCoordinator
        发送一个AddPartitionsToTxnRequest请求，这个请求会让TransactionCoordinator将transactionId与TopicPartition的
        对应关系存储在__transaction_state主题中。如果该分区是对应事务中的第一个分区，那么TransactionCoordinator还会
        启动对该事务的计时
            2、发送ProduceRequest请求，生产者正常地给broker发送消息，这一点基本与普通消息相同，不同的是
        ProducerBatch里会包含PID、producer_epoch、sequence number等事务与幂等相关的信息
            3、调用producer.sendOffsetsToTransaction()方法，这个方法会发送AddOffsetsToTxnRequest请求，这个请求会被
        TransactionCoordinator通过groupId来推导出在__consumer_offsets中的分区，然后将这个分区的信息保存在
        __transaction_state中
            3.1、producer.sendOffsetsToTransaction()方法还会发送TxnOffsetCommitRequest请求，在发送完上一个请求后
        生产者会将TxnOffsetCommitRequest请求发送给GroupCoordinator，从而将本次事务的消费位移存储到
        __consumer_offsets中
            4、调用producer的commitTransaction()或abortTransaction()来结束当前的事务。首先发送EndTxnRequest请求，
        它内部会有一个tansaction_result参数，0为ABORT，1为CIMMIT。
            TransactionCoordinator收到EndTxnRequest请求后，会将prepare_commit或prepare_abort消息写入
        __transaction_state，然后通过WriteTxnMarkersRequest请求将commit或abort写入用户所使用的普通主题和
        __consumer_offsets，再然后将complete_commit或complate_abort写入__transaction_state主题
            其中WriteTxnMarkersRequest请求是由TransactionCoordinator发向事务中各个分区的leader节点的，节点收到这个
        请求后，会在相应的分区中写入控制消息，用来标识事务的终结。RecordBatch中的attributes字段的第五位用来表示是否处
        于事务中，第六位用来表示是否控制消息。控制消息中的key里除了版本号就只有一个type，0表示中断，1表示提交；value
        里除了版本号就只有一个coordinator_epoch，用来表示TransactionCoordinator的纪元，每个事务管理器切换都会更新其值
            只要最终的complete_commit或complete_abort写入了主题__transaction_state，就表明当前事务已经结束了，此时可以
        删除__transaction_state事务中的所有消息。由于__transaction_stte的日志清除策略为压缩，所以这里的删除只要将消息
        设置为墓碑消息即可
        
        如果事务的隔离级别被设置为read_committed，那么在执行commit之前，kafka内部都会缓存这些消息，等到生产者
    commit后才将这些消息一次性推送给消费端应用。如果执行的是abort，则会将这些消息丢弃



kafka的可靠性探究：

    副本：
        副本是分布式系统中场景的概念之一，指的是分布式系统对数据和服务提供的一种冗余方式。副本有两类：数据副本与服务
    副本。数据副本是指在不同的节点上持久化同一份数据，当某一个节点上的数据丢失时，可以从副本上读取该数据。服务副本是
    指多个节点提供相同的服务，每个节点都能接收外部的请求并进行相应的处理

    多副本的消息发送流程：    
        从生产者发出一条消息，首先会被写入分区的leader副本，不过还要等所有ISR都同步完成后才能被认为已提交，之后才会更
    新分区的HW，进而消费者可以消费到这条消息。具体ISR、HW的含义看上面

    失效副本：
        正常情况下，分区的所有副本都处于ISR集合中，但是难免会有异常情况，导致某些副本被剥离出ISR集合。在ISR集合之外，
    处于同步失效或功能失效(比如副本挂了)的副本统称为失效副本，失效副本对应的分区也就被称为同步失效分区，既
    under-replicated分区
        是否同步失效副本可以通过broker端参数replica.lag.time.max.ms来判断，当ISR集合中的一个follwer副本滞后leader副本
    的时间超过此时间则判定为同步失败，需要将此follower副本剔除出ISR集合，该值默认为10000



ISR的伸缩：

    ISR的缩减：
        kafka在启动时会开启两个与ISR相关的定时任务：
            isr-expiration：
                isr-expiration任务会周期性地检测每个分区是否需要缩减其ISR集合，这个周期和replica.lag.time.max.ms参数
            有关，周期大小是它的一半，默认值为5000
                如果某个分区的ISR集合发生变更，则会将变更后的数据记录到ZooKeeper的/brokers/topics/<topic>/patition/
            <partition>/state节点中,节点的数据如下：
                {"controller_epoch":26,"leader":0,"version":1,"leader_epoch":2,"isr":[0,1]}
                controller_epoch：当前控制器的epoch
                leader：当前分区的leader副本所在broker的id编号
                version：版本号，当前固定为1
                leader_epoch：当前分区的leader纪元
                isr：变更后的ISR列表
            ISR集合变更时还会将变更后的记录缓存到isrChangeSet中

            isr-change-propagation：
                isr-change-propagation任务会周期性(固定2500ms)地检查isrChangeSet，如果发现isrChangeSet中有ISR集合的变
            更记录，那么它会在zookeeper的isr_change_notification路径下创建一个isr_change_开头的持久顺序节点，例如：
            /isr_change_notification/isr_change_0000000000，并将isrChangeSet中的信息保存到这个节点中。
                kafka控制器在/isr_change_notification添加了一个Watcher，当这个节点中有子节点发生变化就会触发Watcher
            ，以通知控制器更新相关元数据，并向它管理的broker节点发送更新元数据的请求，最后删除/isr_change_notification
            下已处理过的节点。
                由于频繁触发Watcher会影响kafka控制器、zookeeper、还有其他被通知的节点的性能。为了避免这种情况，kafka
            对/isr_change_notification路径下节点的变更还添加了两个条件：上次ISR变化已经超过5S、上次写入zookeeper距离
            线程已经超过60S。只有满足以上条件之一才可以将ISR集合的变化写入目标节点


    ISR的扩充：
        随着follower副本不断与leader副本进行消息同步，follower副本的LEO也会逐渐后移，并最终追赶上leader副本，此时该
    follower副本就有资格进入ISR集合。追赶上leader副本的判定标准是此副本的LEO是否不小于leader副本的HW，注意这里并不是
    与leader副本的LEO相比，而是HW。
        ISR扩充后同样会更新ZooKeeper中的/brokers/topics/<topic>/partition/<partition>/state节点和isrChangeSet集合，
    之后的步骤与ISR缩减时一样。检测follower副本是否有资格加入ISR的线程难道也是isr-expiration吗？

        当ISR集合发生扩充与缩减，或者ISR集合中任一副本的LEO发生变化时，都可能会影响整个分区的HW。发生扩充时代表有新的
    follower追上了leader，那么它一般都是ISR里LEO最小的，因此它的增长一般就代表HW的增长。如果发生缩减时，把某个ISR集合
    中的节点删除，那么由于leader的HW是ISR里LEO最小的，如果把ISR里最小的LEO剔除，那么剩下的可能都是较高的，因此也可能会
    影响到HW



本地副本(local replica)与远程副本(remote replica)：
    本地副本是指对应的log分配在当前的broker节点上，远程副本是指对应的log分配在其它broker节点上。

    在kafka中，同一个分区的信息会存在多个broker节点上，并被其上的副本管理器所管理，这样每个broker节点就有了多个副本，
但只有本地副本才有对应的日志。

    LEO与HW的变化：
        随着生产者一直往leader副本写入消息，因此leader副本的LEO会一直增加，并在某一时刻增加至5，此时所有副本的HW
    都还为0。
        随后follower副本向leader副本拉取消息在拉取时会附带自身的LEO信息，这个信息放在FetchRequest中的fetch_offset
    属性中，还有自身的HW信息，放在high_watermark。leader副本返回的FetchResponse会带有自身的HW信息，对应
    FetchResponse中的high_watermark
        此时follower副本拉到了消息，并更新各自的LEO与HW，LEO就是拉取消息中的最大偏移量，HW是当前LEO和leader
    副本传过来的HW取较小值，此时LEO为3，HW为0(因为leader的HW还是0)
        follower继续发送FetchRequest请求，leader收到请求后，选取follower中最小的LEO作为新的HW，假设为3，然后返
    回leader的LEO与HW，follower收到响应后会先更新LEO，假设为7，随后在leader的HW与自己的LEO取较小值，也就是3
    
    变化总结：
        在一个分区中，leader副本会记录所有副本的LEO，而follower自会记录自身的LEO。对于HW而言，各个节点都只记录
    它自身的HW。
        leader的HW是在收到FetchRequest之后更新的，follower的HW是在收到FetchResposne之后更新的，它们各自的LEO
    都是在收消息的时候更新的

    kafka根目录下的一系列checkpoint文件：
        recovery-point-offset-checkpoint(恢复点文件)：
            存放所有分区的LEO，kafka会有一个定时任务负责将所有分区的LEO刷写到恢复点文件中，刷新周期由broker端参数
        log.flush.offset.checkpoint.interval.ms来配置，默认为60000

        replication-offset_checkpoint(复制点文件)：
            存放所有分区的HW，kafka会有一个定时任务负责将所有分区的HW刷写到复制点文件中，刷新周期由broker端参数
        replica.high.watermark.checkpoint.interval.ms来配置，默认为5000

        log-start-offset-checkpoint(起始点文件)：
            存放所有分区的日志起始偏移量，LEO与HW变动的过程中，起始偏移量可能会跟着变动，因为数据拉下来了，可能
        就要创建新的日志分段，一旦日志总量达到指定大小，旧的日志将被清除，导致起始偏移量跳到下一个最小的日志分段
        。kafka也有一个定时任务负责将所有分区的logStartOffser写入起始点文件中，这个任务的刷新周期由broker端参数
        log.flush.start.offset.checkpoint.interval.ms来配置，默认值为60000

        cleaner-offset-checkpoint(清除点文件)：
            存放所有分区的清理检查点，用来记录每个分区已清理的偏移量，通过检查点文件将log分为两个部分：干净与污浊
































