
客户端：


生产者：

    生产流程：
        1、生产者调send方法进行消息发送：KafkaProducer.send
        2、一进来直接走生产者拦截器的onSend方法
        3、拉取集群的信息，也就是构建一个Cluster对象，这个对象包含了生产者感兴趣的topic对应的分区与节点信息，包括集
    群控制器是哪个节点、当前集群里有哪些节点、每个节点里有哪些分区、每个topic有哪些分区、每个分区的leader是哪个节
    点等等，都保存在一大堆Map里面，主要有两种对象：Node：节点信息，PartitionInfo：分区信息，每个对象代表一个分
    区，包含了leader节点是哪个、副本在哪些节点、同步节点是哪些、非同步节点是哪些等信息
        4、通过key与value序列化器对key与value进行序列化，将它们转成byte数组
        5、如果消息未指定分区，则用分区器进行分区，默认是DefaultPartitioner分区器。它通过key进行分区。如果key为null
    ，则通过一个topic独有的计数器进行自增与可用分区进行取模来分区。如果key不为null，则将key计算哈希，并与总分区数
    进行取模来分区
        6、将record.headers()设置为只读，这意味着消息头的内容后续无法被修改
        7、估算出这条消息的大小，然后判断是否超出单条消息的最大限制或者生产者缓冲区大小
        8、将消息加入RecordAccumulator累加器
            8.1：为指定topic分区获取一个ProducerBatch泛型的队列，如果已有则拿旧的，否则创建新的
            8.2：尝试将消息加入队列的最后一个批次里面，如果它还有空间，也就是原大小(可能是压缩过的)+当前消息的大小小
        于batch.size。或者批次里连一个消息都没有，那么就不管大小，至少让批次放一个消息
            8.3：如果该队列里没有批次，则从BufferPool类型的free里分配一段ByteBuffer缓存出来，分配的内存在batch.size与
        当前消息大小之间取大的值。缓存池里的缓存是可以被复用的，它将在批次发送之后回收(待确定)
            8.4：创建一个新的ProducerBatch批次，并未它分配缓存内存，然后加入分区队列里，并尝试将消息加入这个批次，
        由于这是一个全新批次，因此这次总会成功。
            8.5：加入成功后会将future与用户的callback绑定在Thunk对象并保存在batch的thunks里，这个似乎是为了触发
        callback时保证消息回调的顺序与生产的顺序相同
            8.6：所有ProducerBatch的append都是同步代码块下进行，因此不会因为并发导致批次大小溢出
        9、sender线程，sender线程是在KafkaProducer的构造器里启动的
        10、sender线程的内容大概为：
            1、如果处于running状态下，则一直尝试消费RecordAccumulator累加器并发送消息
            2、如果不处于running状态，且不是强制关闭，且累加器里还有消息，则将累加器里剩余的消息发送完成
            3、如果是强制关闭，则中断累加器里所有的消息，就是批次不允许再追加、删除队列里所有批次、完成批次的future
        、回收分配的内存
            4、关闭客户端
        11、sendProducerData方法从累加器里获取批次并发送
            11.1：拿到累加器里所有分区的leader节点，如果没有leader且批次队列里有数据，则将topic加入
        unknownLeaderTopics，也就是无leader的topic集合
            11.2：判断队列里的第一个分区是否就绪，例如是否达到批次等待最大时间、批次是否已满、缓存池是否有线程在等待
        分配、累加器是否已被关闭等待，如果满足这些条件任意一个，且不是重试批次且小于重试间隔，就会将leader加入就绪
        节点中返回去，此外还有一个下次重试时间，它会取所有批次重试间隔里最短的返回去，用来跟单次请求间隔取最小值来
        进行nio的select(timeout)操作，而它默认是Integer.max，因此可发送的情况下用的是请求超时的参数
            11.3：因此上面两步操作返回的值包括：无leader的topic列表、批次可以发送的leader列表、下次select时间
            11.4：如果未知leader的topic列表不为空，则将对应的topic加入metadata中，并尝试去更新集群的元数据
            11.5：如果ready的leader节点列表不为空，则判断每个节点是否就绪，例如客户端中节点同时发送的最大请求是否达
        到上限、客户端的各种组件状态是否正常，例如selector与channel等等，如果客户端判断该节点未就绪则将该节点从就
        绪集合中移除，并根据节点的重连接时间配置来决定下次select的时间，依然是跟上面的下次select时间取最小值
            11.6：调用accumulator.drain方法，去拿到可用leader里所有可发送的批次
            11.7：isMuted方法是用来判断该分区是否有正在发送的消息，如果有说明配置要求保证消息顺序，因此只能等上次消
        息发送完成后才能发送，而这次跳过这个分区
            11.8：如果不是重试且未满足重试间隔、且未到达最大请求字节数，就从队列里poll出第一个批次，将这个批次关闭，
        然后加入批次就绪集合中返回。每次sender线程轮询都只会拿每个批次队列里的第一个批次，哪怕后面也有就绪的也要等
        下次
            11.9：如果要保证消息的顺序性，也就是guaranteeMessageOrder为true，也就是
        max.in.flight.requests.per.connection等于1，就会将分区加入累加器的muted集合中，在muted集合中已有的分区就不
        会发送消息，只能等上次发送完后从muted中清除对应的分区才能发下次
            11.10：查找累加器中的过期批次，比如批次创建时间大于请求最大等待时间，就会将批次删除，传入超时异常回调批
        次里的future、回收缓存、记录指标等等，通过这种方式进行的超时是不会重试的，因为它直接进行回调了
            11.11：更新生产者请求的各项指标
            11.12：如果有就绪的leader节点，就将下次select的时间设置为0
            11.13：调用sendProduceRequests正式发送批次消息
            11.14：遍历batches发送消息，这是个map，key为nodeId，value为批次队列，因此每次只给一个发送节点
            11.15：包装成一个RequestCompletionHandler callback对象，这个对象里有一个外部构建的map，key是TopicPartition
        ，value是此次该分区发送的批次，每次每个分区只会发送一个批次，回调里也包含了TopicPartition，因此可以根据响应
        从map里拿到批次，然后进行回调
            11.16：创建一个clientRequest对象，这个client是跟节点绑定的业务request，然后KafkaClient.send，这个client是
        共用的client
            11.17：将请求构建成一个InFlightRequest inFlightRequest对象，然后保存到inFlightRequests中，这里记录的是当
        前所有正在发送的请求
            11.18：以上只是将request注册到selectKey里，它们通过一个KafkaChannel对象来关联
            11.19：client.poll，进行select
            11.20：client.poll后，里面有个metadataUpdater.mybeUpdate，这是发送更新元数据请求的，还有个selector.poll
        ，这是处理就绪的SelectionKey
            11.20：select到key之后，就会调用pollSelectionKeys方法处理key，这个方法里面有个attemptRead(key)操作，这是
        用来将读到的数据转成NetworkReceive的，NetworkReceive就是响应数据的包装，然后保存到stagedReceives里
            11.21：selector.poll的最下一行会调用addToCompletedReceives，将所有receive都加入completedReceives里
            11.22：然后再client.poll里，调用handleCompletedReceives方法，将receive用AbstractResponse.parseResponse
        转成AbstractResponse对象，这个对象会根据api的key转成不同的响应类型，如果这些响应是元数据或者api版本响应，
        则去更新元数据与api版本的信息，否则就都转成ClientResponse类型加入List<ClientResponse> responses对象中
            11.23：在client.poll里，调用completeResponses方法去处理所有的responses，也就是调用它们的callback
            11.24：在handleCompletedReceives中，response和callback的关联保存在inFlightRequests集合里，这个集合保存
        了所有正在请求的对象，每个响应 都从对应的节点请求集合里拿一个最早发起的请求对象，那么怎么保证最早发起的请求
        一定是最早响应的。因为对于相同的node，每个KafkaProducer只会创建一个连接，因此响应都是顺序返回的。
            

    kafka缓存池的消息存储：
        每个批次都会分配一段内存，批次的发送结束后回收内存，内存指的是一个ByteByffer对象，消息写入批次时，会按照
    key、value、headers的顺序写入，先写入key的长度，再写key的内容，value也一样，headers则先写入header的数量，
    然后再写每个header的长度与内容

    metadata的更新：
        在client.poll里，有个metadataUpdater.maybeUpdate(now)方法，里面会调用maybeUpdate，里面会构建一个
    metadataRequest请求，然后注册到select上，这里会选择一个调用最少的节点进行元数据拉取

    ApiVersions：
        这里的Api指的是操作的类型，比如生产消息、心跳、元数据拉取等待，都在ApiKeys里，由于Kafka有多个不同的版本，
    因此通过从节点拉取api版本列表的确定broker支持哪些api的哪个版本

    错误重试：
        重试是在处理响应的callback时处理的，会将批次进行重新发送，直接加入累加器的队列中，且会加在头部。可以在
    sender的sendProduceRequest方法中看到，里面会创建一个RequestCompletionHandler callback用来构建客户端，这里
    面就包含了重试逻辑，比如重试次数判断，是否是重试异常等等

    好像过期的批次，进行回调后依然会进行发送？

        kafka发送消息时，NetworkClient的InFlightRequests正在发送队列满了咋办，此时会判断是否内部消息，比如元数据或API版
    本请求，如果是的话就硬发，如果不是，比如消息生产请求，就会抛出IllegalStateException异常
        由于sender线程在从RecordAccumulator里拿批次前就会判断该节点请求是否可用，如果可用才会拿，而sender是单线程的，能
    保证不会超，所以正常情况下是不会出现poll出了批次，请求又满了的情况，如果真的出现了，这次发送的所有批次应该都会因为
    报错导致丢失


消费者：

        假设消费者消费到的偏移量是7，那么此时提交的偏移量就是8，也就是提交的偏移量总是会比消费到的+1，在日志里的
    偏移量也一样

        由于消费者是线程不安全的，因此大部分操作内部都会先调用acquire()，结束时调用release()，这是一个加锁操作，通过
    CAS获取当前线程id进行加锁，如果CAS失败，就代表多个线程在同时操作consumer，就会抛出异常，提示节点未准备好发送请求






ISR(In-Sync Replicas)：
    同步副本

优先副本：
    指进行leader选举时，该副本会优先被选举为leader

分区重分配：
    如果某个broker挂了，它的分区又有多个副本，此时最多就会将leader转移到其他副本上。但是这个分区的副本已经处于不
可用的状态了，失效的副本不会自动转移到其它可用的节点上，这会影响到系统整体可靠性和可用性。因此就需要进行分区重
分配来将分区副本迁移到可用节点上




日志存储：

    日志文件布局：
        主题 -> 对应多个分区 -> 对应多个副本 -> 对应一个日志目录(一个分区副本) -> 对应多个日志分段(LogSegment) ->↓ 
    对应一个日志文件(.log)、偏移量索引文件(.index)、时间戳索引文件(.timeindex)、以及其它文件

    日志目录所在与格式：
        一般会在kafka安装目录所在盘的/tmp/kafka-logs，命名方式为topic-partition


        kafka向Log中追加消息时是顺序写入的，每个日志目录，也就是分区副本中，只有最后一个LogSegment才能被执行写入操作，
    其余同分区中的所有LogSegment都不能写入数据。最后一个LogSegment称为activeSegment，既活跃的日志分段。随着消息的不断写
    入，当activeSegment满足一定条件时，就需要创建新的activeSegment，之前activeSegment不再是活跃分段，之后追加的消息都将
    写入新的activeSegment

        每个LogSegment中的日志文件都以.log作为文件后缀，每个.log文件都有对应的两个索引文件：.index与.timeindex，分别是
    偏移量索引与时间戳索引文件。每个LogSegment都有一个基准偏移量baseOffset用来表示当前LogSegment中第一条消息的偏移量。
    偏移量是一个64位的整型，日志文件和两个索引文件都是根据baseOffset命名的，名称固定为20位数字，比如每个分区的第一个日
    志文件名为：00000000000000000000.log，这代表该文件的偏移量是从0开始的。这个数字代表的是消息偏移量，而非字节数

        _consumer_offsets偏移量主题初始是不存在的，当第一次有消费者消费消息时会自动创建这个主题。每个kafka-logs根目录都
    会包含4个xxx-checkpoint检查点文件与一个meta.properties元数据文件，这些文件会在kafka服务第一次启动时创建


    日志格式：
        kafka消息从0.8.x之后到现在的2.0.0经历了3个版本：v0、v1、v2，0.8之前的不用再管

        v0版本：
            消息分为两部分：
                LOG_OVERHEAD，即消息头，它包含了：
                    offset(8B)：消息偏移量
                    message size(4B)：消息大小

                RECORD，即消息本身，它包含了：
                    crc32：校验值，校验后面的所有其余信息，4B
                    magic：消息版本号，1B
                    attributes：低三位表示消息压缩类型比如NONE、GZIP、LZ4等，其余位保留，1B
                    key length：key的长度，-1为没有key，4b
                    key：key内容
                    value length：value的长度，-1为没有value，4B
                    value：value内容，也可能会为空，比如墓碑消息
            每个完整的消息都由LOG_OVERHEAD与RECORD两部分组成，由于它们之间总是一对，因此通常也罢它们看作是一条消息的整
        体，每个log文件都由多个这种格式的消息组成
            每个消息最小长度为：4+1+1+4+4=14B，也即v0版本中一条消息长度如果小于14B，就代表它是破损消息而被拒收。kafka保
        存时会在另外加上12B的LOG_OVERHEAD，假设发送一条key为key，value为value的消息，那log文件的大小就为34B=12+4+1+1+4+
        3+4+5，其中3和5分别是key与value的大小

        v1版本：
            v1版本相比v0版本，也就在每条消息的magic后面多了一个8B的时间戳，然后attributes的第4位用来存储时间戳的类型。
        0表示CreateTime，1表示LogAppendTime。这个类型由broker端的log.message.timestamp.type来配置，默认值为CreateTime，
        既生产者创建消息的时间戳，如果用户没有显式指定，那么生产者发送消息时会自动创建这个时间戳
            因此v1版本的消息会比v0版本的大8个字节

        v2版本：
            消息分为两部分：
                Record Batch Header，既批次头，它包含了：
                    first offset：当前消息批次的起始偏移量，8B
                    length：从partition leader epoch字段到整个批次末尾的长度，4B
                    partition leader epoch：分区leader纪元，可以看做分区leader的版本号或更新次数，4B
                    magic：消息格式版本号，1B
                    crc32：校验值，4B
                    attributes：消息属性，低三位代表压缩格式，第四位代表时间戳类型，第五位标识此批次是否处于事务中，第
                六位表示是否控制消息，2B
                    last offset delta：消息批次中最后一个记录的偏移量与first offset的差值，4B
                    first timestamp：消息批次中第一条记录的时间戳，8B
                    max timestamp：消息批次中最大的时间戳，一般情况下是指最后一个消息的时间戳，8B
                    producer id：生产者id，用来支持幂等和事务，8B
                    producer epoch：生产者纪元，用来支持幂等和事务，2B
                    first sequence：用来支持幂等和事务，4B
                    records count：消息批次中的消息个数，4B

                Records：既批次列表，每个批次包含了：
                    length：消息总长度，varint
                    attributes：弃用，但是还存在着，1B
                    timestamp delta：基于批次头时间戳的增量，varlong
                    offset delta：基于批次头偏移量的增量，varint
                    key length：key长度，varint
                    key：key内容
                    value length：value长度，varint
                    value：value内容
                    headers count：header数量，varint
                    headers：header内容
                        header key length：header key的长度
                        header key：header key内容
                        header value length：header value的长度
                        header value：header value的内容
                以上很多地方都采用了增量存储，因为增量配合着变长字段可以进一步节省占用的字节数

    变长字段：
        Varints是kafka从0.11.0版本引入的变长整型，它的作用是：例如value length是-1，原本需要用4个字节来保存，用了
    Varints之后就只需要1个字节来保存，数字越小越节省空间，不过Varints并非会一直节省空间，一个int32一般是4个字节，但是
    用了Varints后最长会占用5个字节，一个int64一般是8个字节，用了Varints后最长会占用10个字节
        不过kafka的使用场景一般length都会比较小，因此总是能节省到空间

    消息压缩：
        常见的压缩算法是数据量越大压缩效果越好，但是一条消息通常不会太大，这就导致了压缩效果不太好。而kafka实现压缩的方
    式是将多条消息一起进行压缩，这样可以保证较好的亚索效果。在一半情况下，生产者发送的压缩数据在broker端的存储时也是保
    持压缩状态的，消费者从服务端获取的也是压缩的消息，消费者处理消息时才会解压消息，这样就保持了生产端到消费端的压缩

        kafka日志使用哪种压缩方式是通过compression.type参数来决定的，默认值为producer，表示保留生产者使用的压缩方式，这
    个参数还可以设置为：GZIP、SNAPPY、LZ4这几种压缩算法，如果配置为uncompressed则表示不压缩

        消息集合被压缩后作为内层消息(inner message)存在，整个内层消息作为外层消息(wrapper message)的value，也就是说压缩
    后，整个压缩结果都作为一个新消息的value，这个消息的key是空的
        生产者在压缩消息时，会为内层每个消息创建一个LOG_OVERHEAD，其中Offset是从0开始的，这代表消息在内层消息里的位置
        而外层消息的偏移量会根据内层消息的数量来决定，例如：上条消息的偏移量为1024，而此次压缩了6条消息，那么外层消息的
    偏移量就是1030，再下条消息就是1031。所以外层消息保存的是内层消息里最后一条消息的绝对偏移量

        压缩消息是指compress message，kafka里还有另外一个compact message，是指日志清理时的压缩，不要搞混

    消息压缩时的timestamp与attributes字段处理：
        外层消息的时间戳处理：timestramp是CreateTime，那么取的是内层消息中最大的时间戳，如果是LogAppendTime，那么取的是
    Kafka服务器的时间戳。
        内层的时间戳处理：外层消息是CreateTime，取得是生产者创建消息时的时间戳；外层是LogAppendTime，内层的时间戳将会忽
    略
        auutibutes的处理：外层消息会根据配置设置，内层消息的时间戳类型位总是CreateTime，因为其它类型不会处理


日志索引：

    每个日志文件都对应两个索引文件，分别是：
        偏移量索引文件：
            用来建立消息偏移量到物理地址之间的映射关系，方便快速定位消息所在的物理文件位置。里面的偏移量保存的都是增量
        数据，基础偏移量用文件名保存，比如，要找偏移量277的消息，那么就先找到小于277的最大日志分段文件，比如250，然后计
        算相对偏移量：277-250=27，之后在250.log日志分段对应的偏移量索引文件中，根据二分查找法来找到偏移量小于27的最大偏
        移量，最后顺着该偏移量对应的物理地址在日志分段中往后找，直到找到对应偏移量的消息
            偏移量索引的存储结构是：前面4B存放偏移量增量，后面4B存放物理地址，由于默认每4K的消息才存放8B的索引，因此1G
        日志只需要2M的索引文件就能完成索引
            kafka强制要求索引文件大小必须是索引项(每一个索引的数据)的整倍数，因此偏移量索引的索引文件大小必须是8B的整倍
        数，如果配置为67B，kafka会将其转为64B，因为64B是小于67B的最大8B的倍数

        时间戳索引文件：
            时间戳索引文件则根据指定的时间戳来查找对应的偏移量信息
            时间戳索引的存储结构是：前面8B存放当前日志分段的最大时间戳，后面4B存储这个时间戳对应的相对偏移量
            由于时间戳索引和偏移量索引是同时插入的，因此时间戳索引会比偏移量索引文件大1/3，时间戳索引的大小也有强制要求
        ，时间戳索引也是通过二分查找法先找到对应的时间戳，然后拿到时间戳对应的相对偏移量，再去偏移量索引里面找物理地址
        ，最后根据物理地址找到消息


    稀疏索引：
        稀疏索引就是，并不会为每条消息构建一个索引，而是每当写入一定的数据量之后，偏移量与时间戳索引文件分别增加一
    个偏移量索引项和时间戳索引项，这取决于log.index.interval.bytes，默认为4096，既4K，增大该值可以减小索引的密度，
    反之亦然
        稀疏索引通过MappedByteBuffer将索引文件映射到内存中，MappedByteBuffer会通过内存映射的方式读写文件内容，
    这种方式直接调用系统底层，没有JVM和系统之间的复制操作，所以效率很高。
        
    kafka中的索引文件以稀疏索引的方式来构造消息的索引，以提高查询速度。偏移量索引文件中的偏移量时单调递增的，当
查询指定偏移量时，使用二分查找法来快速定位偏移量的位置，如果指定的偏移量不在索引文件中，则会返回小于指定偏移量
的最大偏移量；时间戳索引的查询方式与存储规则也跟偏移量基本一致
    稀疏索引是在磁盘空间、内存空间、查找时间多个方面之间的一个折中方案


    零拷贝：
        正常如果要将数据从磁盘读出来到通过网络发送出去，那么要经过4步：
            1、将数据从磁盘读到内核读缓存中
            2、从内核态切换到用户态，并将数据复制到用户模式下
            3、从用户态切换到内核态，并将数据复制到内核模式下(将数据写入socket)
            4、将内核态的数据写复制到网卡设备中传输

        如果使用零拷贝，相当于省去了切换到用户态复制的操作，只剩下：
            1、将数据从磁盘复制到内核读缓存
            2、将读缓存的数据复制到网卡设备中进行传输


Kafka协议：

    协议数据类型：
        boolean：用0和1代表false和true
        int8：有符号整型，占8位
        int16：有符号整型，占16位
        int32：有符号整型，占32位
        int64：有符号整型，占64位
        unit32：无符号整型，占32位
        varint：变长整型，变长类型参考前面的介绍
        varlong：变长长整型
        string：string类型，开头是int16，代表字符串的长度，后面跟着长度个数的UTF-8编码的字符
        nullable_string：可以为空的字符串，与string一样，如果开头的int16是-1，代表此值为null
        bytes：一个字符序列，开头是int32，代表字符序列的长度，后面跟着长度个数的字节
        nullable_bytes：一个可以为空的字符写，为空时int32为-1，其余情况与bytes类型相同
        records：表示kafka中的一个消息序列，可以看做一个nullable_bytes
        array：数组，元素类型可以是任意其它类型，开头是int32，代表元素个数，int32为-1时表示空数组

    kafka请求协议分为两部分：

        RequestHeader(请求头)：
            api_key：API类型表示，比如PRODUCE、FETCH，16位int
            api_version：API版本号，16位int
            correlation_id：由客户端指定的一个数字来标识这次请求的id，服务端响应时会返回相同的id，客户端就能将请求和响
        应对应起来了，32位int
            client_id：客户端id，string

        RequestBody(请求体)：
            请求体的结构由不同的API类型决定


    kafka响应协议分为两部分：

        ResponseHeader(响应头)：
            correlation_id：请求的id，响应会原样返回，32位int

        ResponseBody(响应体)：
            响应体的结构由不同的API类型决定


    消息生产协议(produce)：
      请求：
        ReauestHeader：请求头，这部分格式是统一的，可参考上面描述，不在赘述

        RequestBody：请求体

          transactional_id：事务id，不使用事务时为null，nullable_string类型

          acks：对应生产者的acks参数，int16类型

          timeout：请求超时时间，对应生产者request.timeout.ms，默认为30000，int32类型

          topic_data：生产者发送的消息集合，每个主题都是一个元素，array类型

            topic：主题名称，string类型

              data：主题对应的消息，每个分区都是一个元素，array类型

                partition：分区号，int32类型

                record_set：records类型，分区的消息
                        
      响应：
        ResponseHeader：响应头，这部分格式是统一的，可参考上面描述，不在赘述

        ResponseBody：响应体

          throttle_time_ms：如果超过了配额(quota)限制，则延迟该请求的处理时间，如果没有配置，那么该字段为0，int32类型

          responses：响应的数据集合，跟topic_data一样，以topic划分元素，array类型

            topic：主题名称，string类型

            partition_responses：主题内所有分区的响应集合，array类型

              partition：分区编号，int32类型

              error_code：错误码，用来表示错误类型，int16类型

              base_offset：消息集合的起始偏移量，该参数+发送消息的数量=发送消息最大的偏移量，int64类型

              log_append_time：消息写入broker端的时间，int64类型

              log_start_offset：所在分区的起始偏移量，int64类型


    消息拉取协议(Fetch)：
      请求：
        ReauestHeader：请求头，这部分格式是统一的，可参考上面描述，不在赘述

        RequestBody：请求体

          replica_id：副本的brokerId，用于follower副本向leader发起FetchRequest请求，普通消费者传-1，int32类型

          max_wait_time：最大拉取等待时间，与消费者的fetch.max.wait.ms参数对应，默认为500，int32类型

          min_bytes：最小拉取字节数，与消费者的fetch.min.bytes对应，默认值为1，int32类型

          max_bytes：最大拉取字节数，与消费者的fetch.max.bytes对应，默认值为52428800(50MB)，int32类型

          isolation_level：事务隔离级别，与消费者的isolation_level对应

          session_id：fetch session的id，int32类型

          epoch：fetch session的epoch纪元，int32类型

          topics：所要拉取的主题集合，array类型

            topic：主题名称，string类型

            partitions：分区集合，array类型

              partition：分区编号，int32类型

              fetch_offset：从分区的哪个位置开始读取消息，int64类型

              log_start_offset：专门用于follower副本发起FetchRequest请求，用来指明分区的起始偏移量，普通消费者为-1
            ，int64类型

              max_bytes：该分区最多能拉取的字节数，对应消费者的max.partition.fetch.bytes参数，默认1MB，int32类型

          forgotten_topics_data：从fetch session中指定要去除的拉取信息，array类型

            topic：主题名称，string类型

            patitions：分区编号集合，array类型

      关于fetch session：
        由于fetch请求是非常频繁的，且大多数时候是不会变的，除了起始偏移量，那么如果能将这些信息保存起来，就可以节省这
      部分带宽，且拉取的分区越多节省的就越多
        session_id创建或变更时会发送全量式的FetchRequest，全量式拉取就是请求体中包含所有要拉取的分区信息，当session_id
      稳定时则发送增量式的FetchRequest，里面的topics为空，因此topics里的内容已经被缓存在了客户端与broker，如果需要从当
      前session中取消某些分区的拉取订阅，则使用forgotten_topics_data字段来实现
        epoch应该就是同一个session_id，每次拉取都会+1，用来保证客户端与broker端的一致性
        fetch session机制在大规模分区拉取或同步时很有用，不过这对客户端来说是无感知的，因为一般而言客户端不会订阅太多
      分区

      响应：
        ResponseHeader：响应头，这部分格式是统一的，可参考上面描述，不在赘述

        ResponseBody：响应体

          throttle_time_ms：如果超过了配额(quota)限制，则延迟该请求的处理时间，如果没有配置，那么该字段为0，int32类型

          error_code：错误码，用来表示错误类型，int16类型

          session_id：对应请求的session_id

          responses：响应的数据集合，跟topic_data一样，以topic划分元素，array类型

            topic：topic名称，string类型

            partition_responses：分区响应数组，array类型

              partition：分区号，int32类型

              error_code：错误码，用来表示错误类型，int16类型

              high_watermark：消费者可以读到的最大偏移量(暂且这么理解，书上对Fetch的响应解释很少)，int64类型

              last_stable_offset：该分区相应的最大偏移量，int64类型

              log_start_offset：该分区相应的最小偏移量，int64类型

              aborted_transactions：事务相关信息，array类型

                producer_id：生产者id

                first_offset：生产者偏移量？事务相关的没介绍

              record_set：该分区的消息集合，records类型


时间轮(TimingWheel)：

    时间轮的设计类似于钟表，可以分为多层，例如第一层是每1ms一格，一共20格，第二层是20ms一格，也是20格，第三个
是400ms一格，一共20格，可以看到当前层的每一格都是上一层的总时间。每一层都是一个圈
    然后每个格都有一个自己的任务队列，这个队列里存放着真正的任务对象

    如果想加一个450ms的任务，那么第一层一共才20ms，满足不了；第二层一共400ms，也满足不了，第三层可以满足，所
以放在第三层的下一格，因为当前不一定刚好从第1格开始，可能第三层的第二格都走完了，因此只能往后放。当第一层走完之
后，会从第二层把任务拉下来，然后根据每个任务的具体时间，分配到第一层里面，因为第二层20ms之内的都是同一格，到第
一层还要进一步分格，当第二层走完后，又会从第三层拉任务，并且层级的指针+1
    第三层的任务拉下来时说明已经到了400ms了，但是还有50ms，因此这个任务会被放到第二层的第三格，当走到第二层的第
三格时，说明已经走了40ms了，每格20ms，还剩下10ms，然后又把该任务放在第一层的第10格，待时间推到那里就会执行该
任务
    当然实际情况肯定更加复杂，比如第一层到第10格，第二层到第5格，第三层到第2格，此时插入一个450ms的任务，那么不
能简单的往第三层后面，因为当前这个轮可能已经跑了一半了，后面插入的任务应该把当前这轮已经经过的时间给加上再计算
格子

    时间轮存放任务之后，还需要一个组件去触发轮子的每一个时间格子，kafka中用了DelayQueue来推进时间轮。
    DelayQueue里存放的是TimerTaskList，这个集合是每个时间格子的任务队列集合，DelayQueue只需对任务队列推进即可
，如果加了一个新任务，新任务是从TimingWheel里插入的，时间复杂度为O(1)，通过时间轮与延迟队列的方式，实现了任务
推进与快速添加与删除任务的操作，达到了少量的空间换时间

DelayQueue与TimingWheel的任务队列之间的配合：
    每个格子都有一个任务队列，每次插入新的任务，都会计算该任务的过期时间，如果新任务的过期时间与该队列上次的过期时间不
一样，那么就会用新的过期时间来将任务队列重新加入延迟队列，也就是说如果有两个任务在一个格子中，那么会出现这个集合被加入
延迟队列中两次，并且每次的过期时间都不同，也会被触发两次
    然后阻塞队列里只有最底层的时间轮，如果有上层的时间轮到期了，就会将它们降级，因此哪怕有重复的任务队列插入阻塞队列也
没关系，因为它们都是以刻度作为超时时间的，同一个任务队列里的刻度相同，所以重新set过期时间也没关系，因为不会改动，除非是
初始状态下的任务队列


延时操作：

    在发送消息时，如果acks为-1，那么消息在leader副本的本地日志文件中写入之后，还要等待所有follower副本都收到消息才能告
知客户端发送成功。在一定的时间内，如果follower没有及时拉取消息，在达到超时时间后就会返回超时异常。这个等待消息写入
follower副本的操作就是延时操作，延时操作可以被超时触发或者外部触发

    再例如在拉取消息时，会先读取一次日志文件，如果收集不够足够多的消息，就会创建一个延迟拉取操作(DelayedFetch)来等待消息拉取到一定的数量才返回，或者达到超时时间强制返回

    延时操作创建后会被加入延时操作管理器\炼狱(DelayedOperationPurgatory)，延时操作可能会超时，所以每个延时操作管理器都
会分配一个定时器(SystemTimer)，定时器的底层实现是时间轮(TimingWheel)，时间轮是靠收割机线程(ExpiredOperationReaper)来轮
转的，收割机线程除了推进时间轮外，还会清理监听池中已完成的延时操作


控制器：

    kafka集群中会将其中一个broker选举为控制器，它负责管理整个集群中所有分区和副本的状态。当某个分区的leader副本
出现故障，由控制器负责为该分区选举新的leader副本；当检测到某个分区的ISR集合发生变化时，由控制器负责通知所有
broker更新其元数据信息；当用kafka-topics.sh脚本为某个topic增加分区数量时，同样还是由控制器负责分区的重分配

    kafka的控制器选举依赖于zookeeper，通过在zookeeper中创建/controller临时节点来进行控制器竞选，临时节点内容如
下：
    {"version":1,"brokerid":0,"timestamp":"1529210278988"}
        version：固定为1
        brokerid：成为控制器broker的id编号
        timestamp：竞选成为控制器时的时间戳
    每个broker启动时都会去尝试读取/controller节点的brokerid值，如果读取到brokerid不为-1，代表已经有broker竞选为控
制器，当前broker会放弃竞选，如果/controller节点为空或者数据异常，就会去尝试创建/controller节点

    zookeeper中海油一个与控制器有关的/controller_epoch节点，这个节点是持久节点，内容是一个整形的controller_epoch
值，用来记录控制器发生变更的次数，既当前控制器是第几代控制器，可称为：控制器纪元，这个节点初始值为1，每次选出一
个新的控制器改字段就+1。每个和控制器交互的请求都会携带controller_epoch字段，如果请求的纪元小于内存中的纪元，则
认为这个是向已过期的控制器所发送的请求，这个请求会被认定为是无效请求。如果纪元大于内存中的纪元，那说明已经有新
的控制器当选了
    由此可见kafka通过controller_epoch来保证控制器的唯一性和相关操作的一致性

    控制器的额外职责：
        1、监听分区的变化，也就是在zookeeper的相关节点注册监听器，然后进行对应动作，比如分区重分配、ISR集合变更
        2、监听主题的变化，也就是监听zookeeper中的/brokers/topic和/admin/delete_topics节点，用来处理主题增减和删
    除主题
        3、监听broker变化，也就是监听zookeeper中的brokers/ids节点，用来处理broker增减的变化
        4、从zookeeper中读取所有主题、分区、broker相关的信息，并进行管理，监听所有主题的/brokers/topics/<topic>
    节点，用来处理broker增减的变化
        5、启动并管理分区状态机和副本状态机
        6、更新集群的元数据信息
        7、如果参数auto.leader.rebalance.enable为true，则开启一个名为：auto-leader-rebalance-task的定时任务来维护
    分区的优先副本均衡

    控制器在选举成功后灰度区zookeeper中个个节点的数据来初始化ControllerContext，并且管理这些上下文，比如某个主题
增加了分区，控制器负责创建这些分区并更新上下文信息，然后将这些变更的信息同步到其它broker节点中。
    由于有很多事件都会读取或更新上下文的信息，比如任务触发的事件、监听器触发的事件，如果单纯使用锁来实现，整体性
能会大打折扣，针对这一现象，kafka的控制器使用单线程基于事件队列的模型，将每个事件进行封装，然后按照先后顺序存储
到LinkedBlockQueue中，然后使用一个ControllerEventThread线程按先进先出原则处理事件，也就是将事件串行化

    早期的kafka中，所有broker都会在zookeeper上注册大量监听器，这有造成zookeeper过载的隐患。现在新版的设计中，
只有controller才会在zookeeper上注册相应的监听器，其它broker极少需要再监听zookeeper中的数据变化，这样省了去很多
不必要的麻烦，不过每个broker还是会对/controller节点添加ControllerChangeHnadler监听器来监听控制器的变化
    当/controller节点的数据发生变化时，每个broker都会更新自身内存中的activeControllerId，如果某个broker原来是控制
器，在变更后activeControllerId的值与自身brokerid不一致，那么它就需要退位，关闭对应的资源、比如状态机与监听器
    当/controller节点被删除时，每个broker都会进行选举(争抢/controller临时节点?)，如果broker在节点删除前是控制器，它
还需要进行退位操作，如果有特殊需要，可以手动删除/controller节点来触发新一轮选举，当然手动关闭控制器或者向
/controller节点写入新的节点id对应的内容也可以触发新一轮选举


优雅关闭：
    通过kafka-server-stop.sh来关闭，这种关闭会等待关闭信号，从而让kafka程序有机会去执行关闭钩子



broker.id：

    kafka集群中，每个broker都有唯一的id，broker在启动时会在zookeeper中的/broker/ids路径下创建一个以当前brokerId
为名称的虚节点，broker的健康转改检查就依赖于此虚节点
    当broker下线时，该节点会自动删除，其它broker节点通过判断/broker/ids路径下是否有此broker的brokerId来确认该
broker的健康状态

    broker.id可以在config/server.properties里配置，也可以通过meta.properties文档来配置，kafka的brokerId只有大于等
于0才会正常启动

    meta.properties(broker的元数据文件，在kafka启动时生成)：
        version：固定值，0
        clusterId：集群id
        broker.id：既brokerId，注意事项如下：
            1、如果config/server.properties与meta.properties中的broker.id不一致，则抛出InconsistentBrokerIdException
            2、如果config/server.properties未配置broker.id，就以meta.properties中的broker.id为准
            3、如果config/server.properties与meta.properties中都没有broker.id，则可以通过broker.id.generation.enable和
        reserved.broker.max.id来配合生成brokerId，这个参数默认开启，默认是true和1000，也就是如果自动生成，那么
        brokerId从1001开始，那个max.id的意思是自动生成的id必须大于这个值
            4、这个id自动生成也是通过zookeeper的/brokers/seqid节点来生成的，会通过它的dataVersion字段，每次设置值
        它都会+1，相当于一个发号器
            5、我们大多数情况下broker.id还是习惯在config/server.properties中进行配置


bootstrap.servers：
    代表连接到kafka集群的broker地址列表

metadata.broker.list：
    用来获取元数据的broker列表，元数据就是例如所有broker的topic分区主从信息、控制器信息等待

zookeeper.connect：
    zookeeper的连接地址


消费者协调器和组协调器：

    旧版本的zookeeper通过大量的zookeeperwatcher来监听kafka集群的状态，比如再均衡时就各自进行分区抢占，消费者
之间并不知道彼此操作的结果，这会导致两个问题：
        1、羊群效应，羊群效应是指zookeeper中一个被监听的节点变化，导致大量的Watcher通知要发送到客户端，导致通知
    期间zookeeper其它操作会有延迟，科有可能发生类似死锁的情况
        2、脑裂问题，zookeeper集群之间相互无法通信，可能导致各个消费者获取的状态不一致。但是zookeeper不是不会有
    脑裂问题吗？总是从leader节点获取不就好了
        属性变更只要能保证同步到大多数节点就行了，那如果刚好从少数节点里读取呢，少数节点如果无法连接到leader，是不
    是直接不提供服务了？


    新版本的消费者客户端对此进行了重新设计，将全部消费组分成多个子集，每个消费组的子集都在服务端对应着一个
GroupCoordinator对其进行管理，GroupCoordinator是kafka服务端用户管理消费组的组件。同时消费者客户端也有一个
ConsumerCoordinator组件负责与服务端的GroupCoordinator进行交互，它们之间最重要的就是负责消费者的再均衡操作，
包括分区分配也是在再均衡期间完成的，一般而言触发再均衡的场景有如下几种：
        1、有新的消费者加入消费组
        2、有消费者下线，并不一定是真的下线，例如遇到长时间GC、网络延迟导致长时间未向GroupCoordinator发送心跳，
    服务端就会认为该消费者已下线
        3、有消费者主动退出消费组，例如发送LeaveGroupRequest，或者调用了unsubscrible方法取消订阅某些主题
        4、消费组所对应的GroypCoordinator节点发生变更
        5、消费组订阅的任一主题或主题的分区数量发生变更

    消费者、消费组、组协调器的工作阶段：
        第一阶段(find_coordinator)：
            消费者需要确定它所属的消费组对应的GroupCoordinator所在的broker，并与该broker创建相互通信的网络连接，
        如果消费者已经保存了GroupCoordinator节点的信息，并且连接能正常通信，那么就可以进入第二阶段，否则就需要向
        集群中的负载最小节点发送FindCoordinatorRequest请求来找到对应的GroupCoordinator，这个请求会将groupId传
        过去，如果找到了就返回对应的node_id、host、port等信息。
            关于负责消费者再均衡的broker，是取决于消费者组落在__consumer_offsets主题的哪个分区上，通过取模计算：
        groupId.hashcode % topicPartitionCount，然后取那个分区的leader作为再均衡的broker，这个broker还负责消费者
        位移

        第二阶段(join_group)：
            成功找到消费组对应的GroupCoordinator后就进入加入消费组的阶段，此阶段的消费者会向GroupCoordinator发送
        JoinGroupRequest请求，并处理响应
            JoinGroupRequest结构：
                group_id：消费组id
                session_timeout：对应消费端的session.timeout.ms，如果GroupCoordinator超过指定时间没收到心跳则认为此
              消费组已下线
                rebalance_timeout：对应消费端参数max.poll.interval.ms，表示消费组再均衡时，GroupCoordinator等待消费者
              重新加入的最常等待时间
                member_id：表示GroupCoordinator分配给消费者的id标识，消费者第一次发送JoinGroupRequest时为null
                protocol_type：表示消费组实现的协议，对消费者而言自字段为：consumer
                group_protocols：分区分配策略数组，取决于消费端参数的partition.assignment.strategy，如果配置了多种策略
              ，这个属性就包含多个值，每个元素内容如下：
                  protocol_name：对应PartitionAssignor接口的name()方法，既协议名称
                  protocol_metadata：分为version、topics、user_data三个字段，其中version固定为0，topics对应
                PartitionAssignor接口的subscription()方法返回值类型的Subscription中的topics，user_data对应Subscription
                中的userData，可为空

            如果原有的消费者重新加入消费组，那么发送JoinGroupRequest之前还要做一些准备工作：
              1、如果开启自动提交偏移量，那么请求之前要先提交消费位移，这个过程是阻塞执行的
              2、如果消费者有添加再均衡监听器ConsumerRebalanceListener，那么会调用onPartitionsRevoked()方法中的自定
            义逻辑，可以清除一些状态，或者提交消费位移等
              3、由于重新加入消费组，那么之前与GroupCoordinator的心跳也不需要了，因此要禁止心跳检测

            服务端收到JoinGroupRequest后，会生成一个memberId，这个id其实就是client.id + "-" + UUID

            GroupCoordinator还要为消费组选举出一个leader，如果消费者没有leader，那么第一个加入消费组的消费者就是消
        费组的leader，如果leader消费者退出了消费组，就会重新选举一个leader，选举就是将所有消费者的放入Map中，key
        是memberId，value是消费者信息，然后拿map.keys()的第一个元素，也就是拿map里的第一个消费者，很随机也很随意。
        leader的职责是在那到所有消费者的信息后，执行具体的分区分配逻辑

            分配策略的选举，每个消费者都可以设置自己的分区分配策略，每个消费组都要选举出一个令彼此都信服的策略来进行整
        体上的分区分配。这个分区分配的选举并非由leader消费者决定，而是根据各个消费者发送过来的分配策略来决定，具体选举
        过程如下：
                1、收集各个消费者支持的所有分配策略，组成候选集candidates
                2、每个消费者从候选集candidates中找出第一个自身支持的策略，为这个策略投一票
                3、计算候选集中各个策略的选票数，选票最多的策略为当前消费组的分配策略
            如果选出的分配策略消费者不支持，就会抛出IllegalArgumentException：Member does not support protocol，消费者
        支持哪个策略，取决于partition.assignment.strategy参数的配置，在这里配置了就算支持，否则就不支持。分配策略选好
        后，服务端就将JoinGroupResponse响应给各个消费者
            JoinGroupResponse响应给普通消费者的信息，除了基础信息外，就只有leader消费者的member_id、最终选择的策略、
        消费组的年代等信息，只有给leader消费者响应的信息才包含各个消费者的订阅信息，好让leader可以进行分区分配。这意味
        着服务端不参与具体的分区分配，这样即使分区分配策略发生变更，也不需要重启服务端，只要重启客户端即可

        第三阶段(sync_group)：
            



































