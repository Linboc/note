
elasticsearch：
    它是一个开源的搜索引擎，建立在Apache Lucene的基础之上，但Lucene仅仅是一个Java编写的库，并且原理非常复杂。Elasticsearch通过封装隐藏Lucene的复杂性，提供一套简单的RESTful API来使用

它是：
    一个分布式的实时文档存储，每个字段都可以被索引和搜索
    一个分布式实时分析搜索引擎
    能胜任上百个服务器节点的扩展，并支持PB级别的结构化或者非结构化的数据

优缺点：
    优点：
        集群扩展方便，支持大数量，查询性能高
    缺点：
        结构简单，分页性能差，不支持复杂查询(联表)

启动：
    直接在bin目录下启动elasticsearch.bat(windows)或者elasticsearch(linux)就可以了
    启动完成之后可以访问localhost:9200来获取elasticsearch的相关信息
    默认访问地址是9200，访问地址是用来给客户端调用的；默认通信地址是9300，通信地址是用来与集群中的其它es或者插件进行通信用的

配置：
    可以在config/elasticsearch.yml文件里修改配置

ELK：
    Elasticsearch(全文搜索服务器) + Logstash(数据收集、解析、转换，也就是拿到数据->进行过滤->输出数据) + kibana(基于es的数据可视化)。ELK就是通过三者进行整合，进行日志收集、解析、可视化的分布式日志分析系统

Elastic Stack：
    Elastic Stack是在ELK的基础上加入了Beats，Beats是数据采集器，能从成千上万台机器上向Logstash或Elasticsearch发送数据。因此形成了：数据采集(Beats) -> 数据分析(Logstash) -> 数据存储(elasticsearch) -> 数据可视化(Kibana)

面向文档：
    ElasticSearch是存储整个对象或文档的，而不是像关系型数据库一样把数据分成行列。ElasticSearch不仅存储文档，还索引每个文档的内容。在ElasticSearch中针对文档进行索引、检索、排序和过滤，而非行列数据，这也是ElasticSearch能支持复杂全文索引的原因

数据格式：
    ElasticSearch使用JSON作为文档的序列化格式，因为几乎所有语言都支持对JSON的处理

es和solr的比较：
		es				solr
底层      	              lucene			                lucene
访问方式	              RESTful			                web service
数据格式	              JSON			                多格式(JSON/XML/CSV)
性能	              查询速度，建索引加数据快	                查询快，建索引加数据慢
分布式管理             自带分布式协调管理		                zookeeper
功能	              第三方插件提供		                solr官方很多自带
社区	              维护者较少			                有更多的用户，开发和贡献者社区
综上所述，如果数据和索引是不断增加的，es会更合适，如果数据变动较少，solr更合适

索引：
    索引(名词)：类似于关系型数据库的一个数据库，是一个存储关系型文档的地方
    索引(动词)：索引一个文档就是存储一个文档到索引中以便被检索和查询

倒排索引：
    为文档的每个属性创建索引，跟关系型数据库的索引类似，根据索引找到对应的数据。每个被索引的属性可以进行分词分成多个索引，每个索引可以指向多个文档
    默认情况下一个文档的所有属性都会建立倒排索引，一个没有倒排索引的属性是不能被搜索到的
    倒排索引会保存每一个词项出现过的文档总数，每个文档中词项出现的次数，词项在文档中的顺序，每个文档的长度，所有文档的平均长度等等

保证es和数据库的一致性：
    1、直接在主业务中同时写入es
        优点：方便简洁，数据更新实时性高
        缺点：侵入代码，跟业务耦合度高，如果es出问题将导致原业务受到影响
    2、通过监听binlog将数据同步到es
        优点：不侵入代码，跟业务耦合度低
        缺点：数据是异步添加，只能保证最终一致性。并且引入了binlog监听和同步服务，增加了开发量和维护成本
     ，如果binlog更新过快可能会处理不过来，还要兼顾插入es失败情况下的处理
    3、在监听binlog后把数据放到mq
        优点：不侵入代码，跟业务耦合低，能进行削峰，可以利用mq特性保证顺序
        缺点：额外维护mq服务跟消费者，无法保证mq消息什么时候被处理

管理ui：
    可以下载elasticsearch-head的浏览器插件，只要几百k，如果下载elasticsearch-head项目，还要另外起一个前端项目

kibana中文支持：
    在kibana.yml里添加：i18n.locale: "zh-CN"
    国际化支持的中文数据源在x-pack\plugins\translations\translations\zh-CN.json里面

解决es跨域：
    在elasticsearch.yml中添加：
        http.cors.enable: true
        http.cors.allow-orgin: "*"

ik分词器：
    使用：
        1、到github将elasticsearch-analysis-ik的zip包下到本地
        2、在es的plugins目录下，新建一个名为ik的目录，将zip包解压到ik
        3、重启es
        4、发送请求获得分词结果：
            POST localhost:9200/_analyze
            body：{ "analyzer": "ik_max_word", "text" : "你算什么东西"}
        5、将会返回一个属性名为tokens的分词列表
    analyzer类型：
        keyword：不会被切分
        ik_smart：最少切分，会分成：你、算什么、东西
        ik_max_word：最细粒度切分，会分成：你、算什么、什么东西、什么、东西
    在ik/config/IKAnalyzer.cfg.xml中的<entry key="ext_dict">boc.dic</entry>标签里加上自定义的扩展dic文件，dic文件放在xml文件的同目录下，dic里写的每行字符串会被ik分词器在进行分词时解析到，认为是一个词

Elasticsearch相关概念：

类型：
    官方解释是类似于表，对同一索引下的文档进行逻辑分隔。
    类型的初衷是为了类比关系型数据库的表，目的是方便管理数据之间的关系。移除类型是因为es中同一个索引中不同的type是存储在同一个索引中的，而不是像关系型数据库的表那样分为不同文件，因此同一索引中不同类型相同的字段名称mapping必须一致，如果不一致会影响lucene的压缩性能
    现在es希望一个index的文档中所有相同字段类型都相同，默认为_doc，es不再推荐添加索引时指定类型，如果业务需要类型进行区分，可以用一个文档的字段来代替，或者不同类型分为不同的index存储

文档：
    一个JSON对象，文档本身没有规定字段与格式，不需要像关系型数据库那样先定义字段才能写入数据

    文档往往也是一个对象，它也有属性，并且属性中可以嵌套对象，不过它与对象不同的是，文档着代表根部最顶级的对象

    文档除了它本身的数据之外还包含着元数据，也就是有关文档的信息：

        _index：索引，文档存放位置
            索引应该是具有相同特性的文档集合，索引名称必须小写、不能以下划线开头、不能包含逗号

        _type：文档类别
            类型是索引中数据的逻辑分区，理论上每个类型可以有不同的字段和不同的字段类型映射，但实际上不同类
     型下的相同字段的类型映射必须相同，因为es会将一个索引中所有的类型下的类型映射合成一个，如果相同字段
      却有不同类型会出现冲突。类型不能以下划线开头，不能包含逗号，长度限制为256个字符

        _id：文档唯一标识
            id是一个字符串，一个_id与_index与_type组合就可以确定es中的一个文档，创建文档是可以提供自己的id
         ，可以以让es来生成。自己指定用PUT，自动生成用POST，自动生成的id是基于Base64编码长度为20的字符
        串

        _version：版本号(乐观锁)
            版本号一个整形，初始值为1，每次修改都+1，为了应对多个线程同时查询，然后基于查询数据进行修改，出现相
        互覆盖的场景，只要在PUT或DELETE时带上version参数就行了，version参数的值是当前的版本号，如果版本号不匹
        配将返回409
            如果数据库本身就有版本号，就可以使用外部版本号，外部版本号也必须是long类型，只要在修改或添加时指定
        version参数，并且指定参数version_type=external就可以了，比如数据库用时间戳做版本号，那么version的值就填
        数据库中的时间戳。如果使用外部版本号，那么版本号就不是比较是否相等了，而是每次修改时es中的当前版本号必
        须小于新的外部版本号
            注意：新版本不再使用version参数，而是同时使用if_seq_no(数据序列号)和if_primary_term(主分片版本号)，这两
         个值会在查询或修改后返回
            _seq_no和_version的差异：很多时候_seq_no都会比_versio大，那是因为_version记录的是当前文档被修改的次数
         ，而_seq_no记录的是修改文档时整个索引中所有的数据被修改次数，只要索引中任一一个文档被修改/添加/删除了
        索引的_seq_no就会+1，然后被修改文档的_seq_no的新值就是索引的_seq_no+1，其余未被修改的文档依然维持不
        变，我们看到的是文档的_seq_no，实际上索引也维护了一个_seq_no，因此有时候修改文档_seq_no可能不止+1

属性：
    每个文档都有多个属性，建立倒排索引就是针对属性进行建立，然后索引映射到文档id找到对应文档

mapping：
    映射每个文档的字段与其对应类型，索引中所有文档每个字段的数据类型，如果不指定类型则默认text，指定类型后如果输入数据和类型不匹配则添加失败，比如类型是long，却不输入整数
    如果type是keyword，那就代表这个字段要完全匹配才会被搜索

query：
    match：分词器解析查询
    term：精确查询
    bool：多条件(or、and)
        must：and
        should：or
        must_not：and not
        filter：
           range：范围查询
_source：字段过滤
sort：排序
from、size：分页
highlight：高亮
    pre_tags：匹配到高亮字符要加的前缀
    post_tags：匹配到高亮字符要加的后缀
    fields：要匹配高亮的文档字段，比如高亮name，那就要在query下对name进行条件匹配，并且指定匹配值，这样才会对name里能匹配的值进行高亮

id：
    id可以自己指定，如果自己不指定就会自动生成随机id，自动生成id是基于Base64编码长度为20的字符串

数组查询：
    要查询的多个值用空格隔开

嵌套对象查询：
    "外部属性.内部属性" : "xxx"

reindex：
    将一个索引的数据复制到另一个索引里

全文：
    全文是指人类容易识别的文本数据，比如推文的内容或邮件的内容

分析器：
    分析器具有如下三个功能：
        字符过滤器：
            过滤特殊字符，比如去掉html，将&转为and，去掉，。！等
        分词器：
            将字符串分成多个词条
        token过滤器：
            每个词条都会通过每个token过滤器，这个过程可能会将一些单词小写化；或去掉一些无用词，比如a、      
        and、the等；或增加一些近义词

    指定分析器后，该字段的值会被指定分析器进行分析建立倒排索引，对该字段进行查询的值也会被分析器进行分析成多个词条后才分别查询

精确查询：
    精确查询不会使用分析器去解析查询的值

全文查询：
    每个字段都有一个分析器，包括_all字段，全文查询会使用分析器将查询的值进行分析，然后再查询，比如2014-09-15可能会被分为2014、09、15这三个词条去查询

类型映射：
    elasticsearch支持以下简单类型(这只是部分类型)：
        字符串：string(text)
        整数：byte、short、integer、long
        浮点：float、double
        日期：date
        布尔：boolean

    如果字段已经指定了映射类型，就会尝试将值转为所映射的类型，无法转换则抛异常；如果未指定映射类型，es则会尝试动态映射

    动态映射规则：
        true或false：boolean
        数字：long
        浮点：double
        日期类型的字符串，比如2014-09-15：date
        字符串：string(text)
        嵌套对象：object
        注意：字符串与其它类型的区别是有没有加双引号；对于数组的动态映射会以第一个值的数据映射结果为准；
    如果值为null或者[]或者[null]则认为是空域，空域不会被索引

    查看映射信息：
        /index1/_mapping

    string类型字段的值默认会在索引前通过一个分析器，针对这个字段查询的值默认也会通过一个分析器

    映射字段的属性：
        dynamic：动态映射
        type：映射类型
        index：索引方式

    动态映射属性，可以在属性映射的的dynamic属性里设置，包括内部对象：
        true：动态添加新字段的映射，默认
        false：忽略新的字段，文档里的数据依然保留，只是无法被索引与搜索
        strict，如果有新字段就抛异常

    索引方式属性，可以在属性映射的index里设置：
        analyzed：先分析字符串再索引，也就是全文索引，字符串类型的默认值
        not_analyzed：直接索引整个值，精确索引，非字符串类型的默认值
        no：不索引这个字段，无法被搜索
        新版的只有true和false选型，代表是否被索引，默认为true

    分析器属性，可以在属性映射的analyzer里设置：
        standard：默认的标准字符串分析器(一般只有字符串类型才会用到分析器)
        simple：简单分析器
        english：英文分析器
        keyword：不会被切分
        ik_smart：最少切分，要添加ik分词器插件
        ik_max_word：最细粒度切分，要添加ik分词器插件

    创建索引并添加类型映射：
        PUT /boc
        {
            "mappings" : {
                "properties" : {
                    "name" : {
                        "type" : "string",
                        "index" : "analyzed",
                        "analyzer" : "standard"
                    },
                    "age" : {
                        "type" : "short"
                    },
                    "title" : {
                        "type" : "string"
                        "analyzer" : "simple"
                    }
                }
            }
        }

    为已存在的索引添加类型映射：
    PUT /boc/_mapping
    {
        "properties": {
            "tag2" : {
                "type": "text",
                "analyzer": "ik_smart"
            }
        }
    }

    为基础类型的字段添加一个额外的下级字段，它们共享同一个值：
    PUT /boc/_mapping
    {
        "properties" : {
            "name" : {
                "type" : "string",
                "analyzer" : "ik_smart",
                "fields" : {
                    "raw" : {
                        "type" : "string",
                        "index" : "not_analyzed"
                    }
                }
            }
        }
    }
    添加一个name与name.raw属性，它们的值一样，name会被解析器解析，name.raw不会，搜索时使用name，排序时使用name.raw，因为如果用name的话，name是分词后被索引的，这种索引排序有很大的不确定性。name.raw是看不到的，但是可以在请求时引用

    测试索引中指定类型映射分析器的解析结果：
        GET /boc/_analyze
        {
            "field" : "name",
            "text" :  "name字段解析我会返回什么结果呢"
        }

多层级对象：
    对于有多层嵌套对象的文档，可以将字段type用object表示

    lucene是不支持内嵌对象的，为了elasticsearch能搜索到内部对象，会将内部对象转为user.id或user.name这种字段，多个嵌套用"."进行关联，如果是内嵌数组，且数组内有对象，则用数组名.内部对象的属性，对于所有内嵌对象，都可以直接指定最内部的属性来引用，比如user.id和user.name可以用id和name来引用
    
    数组内嵌对象：
        数组内嵌对象会导致对象的关联性丢失，比如users数组内有两个对象，小红26岁，小黄23岁，搜索users.name =小红且users.age=23，能把整个文档查出来，因为这个文档内同时存在小红跟23，但是希望匹配的值是数组内的其中一个对象name为小红并且age为26岁，而不是所有的值聚合起来一起匹配
        此时可以把users的类型设置为nested，也就是嵌套类型。如果类型为nested，每一个嵌套对象都会被索引成一个独立的文档，比如数组对象里的每个对象都会成为一个独立文档，这个嵌套对象的文档是跟整个文档绑定的，并且是隐藏存储的，并不会被查出来。如果要进行增删改，就要对整个文档进行重新索引，如果要查，只能查整个文档，而不能只查嵌套文档，被查询到符合条件的嵌套文档返回的是整个文档，而不是嵌套文档

文档相关性：
    文档的相关性通过_score字段来表示，值越大代表相关性越高，值的统计和以下条件有关：
        1、检索的值在字段中出现的频率，出现频率越高相关性越高
        2、每个检索值在字段中的出现的频率，可能一次性会检索多个值，出现频率越高相关性越高
        3、字段内容长度越短相关性越高
    并不是只有统计_score才会有相关性，单纯的true or false查询也会有，在多条件子句的情况下，匹配的子句越多相关性评分越高

doc values：
    在es中doc values是一种列式存储结构，也就是将某一列的值单独存储起来，并进行排序然后序列化到磁盘。默认情况下每个字段的doc values都是激活的。当字段被索引时，es会同时存储该字段的doc values。
    在以下场景下doc values将会被使用：
        1、对某一个字段进行排序
        2、对某一个字段进行聚合
        3、某些过滤，比如地理位置过滤
        4、某些与字段相关的脚本计算
    可以理解为doc_values就是为文档的每个列进行额外的单独存储并排序，这个列可以是多值(数组)也可以是单值。doc values默认对所有的字段使用，除了带分析器的字符串字段，因为这种字段会产生很多token(词项)。如果使用字符串，那么es会将字符串转为数字再排序存储
    禁用doc values：
        假如想禁用doc values来节省磁盘空间，可以在字段的mappings中设置doc_values为false。字段在禁用doc _ values后将不能被用于聚合、排序和脚本操作，就像字段不索引就无法被搜索到一样
    
Fielddata：
    Fielddata是用来针对带分析器的字符串的。功能与doc_values类似，因为带分析器的字符串会产生大量token，会很耗内存，因此Fielddata是懒加载的，只有在使用到某个字段进行聚合或排序时才会将该字段所有相关的Fielddata加载到内存。
    由于Fielddata很吃内存，有可能会造成堆溢出，因此有一系列针对Fielddata的配置防止发生堆溢出

轻量查询：

    GET /index1/type1/1?pretty：查询格式化
    pretty会将查询结果格式化，不过_source的数据不会被格式化，它的格式只会跟添加时的一样，found字段    说明查询是否命中

    GET /index1/type1/1?_source=name,tags：查询指定字段
    只查询source中的name和tags字段

    GET /index1/type1/1/_source：只查询_source字段
    只查询_source字段和其下级节点，其他字段不会返回

    GET /index1/type1/_search：简单文档查询
    查询索引为index类型为type1的所有文档，index可以省略，默认为所有索引，type可以省略，默认为_doc

    GET /index1/type1/_search?q=name:天：条件查询
    在上一条的基础上增加条件：文档的name中包含天

    GET /index1/type1/_search?q=+title:打 -desc:二佬：多条件查询
    在上一条的基础上进行多个条件查询，前缀是+的代表一定匹配，前缀是-的代表一定不匹配，每个条件隔一个
    空格，多个条件之间是或者的关系

    GET /index1/type1/_search?q=天：对所有字段进行条件查询
    当不指定查询字段时，就会对所有字段的进行查询。es会将所有字段拼接成一个大字符串作为一个额外的_all
     字段，类型为string，并进行分词，并且字段的类型依然生效，比如类型为keyword的不会被分词，普通的text
    则会被分词
    可以禁用索引中所有文档的_all字段，也可以让某些字段不包含在_all中，这些都可以进行配置
    see：https://www.elastic.co/guide/cn/elasticsearch/guide/current/root-object.html#all-field
    注意：由于_all是string类型，如果其它字段不是string类型，比如date，那么就可能出现通过2014在_all中能
    查出数据，在date中查不出来，因为不同类型的索引方式不同，比如date索引包含2014-09-12，不会单独抽出
    年月日进行索引，因此要注意不同类型字段之间相互转换时的类型差异

    GET /index1/type1/_search?q=+name:(天 大) +age:>1000 +蓝：复杂多条件查询
    查询name里包含天或大，或者age大于1000，或者_all字段包含蓝的文档

    GET /index1/_mapping：查询字段映射
    查询索引为index1的所有字段映射

    HEAD /index1/type1/1：检查文档是否存在
    只会返回响应头，如果响应状态为200则代表存在，如果为404则代表不存在

    PUT /index1/type1/1：修改索引为index1，类型为type1，id为1的文档
    {
        "name" :  "小黄",
        "age" : 18
    }
    es中的文档是不可变的，如果修改的话实际上会将旧文档标记为已删除，然后添加一个新文档。尽管旧文档不     会被    访问到，但它不会立即消失，es会在后台清理这些标记为已清除的文档。之所以这样是因为修改后的文档需      要重新进行索引

    PUT /index1/type1/1?op_type=create：添加文档
    {
        "name" :  "小黄",
        "age" : 18
    }
    如果id为1的文档已存在则返回409

    PUT /index1/type1/1/_create：添加索引为index1，类型为type1，id为1的文档
    {
        "name" :  "小黄",
        "age" : 18
    }
    如果id为1的文档已存在则返回409

    POST /index1/type1/：添加索引为index1，类型为type1的文档，id自动生成
    {
        "name" :  "小黄",
        "age" : 18
    }
    文档id由es自动生成，自动生成的id基于Base64编码长度为20的字符串

    DELETE /index1/type1/1：删除文档
    删除id为1的文档，删除只是进行标记，且使版本号+1，found为true且响应码为200则删除成功，found为        false且返回状态为404则文档不存在

    POST /index1/type1/1/_update：更新部分文档
    {
        "doc" : {
            "name" : "小黄"
        }
    }
    只将name进行修改，其余保持不变

    POST /index1/_update/1：更新部分文档
    {
        "doc" : {
            "name" : "小黄"
        }
    }
    只修改name，忽略了type，将使用默认类型_doc

    POST /index1/type1/1/_update：使用Groovy脚本更新部分文档
    {
        "script" : "ctx._source.age+=addAge",
        "params" : {
            "addAge" : "5"
        }
    }
    ctx代表查询到的文档本身，_source代表文档的_source属性，age+=addAge代表age属性+=传入的addAge参数，params代表代表要传给脚本的参数，"addAge":5代表传给文档的addAge参数，值为5
    注意：脚本默认是不可用的，配置值是script.groovy.sandbox.enabled: false，如果要用就改为true

    POST /index1/_update/4：如果该id不存在则进行添加
    {
      "doc" : {
        "age" : 998
      },
      "upsert": {
        "name" : "无名",
        "age" : "0",
        "title" : "未知",
        "tags" : ["武林高手", "来无影去无踪"],
        "desc" : "这是一个神秘的人"
      }
    }
    如果索引为boc，type为_doc，id为4的文档存在则修改age为998，如果不存在则添加一个文档，id为4，
_source为upsert的内容

    POST /index1/_update/1?retry_on_conflict=5：更新部分文档，并在版本号不匹配的情况下重试5次
    {
        "doc" : {
            "name" : "小黄"
        }
    }
    由于部分更新文档也是先复制旧值，再修改旧值，再标记删除，再添加。这期间文档可能被其它进程进行修改     ，因此也会出现版本号不匹配而更新失败的情况。这种情况下可以用retry_on_conflict参数进行重试，值为重试    次数

    POST /index1/_update/4?if_seq_no=2&if_primary_term=2：使用乐观锁，PUT和POST都能用
    {
        "doc" : {
            "name" : "小黄"
        }
    }
    使用乐观锁版本控制，只有指定版本号匹配才能修改成功，否则返回409。if_seq_no代表索引序列号，每次修改数据都会+1；if_primary_term代表主分片版本号，每次切换主分片都会+1，这两个值会在每次修改、添加或查询时返回。乐观锁版本控制不能与upsert或retry_on_conflict同时使用，因为upsert可能会添加，
    retry_on_conflict会在es内部控制版本号

    GET /_mget：Multiple Get，一次性查询多个文档
    {
        "docs" : [
            {
                "_index" : "index1",
                "_type" : "type1",
                "_id" : 1
            }, {
                "_index" : "index1",
                "_id" : 2,
                "_source" : "name"
            }
        ]
    }
    查询多个文档，_type不填默认为_doc，参数跟查询单个文档一样，_source里的是要返回的字段。如果其中某些文档不存在依然会返回请求成功，这些不存在的文档需要用每个响应元素的found属性去进行判断

    GET /index1/_mget：一次性查询多个文档，并指定默认的索引和类型
    {
        "docs" : [
            {
                "_id" : 1
            }, {
                "_id" : 2,
                "_type" : "type1"
            }
        ]
        "ids" : [3, 4, 5]
    }
    指定默认的索引和类型，类型不写默认为_doc，也可以在每个docs元素里指定索引和类型，它会覆盖默认值。ids是在索引和类型都使用默认值时使用，它会查出索引为index1，类型为_doc，id为3,4,5的文档。其余使用事项和上一个请求一样

    POST /index1/_bulk：批量操作，注意换行符的使用，如果换行符不正确会报400
    {"index" : {}}
    {"name" : "小黄", "age" : 15}
    {"update" : {"_id" : "1", "_retry_on_conflict" : 3}}
    {"doc" : {"name" : "小绿"}}    // update里要写doc代表_source的数据
    {"delete" : {"_index" : "index2", "_type" : "type1", "_id" : "33"}}
    {"create" : {"_id" : "2"}}
    {"name" : "小黑", "age" : "22"}
    以上请求会：
        1、创建一个索引为index1，类型为_doc，随机id的文档，name为小黄，age为15
        2、然后更新索引为index1，类型为_doc，id为1的文档，name更新为小绿，并重试三次
        3、然后删除索引为index2，类型为type1，id为33的文档
        4、然后创建索引为index1，类型为_doc，id为2，name为小黑，age为22的文档
    以上请求默认索引是index1(url指定)，默认类型是_doc，每个操作中都可覆盖，也可以只写/_bulk，让每个操作自己指定索引和类型
    批量操作只支持：create(创建)、index(创建或所有字段更新)、update(部分字段更新)、delete(删除)四种操作
，每个操作之间用换行符分隔，其中除了delete之外的操作都需要带上一个数据体，数据体和操作也用换行符分隔。也就是除了delete之外的操作，下一行都是数据体，在下一行就是新操作，结束也要用一个换行符结束
    之所以用换行符分隔，是因为服务器解析不再需要读取所有数据才能进行解析，然后挨个处理请求。而是读取到换行符之后就可以马上发请求，可以一边读一边请求，读完的内容请求后可以马上释放，避免了内存中一次性存储整个请求体，这些请求体可能会非常大。这可以用最小的内存对多请求进行处理
    操作的值是文档的源数据，比如索引，类型，id，数据体的值就是操作相关的_source数据
    并不是请求越大性能越高，最好大小是5-15M

    GET /_search：搜索所有索引的所有文档
    空搜索，搜索所有数据，默认返回10条
    响应字段：
        hits：索引命中信息
            total：匹配到的文档总数
            hits：一个文档数据数组，空搜索默认返回前10个文档
                _score：匹配程度，越大说明越匹配，由于是空搜索，所有_score都是1
            max_score：所有查询出文档中最大的score
        took：执行请求的耗时，单位毫秒
        _shards：查询中用到了几个分片，哪些分片查询成功哪些分片查询失败，如果有部分分片不可用也会返回剩        余的分片的结果
        timeout：查询是否超时，可以查询指定timeout参数，值可是是10ms或3s。在超时前es会返回当前已获取的         数据。假如已超时，协调者会返回已查到的部分数据，并且timeout为true。后台其它分片可能依然还在查询

    GET /a*,boc,*s*/t1,t2/_search：搜索指定的多个索引和多个类型
    查询索引名为a开头、等于boc、包含s的索引，并且类型为t1或t2
    索引名可以用*进行模糊匹配，类型必须精准匹配。索引部分写成_all代表所有索引

    GET /_search?from=0&size=5：分页搜索
    from代表偏移量，默认是0；size代表数量，默认是10。这两个参数可以单独存在
    注意：如果需要查1000页的10个，并且有5个主分片，那么就要查出10010(每页10个+第一千页) * 5(每个分片都查出这个数目)个数据，然后协调节点要对这些数据进行排序，并丢弃其中50040个数据，这种情况下的查询成本很高。因此不建议查询太靠后的分页数据

    GET /_search?sort=age:asc&sort=_score:desc&q=小：查询并排序
    查询_all字段的索引中带有小的文档，对age进行升序排序，在age相同的情况下对_score进行倒序排序

请求体查询：
    请求体查询就是将查询参数用请求体传到服务器，依然是用GET方法，但是很多服务器不支持GET带请求体，因此可以同时使用POST方法进行查询请求，查询时POST等价于GET

    请求体查询结构：
    {
        "查询" : {			比如：query查询的结果有匹配评分，过滤的结果没有评分。通常过滤的			           		 	   速度会比查询快，因此如果不需要评分最好选择过滤
            "复合语句"			比如：bool,constant_score(复合语句可以嵌套任何语句,包括复合语句)
            "组合语句" : {		比如：must、must_not、should、filter(与或非)
                "叶子语句" {		比如：match、range、starred、term(查询一个或多个字段)
                    "字段名" : "参数值",
                    "字段名" : "参数值",...
                }
            }
        }
    }

    常用的复合语句：
        bool：评分查询
        constant_score：如果仅有filter，也就是无需评分，那么就可以用这个，结果和bool相同，不过语义更简洁

    常用的组合语句：
        must：条件必须匹配，并计算_score
        must_not：条件必须不匹配
        should：匹配其中任意一条就会增加_score，仅用来提升_score。如果没有must语句，它将查出匹配的数据
        filter：条件必须匹配，但是匹配到了也不会加_score，可以内嵌bool

    常用的叶子语句：
        match_all：查询所有文档
        {"match_all" : {}}

        match：对字段查询，如果字段有指定解析器，就会对查询值进行解析后查询，否则就直接查询。这种方式不     同于轻量查询，它类似于预编译，不会解析用户的查询值
        {
            "match" : {
                "name" : "小红"
            }
        }

        mutil_match：在多个字段上查询相同的值
        {
            "multi_match" : {
                "query" : "小黑",
                "fields" : ["name", "tag"]
            }
        }

        range：范围查询，gt：大于，gte：大于等于，lt：小于。；lte：小于等于
        {
            "range" : {
                "age" : {
                    "gte" : 20,
                    "lt" : 30
                }
            }
        }

        term：精确值匹配，不会对值进行解析，与match的差别是，match将解析的选择交给字段来决定
        {
            "term" : {
                "name" : "小红",
                "tag" : "红头发"
            }
        }

        terms：对多个值进行匹配，只要匹配任意一个就可以
        {
            "terms" : {
                "tag" : ["java", "code"]
            }
        }

        exists：某个字段的值是否存在，如果文档的该字段不为NULL则返回，类似于SQL的IS NOT NULL
        {
            "exists" : {
                "field" : "name"
            }
        }

        missing：某个字段的值是否不存在，如果文档的该字段为NULL则返回，类似于SQL的IS NULL
        {
            "missing" : {
                "field" : "name"
            }
        }

    GET/POST /_search：空查询，并分页，后面的请求体POST等价于GET，将会忽略POST
    {
        "from" : 30,
        "size" : 10
    }
    查询第30-40个文档

    GET /_search：空查询
    {
        "query" : {
            "match_all" : {}
        }
    }
    空查询，等价于GET /_search

    GET /boc,java*/_validate/query?explain：校验查询是否合法，explain的作用是返回解析后的查询
    {
        "query" : {
            "match" : {
                "name" : "大 小"
            }
        }
    }
    查询boc和java开头的索引里name字段的值，匹配大或小
    如果加了explain会返回三个字段：
        _shards：查询的分片信息，上面的笔记有描述
        valid：查询校验是否通过，true or false
        explanations：一个数组，里面是查询被解析后的内容
            index：解析后的索引
            valid：解析是否成功
            explanation：解析后将查询哪个字段的什么值，同一个索引中会出现多个，用空格分隔
            error：如果valid为false，则这里显示错误提示

    GET /index1/_search：查询index1索引中所有name字段中倒排索引里带有大或小的文档，并对age进行排序
    {
        "query" : {
            "constant_score" : {
                "filter" : {
                    "match" : {
                        "name" : "大 小"
                    }
                }
            }
        }
        "sort" : "age"
    }
    以上查询会给age字段进行升序排序，此时_score为null，因为它没有用于排序，计算没有意义。如果不加sort默认情况下会给_score字段进行倒序排序

    GET /index1/_search：查询name中解析器解析后索引能匹黄 小 红的，并对age和_score进行排序
    {
        "query" : {
            "bool" : {
                "must" : {
                    "match" : {
                        "name" : "黄 小 红"
                    }
                }
            }
        }，
        "sort" : [
            {
                "age" : {
                    "order" : "desc"
                    "mode" : "avg"  // 这个是可选的，当age有多个值，比如是数组时，这个参数就会将值聚合再排序 
                }
            }, {
                "_score" : {
                    "order" : "desc"
                }
            }
        ]
    }
    只有在age完全相同的情况下才会对相同的age进行_score排序
    如果age里有多个值，比如age是个数组，那么可以在sort的age里加mode属性，可以填min、max、avg、sum，通过这种方式把多个值聚合为一个值后再排序

    GET /index1/_search?scroll=1m：游标查询
    {
        "query" : {"match_all" : {}},
        "sort" : ["_doc"],
        "size" : 1000
    }
    在index1进行游标查询，除了数据之外还会返回一个scroll_id，可以用这个id去查询游标的下一批数据
    游标查询会存储某个时间点的数据快照。并且由于es会记录游标的查询信息，游标的过期时间能让es自动释放这部分资源，过期时间会在游标每次查询时刷新。_doc排序是让es从还有结果的分片返回下一批结果，不考虑额外的顺序，因此是最有效的排序

    GET /_search/scroll：通过scroll_id查询下一批游标的数据
    {
      "scroll" : "1m",
      "scroll_id" : "DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAAeIWeUV2akN4bThTUks5bUtmTTFkd2FQZw=="
    }
    同一批次游标查询scroll_id不会变，游标的过期时间每次都可以重新设置，判断游标是否结束可以判断hits数组是否为空
    
    GET /boc/_search：精确查询
    {
        "query": {
            "term": {
                "name": "红"
            }
        }
    }
    term可以用来查询数字、布尔值、日期，这几种都能进行精确匹配，不过这种方式只适合简单查询，如果要进行多条件复合查询就要用bool了
    不过对于文本有点特殊，因为文本进行索引的不是精确值，而是解析后的索引值，而通过term进行查询的值不会被解析器解析，因此查询文本会查不到，除非查解析后的索引值，因此如果文本要进行精确查询，可以将该字段设置成not_analyzed
    不需要评分记得用constant_score+filter，因为es将免于计算评分，并对查询进行有条件的缓存，filter的与或非使用and、or、not，而不是must、should、must_not

    GET /boc/_search：精确查询多个值
    {
        "query" : {
            "terms" : {
                "tags" : ["汽", "武"]
            }
        }    
    }
    它跟term差不多，不过它能一次指定多个值，多个值之间的关系是or，文档只要能匹配任意一个值都将被返回

    GET /boc/_search：嵌套bool查询，bool就是布尔查询，里面的每个子查询都可以当成返回bool值的结果
    {
        "query": {
            "bool": {
                "must": [
                    {
                        "bool": {
                            "should": [
                                {
                                    "term": {
                                        "tags": "汽"
                                    }
                                }, {
                                    "term": {
                                        "tags": "武"
                                    }
                                }
                            ],
                            "must_not": [
                                {
                                    "term": {
                                        "desc": "大佬"
                                    }
                                }
                            ]
                        }
                    }, {
                        "constant_score": {
                            "filter": {
                                "range": {
                                    "age": {
                                        "lt" : 1000
                                    }
                                }
                            }
                        }
                    }
                ]
            }
        }
    }
    以上可以理解为：age < 1000 and ((tags = 汽 or tags = 武) and desc != 大佬)
    bool：布尔查询、must：与、must_not：非、should：或，其中bool节点下must和should只能同时存在一个     bool里能嵌套与或非，与或非里能嵌套bool
    如果想做到(a and b) or c可以should里写c，再嵌套一个bool，bool里写must，must写a、b
    加constant_score是因为非倒排索引的值计算_score无任何意义

    GET /boc/_search：范围查询
    {
        "query": {
            "range": {
                "timestamp": {
                    "gt": "2014-01-01 00:00:00||+1M",
                    "lt": "now-1h"
                }
            }
        }
    }
    时间戳大于2014-01-01 00:00:00加一个月，并小于当前时间-1小时，||是一个表达式符号，代表后面是表达式
    gt：大于、lt：小于，后面加e就是大于等于或小于等于，至于纯等于用term就行
    字符串也允许用range过滤，大小排序按字典排序，不过词项越多性能越差

索引管理：

    DELETE /index1：删除索引index1

    DELETE /index1,index2：删除索引index1和index2

    DELETE /index*：删除以index开头的索引

    DELETE /_all：删除所有索引

    DELETE /*：删除所有索引

    禁止使用通配符或者_all来删除索引：action.destructive_requires_name: true

    PUT /my_index：创建索引并配置相关参数
    {
        "mappings" : {}				字段的类型映射
        "aliases" : []					别名数组
        "settings" : {
            "number_of_shards" : 1,			主分片数量，默认5，不可变
            "number_of_replicas" : 0,			副分片数量，默认1，可变
            "analysis" : {				分析器配置
                "char_filter" : {				自定义字符过滤器
                    "&_to_and" : {				定义一个叫&_to_and的字符过滤器
                        "type" : "mapping",			类型为映射
                        "mappings" : [ "&=> and" ] 		将&转为and
                    } 
                },
                "tokenizer" : {},				自定义分词器，非全局，只能在本索引使用
                "filter" : {					自定义token过滤器，非全局，只能在本索引使用
                    "my_stopwords"	: {			定义一个叫my_stopwords的过滤器
                        "type" : "stop",			类型为停词器
                        "stopwords" : ["the", "a"]		删除词条里的the和a
                    }
                },
                "analyzer" : {				自定义分析器，非全局，只能在本索引使用
                    "es_std" : {				创建一个名称为es_std的分析器
                        "type" : "standard",			es_std分析器为标准类型
                        "stopwords" : "_spanish_"		使用西班牙停词器
                    },
                    "my_analyzer" : {				定义一个叫my_analyzer的分析器
                        "type" : "custom"			类型为自定义
                        "char_filter" : ["html_strip", "&_to_and"]	使用html和自定义的字符过滤器，可以有多个
                        "tokenizer" : "standard"			分词器使用标准的，分词器只能有一个
                        "filter" : ["lowercase", "my_stopwords"]	使用转小写和自定义的token过滤器，可以有多个
                    }
                }
            }
        }
    }

    PUT /my_index/_settings：修改已存在索引的配置
    {
        "number_of_replicas" : 0			副分片数量，默认1，可变
    }

    "_all" : {"enable" : false}：直接将索引中所有的_all字段禁用
    "include_in_all" : false：将文档的某字段在_all里排除，并且父对象的配置会改变子对象的默认值
    "_all": { "analyzer": "whitespace" }：配置_all字段的分析器

    PUT /my_index_v1/_alias/my_index：为my_index_v1增加别名my_index
    将my_index作为my_index_v1的别名
    假如需要修改索引的结构，又不希望停机，此时可以将别名指向索引1，系统则调用别名，将修改后的索引命名为索引2，然后将索引1的数据reindex到索引2中，此时只要将索引1的别名删除并添加索引2的别名就能进行不停机更新了

    GET /*/_alias/my_index：查询my_index指向哪些索引
    查询哪些索引将my_index作为别名

    GET /my_index_v1/alias/*：查询my_index_v1索引有哪些别名
    查询my_index_v1的所有别名

    POST /_aliases：原子性的批量增删别名
    {
        "actions" : [
            {
                {"remove" : {"index" : "myindex_v1", "alias" : "my_index"}},
                {"add" : {"index" : "myindex_v2", "alias" : "my_index"}},
            }
        ]
    }
    先删除my_index_v1的my_index别名，再添加my_index_v2的my_index别名，这个操作是原子性的

    POST /_refresh：手动刷新所有索引
    es新添加文档的索引不是马上生效的，而是会先写入内存缓存并周期性地写入段中，提交点文件记录着所有的段，默认情况下时每个分片每秒写入一次，每次都会产生一个新的段。每次写内存缓存时会同时写到translog(事务日志)中。写入事务日志是同步的，也就是说如果有多个节点有主副分片 ，那么要所有分片都写入事务日志才会返回写入成功

    PUT /index1/_settings：异步写入事务日志
    {
        "index.translog.durability": "async",	设置为异步写入translog
        "index.translog.sync_interval": "5s"	写入时间为每5s一次
    }
    修改index1索引的translog写入方式，async为异步，默认是request。如果为异步的话就5s写入一次

    POST /index1/_refresh：手动刷新index1索引的段

    PUT /index1/_settings：修改index1段的刷新频率
    {
        "refresh_interval" : "30s"
    }
    refresh_interval可以识别单位，如果直接写数字不写单位默认为毫秒，写个1会直接瘫痪，-1为不自动刷新
    
    POST /index1/_flush：将translog的数据刷新到磁盘
    手动将index1索引的所有事务日志刷新到索引文件中。分片在每30分钟或者事务日志太大时也会自动flush，flush结束后会同时删除老的translog。flush的数据是从提交点拿的，如果是突然停机，那么事务日志会先加载到段并记录进提交点之后才会看情况flush

    POST /_flush/wait_for_ongoing：将所有索引的translog刷新到磁盘，并等待执行完成
    将index1索引的所有事务日志刷新到磁盘中的索引文件中，并等待刷新完成才返回

    POST /index1/_optimize?max_num_segments=1：强制将索引中所有的段合并成一个段
    将index1索引的所有段合并成一个段，es会自动将小段合并成大段，这会占据服务器的io资源，并且可能会占据节点的所有资源

es的集群与分片：

    垂直扩容：通过提升机器性能来提升服务器性能
    水平扩容：通过增加机器数量来提升服务器性能

    一个运行中中的es实例称为一个节点，集群是由一个或多个永远有相同cluster.name配置的节点组成，它们共同承担数据和负载的压力。当有节点从集群中加入或移除时，集群会重新平均分布所有的数据

    当一个节点被选举称为主节点时，他将负责集群范围内的所有变更，比如索引和节点的增加删除。主节点不需要涉及到文档级别的变更和搜索操作，所以即使流量增加主节点也不会称为瓶颈。任何节点都可以成为主节点如果只有一个节点，那么它就是主节点

    对于客户端来说可以把请求发送到集群中的任意节点，包括主节点。每个节点都知道任意文档所处的位置，并将请求进行转发，然后搜集数据并返回

    GET /_cluster/health：查询集群健康信息
        status：
            green：所有主副分片都运行正常
            yellow：所有主分片都运行正常，但有部分副分片运行不正常
            red：有主分片运行不正常

    分片是es中实际存储文档的容器，索引实际上指向了一个或多个物理分片，每个分片是一个lucene的实例。es是利用分片将数据分发到集群内的各个节点里，当集群扩大或缩小时，es会自动在各个节点迁移分片，使得数据仍然均匀分布在集群中
    分片分为主分片和副本分片，副本分片是主分片的拷贝，副本分片作为冗余备份，并为查询提供服务。在创建索引时就已经确定了主分片数，但是副本分片可以随时修改。默认情况下索引会分配5个主分片，主分片和副本分片保存在同一节点上没有任何意义，因为一旦丢失这个节点主副分片都会丢失
    在进行集群时，每个节点只需要指向集群中的其中一种一个节点就可以了，节点会自动从它指向的节点中获取到主节点的信息，并获取整个集群的信息，为了防止指向的节点挂了，最好指向多个，但是也不用指向所有的节点，配置：discovery.zen.ping.unicast.hosts: ["host1", "host2:port"]

    扩容：只需要添加新节点，集群就会自动把分片分配到这个新节点中，然后客户端对任意节点进行查询就会自动分配到相关节点，如果想提高吞吐量，那么可以添加更多的服务器并设置更多的副本分片，每个副本分片都相当于一个从机，理论上读性能扩展是很方便的

    设置boc索引的副本分片数量，每个主分片都会有这个数量的副本分片，如果集群中不能有这个数量的副本分片那么集群状态会为yellow：
        PUT /boc/_settings
        {
           "number_of_replicas" : 2
        }

    集群中有主分片的节点被关闭了，如果这个节点中的主分片有副本分片，那么那个副本分片就会成为主分片，如果服务又启动了，那么它就会复制期间被修改的数据，并且作为副本分片存在

    主节点跟主分片没有关联，主节点是负责集群中索引和节点的添加和删除，主分片是负责索引中文档的写和读，副本分片则只负责读。一个负责索引和节点，一个负责文档

    文档的分片存储：文档的分片存储是基于如下公式：shard=hash(routing)%number_of_primary_shards，也就是对主分片进行取余，routing一般用id，也可以自定义。因此主分片的数量在索引创建时就确定好，并且永远都不会改

    写入文档的处理：客户端的ip会指向集群中的一个节点，这个节点称为协调节点，如果是写入请求，它会将请求转发到要文档对应的主分片所在的节点，主分片写入成功后会将请求转发到副本分片所在的节点，如果都执行成功了，就会向协调节点响应成功，协调节点再想客户端响应成功
    但是这个过程是有配置可选择的，这些配置在请求时以请求参数的方式添加：

        consistency参数代表一致性，它有三个值：
            one：主分片可用就允许进行写操作
            quorum：主副分片可用数量超过总数量一半才允许进行写操作(主分片数量 + 副分片数量) / 2 + 1
            all：所有分片都可用才允许进行写操作
        默认使用quorum，由于默认副本分片是1，因此要有2个可用节点才能写入。为了避免单节点操作被阻止，因     此规定只有在副本分片大于1的时候才会进行限制

        timeout代表写入副本分片时的等待时间：
            如果可用的副本分片数量不足，es会等待集群中出现足够的副本分片，默认情况下会等待一分钟，也可以使         用timeout参数进行配置，100代表100号码，30s代表30秒

    读取文档的处理：
        客户端向协调节点发请求，协调节点通过文档id判断文档所属分片，然后对拥有这个分片的所有节点进行轮询    ，被轮询的节点将文档返回给协调节点，协调节点再将文档返回给客户端

    对于多文档(mget、bulk)：协调者会向每个节点转发请求，等收到所有节点的响应后协调者再向客户端响应



